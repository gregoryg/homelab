#+options: ':nil *:t -:t ::t <:t H:3 \n:nil ^:nil arch:headline author:t broken-links:nil
#+options: c:nil creator:nil d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t num:t
#+options: p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t timestamp:t title:t toc:t
#+options: todo:t |:t
#+title: harvester-readme
#+date: <2021-02-08 Mon>
#+author: Gregory Grubbs
#+email: gregory@dynapse.com
#+language: en
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 28.0.50 (Org mode 9.4.4)
#+setupfile: ~/projects/emacs/org-html-themes/org/theme-readtheorg-local.setup

   Official site: [[https://harvesterhci.io/][Harvester - Open-source hyperconverged infrastructure]]

* Harvester notes
** Latest dev =.iso=
   + https://releases.rancher.com/harvester/master/harvester-amd64.iso
** Installation considerations
   + Size of Minio buckets
     The default is 20Gi which iw way too small
     Consider setting =--set helm.persistence.size=200Gi=
** My GitHub issues
   + [[https://github.com/rancher/harvester/issues/646][rancher/harvester#646 {BUG} Specifying an internet proxy during installtion o...]]
** Getting details about vms
   + Jsonpath for getting the CD image IDs
     #+begin_src bash
       kubectl -n default get vm \
               -o jsonpath='{.items[*].spec.dataVolumeTemplates[0].metadata.annotations.harvester\.cattle\.io/imageId}' ; echo
     #+end_src

     #+RESULTS:
     : default/image-q8l5g default/image-q8l5g default/image-pj8k8


   + Then use that to find the image used to create the VM
     #+begin_src bash
       kubectl -n default get vmimage image-q8l5g \
         -o custom-columns=NAME:.metadata.name,DISPLAY_NAME:.spec.displayName
     #+end_src

     #+RESULTS:
     | NAME        | DISPLAY_NAME                            |
     | image-q8l5g | opensuse-leap-15.2.x86_64-nocloud.qcow2 |
   + Info for the running VM instances
     #+begin_src bash
       kubectl -n default get virtualmachineinstances \
               -o custom-columns=NAME:.metadata.name,CPU:.spec.domain.cpu.cores,MAC:.spec.domain.devices.interfaces[].macAddress,IP:.status.interfaces[].ipAddress,HOSTNAME:.spec.hostname,NETWORK:.spec.networks[].name,DISK1:.spec.volumes[0].dataVolume.name
     #+end_src

     #+RESULTS:
     | NAME        | CPU | MAC               |         IP | HOSTNAME    | NETWORK | DISK1                      |
     | gg-party-3  |   2 | <none>            | 10.42.0.68 | gg-party-3  | default | gg-party-3-rootdisk-urvnp  |
     | gg-party-2  |   2 | <none>            | 10.42.0.69 | gg-party-2  | default | gg-party-2-rootdisk-0qens  |
     | gg-rescueme |   2 | 02:00:00:db:57:cc | 10.42.0.81 | gg-rescueme | default | gg-rescueme-rootdisk-alkif |
     | gg-party1   |   4 | <none>            | 10.42.0.85 | gg-party1   | default | gg-party1-disk-0-qz4fd     |
   + Simpler list of hostnames and IPs, just running =k get pod=
     #+begin_src bash :results table
       kubectl get pods -l kubevirt.io=virt-launcher \
         -o custom-columns=LAUNCHER_NAME:.metadata.name,HOSTNAME:.spec.hostname,IP:.status.podIP,STATUS:.status.phase \
         --sort-by=.spec.hostname
     #+end_src

     #+RESULTS:
     | LAUNCHER_NAME                      | HOSTNAME       |          IP | STATUS  |
     | virt-launcher-gg-harvestme-1-9ljzb | gg-harvestme-1 | 10.42.0.104 | Running |
     | virt-launcher-gg-harvestme-2-7ddpf | gg-harvestme-2 | 10.42.0.108 | Running |
     | virt-launcher-gg-harvestme-3-nbxlx | gg-harvestme-3 | 10.42.0.109 | Running |
     | virt-launcher-gg-rescueme-4ls6z    | gg-rescueme    |  10.42.0.81 | Running |

** Setting root password on a =.qcow2= image

   + Install the basics - we're looking for =virsh= and =virt-customize=
     #+begin_src bash
       sudo zypper install guestfs-tools libvirt libvirt-client qemu-tools
     #+end_src
   + Set root password - make it random, since setting it explicitly is apparently harder
     to specify than it appears.  Note down what the generated random password is!
     #+begin_src bash
       sudo virt-customize \
            -a root-openSUSE-Leap-42.1-OpenStack.x86_64-0.0.4-Build2.225.qcow2 \
            --root-password random
     #+end_src
   + Once the new image is booted, login as root in order to:
     1. Change the root password to something sane
     2. Create a new user
        #+begin_src bash
          # (as root user)
          passwd
          useradd -m -s /bin/bash -g users -G docker,wheel gregj
          passwd gregj
        #+end_src
   +

** Getting access to Minio
   + Grab the minio client =mc= from [[https://dl.min.io/client/mc/release/linux-amd64/mc][here]]
   + Get the Access+Secret keys from =harvester-system= secret =minio=
     + If unmodified, they're YOURACCESSKEY/ YOURSECRETKEY
   + Configure and test the client, possibly in a container shell on the cluster
     #+begin_src bash
       mc alias set harvester http://minio.harvester-system:9000 YOURACCESSKEY YOURSECRETKEY
       mc ls harvester
     #+end_src

** Show/Change number of replicas on Longhorn volumes
   #+begin_src bash
     kubectl get volumes.longhorn.io -o custom-columns="NAME:metadata.name,STATE:status.state,NUMREPLICAS:spec.numberOfReplicas,ROBUSTNESS:status.robustness,SIZE:spec.size,ACTUAL:status.actualSize,NODE:status.currentNodeID"
   #+end_src

   #+RESULTS:
   | NAME                                     | STATE    | NUMREPLICAS | ROBUSTNESS |        SIZE |    ACTUAL | NODE                |
   | pvc-057fd8cd-6e7a-43e0-b9d3-0ebb8a34b838 | attached |           4 | degraded   | 21474836480 | 480030720 | chewbacca.magichome |
   | pvc-5868f344-11d7-4d5c-983f-673c6eff86b0 | attached |           2 | healthy    | 21474836480 | 480407552 | chewbacca.magichome |
   | pvc-7320258a-4bd3-4858-aae7-73aba22373ea | attached |           2 | healthy    | 21474836480 | 480378880 | chewbacca.magichome |
   | pvc-ff1c04f6-083b-4727-9aef-3e554ede73a9 | attached |           2 | healthy    | 21474836480 | 479825920 | chewbacca.magichome |

   #+begin_src bash
     kubectl -n longhorn-system patch  volume pvc-057fd8cd-6e7a-43e0-b9d3-0ebb8a34b838 --type merge -p '{"spec":{"numberOfReplicas": 2}}'
   #+end_src

   #+RESULTS:
   : volume.longhorn.io/pvc-057fd8cd-6e7a-43e0-b9d3-0ebb8a34b838 patched

** Grabbing volumes from Longhorn
   + Get Longhorn volumes with info of disk size, actual used size and PVC name
     #+begin_src bash
       k -n longhorn-system get volumes.longhorn.io \
         -o custom-columns=NAME:.metadata.name,STATE:.status.state,SIZE:.spec.size,USED:.status.actualSize,PVCNAME:.status.kubernetesStatus.pvcName
     #+end_src
   + Get Minio URL for Harvester
     #+begin_src bash
       k get pvc \
         -o custom-columns=NAME:.metadata.name,ENDPOINT:.metadata.annotations.cdi\\.kubevirt\\.io/storage\\.import\\.endpoint
     #+end_src
   + Copy disk image
     #+begin_src bash
       mc cp  harvester/vm-images/image-pj8k8 .
     #+end_src
   + Convert to =.qcow2= format
     #+begin_src bash
       qemu-img convert -f raw -O qcow2 image-pj8k8 image-pj8k8.qcow2
     #+end_src

** Prepare machine image for QEMU/KVM
   ref: [[https://octetz.com/docs/2020/2020-10-19-machine-images/][Preparing Machine Images for qemu/KVM | octetz]]
   For use with Kubernetes - installing and configuring Docker set us up for use with RKE
   + install Docker
   + configure Docker
     #+begin_src bash
       cat <<EOF | sudo tee /etc/docker/daemon.json
       {
         "exec-opts": ["native.cgroupdriver=systemd"],
         "log-driver": "json-file",
         "log-opts": {
           "max-size": "100m"
         },
         "storage-driver": "overlay2"
       }
       EOF
       sudo mkdir -p /etc/systemd/system/docker.service.d
       sudo usermod -G docker -a gregj
       sudo systemctl daemon-reload
       sudo systemctl restart docker
       sudo systemctl enable docker
     #+end_src
   + Disable sw2ap if enabled
** Prepare image for cloning
   + Get rid of machine ID so that it will be generated anew for each clone
     #+begin_src bash
       echo -n | sudo tee /etc/machine-id
     #+end_src
   + Host name - give a random one if none set
     Place in /usr/local/bin
     #+begin_src bash
       #!/bin/sh
       SN="hostname-init"

       # do nothing if /etc/hostname exists
       if [ -f "/etc/hostname" ]; then
         echo "${SN}: /etc/hostname exists; noop"
         exit
       fi

       echo "${SN}: creating hostname"

       # set hostname
       HN=$(head -60 /dev/urandom | tr -dc 'a-z' | fold -w 3 | head -n 1)
       echo ${HN} > /etc/hostname
       echo "${SN}: hostname (${HN}) created"

       # sort of dangerous, but works.
       if [ -f "/etc/hostname" ]; then
         /sbin/reboot
       fi
     #+end_src
   + Make executable
     #+begin_src bash
       sudo chmod a+rx /usr/local/bin/hostname-init.sh
     #+end_src
   + Add to =/etc/systemd/system/hostname-init.service=
     #+begin_src conf
       [Unit]
       Description=Set a random hostname.
       ConditionPathExists=!/etc/hostname

       [Service]
       ExecStart=/usr/local/bin/hostname-init.sh

       [Install]
       WantedBy=multi-user.target
     #+end_src
     #+begin_src bash
       sudo chmod 644 /etc/systemd/system/hostname-init.service
       sudo systemctl enable hostname-init
     #+end_src
     #+begin_src bash
       sudo rm -v /etc/hostname
     #+end_src
   + Clean up things like =.bash_history=
     #+begin_src bash
       cat /dev/null > ~/.bash_history && history -c && exit
     #+end_src
** Using cloud-config
   + Remember to literally start the config with =#cloud-config=
     #+begin_src yaml
       #cloud-config
       groups:
         - sudo
         - wheel
       ssh_authorized_keys:
         - >
           ssh-rsa
           AAAAB3NzaC1yc2EAAAADAQABAAABgQDIyhdKNeZnl0+nm5ApMVXCjbSvU/dEtFCU2+32GYBzyw6d8OtH7zs219/0ebsGpzPyIPcltWG/hn93A19feT7h/iZ0ZOl+TpdzvK0ExiEaqolZgiLavKcZyG6pVenfg7OF8HhI47XmjzgeVlFCP818TJF/LyA+eJGHumetAi+w7N34JVGz71gZridii1oWeNbzTC6oouBxZIu4+IVANnyTKYwMRzGdd7/iyOyJ1nvO88uedyD/KEZ4ow4tD1OOZE74VepxhbSEDjPu2Z++KQWQ7Ohjy5DZ8WagF/rgTbP0+wGX6AJSZ3S0p1+iXVjgYPx1kw8pPB1Ay7nLGHsgSSreBzxmiyX5rFmj1LtDti+Cy1m2tdnF+bFDT361j0JRgBGqD1R6AE2xCyizHgO6wLmgSzob0y7FzzafrIjFu64QkfAKuzJLXK02j1MdNPTlIuOc9vw6iKkIh3g4N55uPwyCOfhVZDPUgnmq1UPFr3hK5rHyQEpgeVOy5cia303au88=
           iamk8s-local
       users:
         - default
         - name: gregj
           groups: users, sudo, wheel
           password: >-
             ## mkpasswd --method=SHA-512 --rounds=4096
           ssh_authorized_keys:
             - >
               ssh-rsa
               AAAAB3NzaC1yc2EAAAADAQABAAABgQDIyhdKNeZnl0+nm5ApMVXCjbSvU/dEtFCU2+32GYBzyw6d8OtH7zs219/0ebsGpzPyIPcltWG/hn93A19feT7h/iZ0ZOl+TpdzvK0ExiEaqolZgiLavKcZyG6pVenfg7OF8HhI47XmjzgeVlFCP818TJF/LyA+eJGHumetAi+w7N34JVGz71gZridii1oWeNbzTC6oouBxZIu4+IVANnyTKYwMRzGdd7/iyOyJ1nvO88uedyD/KEZ4ow4tD1OOZE74VepxhbSEDjPu2Z++KQWQ7Ohjy5DZ8WagF/rgTbP0+wGX6AJSZ3S0p1+iXVjgYPx1kw8pPB1Ay7nLGHsgSSreBzxmiyX5rFmj1LtDti+Cy1m2tdnF+bFDT361j0JRgBGqD1R6AE2xCyizHgO6wLmgSzob0y7FzzafrIjFu64QkfAKuzJLXK02j1MdNPTlIuOc9vw6iKkIh3g4N55uPwyCOfhVZDPUgnmq1UPFr3hK5rHyQEpgeVOy5cia303au88=
     #+end_src
** Getting a cluster running in the cluster
   One key thing is to assure there are not overlapping Pod network CIDR ranges
*** k3s
    #+begin_src bash
      #!/usr/bin/env bash

      CONTEXT_NAME=inception
      LOCAL_DOMAIN=magichome
      dbuser=<FILLIN>
      dbpass=<FILLIN>
      export SERVER1_HOST=10.42.0.132
      export AGENT1_HOST=10.42.0.130
      export AGENT2_HOST=10.42.0.131
      export DATASTORE="mysql://${dbuser}:${dbpass}@tcp(<FILLIN>.${LOCAL_DOMAIN})/k3sup"

      # install the first server
      k3sup install --ip ${SERVER1_HOST} \
            --user gregj \
            --ssh-key ~/.ssh/k8s-local \
            --datastore ${DATASTORE} \
            --context ${CONTEXT_NAME} \
            --no-extras \
            --k3s-extra-args '--cluster-cidr 10.52.0.0/16 --service-cidr 10.53.0.0/16 --cluster-dns 10.53.0.10' \
            --print-command

      # Join the agents - to either server
      k3sup join --ip ${AGENT1_HOST} \
            --user gregj \
            --ssh-key ~/.ssh/k8s-local \
            --server-ip ${SERVER1_HOST} \
            --print-command


      k3sup join --ip ${AGENT2_HOST} \
            --user gregj \
            --ssh-key ~/.ssh/k8s-local \
            --server-ip ${SERVER1_HOST} \
            --print-command
    #+end_src
* Mohammed's Harvester Node driver
   AKA node-driver NodeDriver

  + [[https://github.com/belgaied2/docker-machine-driver-harvester/releases/][Releases · belgaied2/docker-machine-driver-harvester · GitHub]]
    All you need to do is to :
    + [[https://docs.docker.com/machine/install-machine/][Install Docker Machine]]: using brew or downloading the version for linux
    + download the [[https://github.com/belgaied2/docker-machine-driver-harvester/releases/][harvester driver]]
    + give the file execution rights and put in your PATH
    + then, run =docker-machine create -d harvester --help=
  The Driver will need Kubeconfig information (CA CERT, CLIENT CERT and CLIENT KEY + K3OS
  Kube proxy URL) and will create the VMs using the Kubernetes objects for
  KubeVirt/Harvester.

  #+begin_src bash
    docker-machine create -d harvester \
                   --ca-cert "LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJoekNDQVM2Z0F3SUJBZ0lCQURBS0JnZ3Foa2pPUFFRREFqQTdNUnd3R2dZRFZRUUtFeE5rZVc1aGJXbGoKYkdsemRHVnVaWEl0YjNKbk1Sc3dHUVlEVlFRREV4SmtlVzVoYldsamJHbHpkR1Z1WlhJdFkyRXdIaGNOTWpFdwpNekU0TURFeU9UTTVXaGNOTXpFd016RTJNREV5T1RNNVdqQTdNUnd3R2dZRFZRUUtFeE5rZVc1aGJXbGpiR2x6CmRHVnVaWEl0YjNKbk1Sc3dHUVlEVlFRREV4SmtlVzVoYldsamJHbHpkR1Z1WlhJdFkyRXdXVEFUQmdjcWhrak8KUFFJQkJnZ3Foa2pPUFFNQkJ3TkNBQVFSdEM1ZXEzWEZSSUY2alE1V1JjOXNhZGliU2NjaWxnVWh5S2x5VSttbgphNEY1WktLS05zQnQxT1dWcmpRbEFtYkRWdUVrejN1WkF2aGN0cnA2Zm5TOW95TXdJVEFPQmdOVkhROEJBZjhFCkJBTUNBcVF3RHdZRFZSMFRBUUgvQkFVd0F3RUIvekFLQmdncWhrak9QUVFEQWdOSEFEQkVBaUIyMEhEaTJhVzAKVnc2SVgvV25sT1VIelFvZm51UUsySlZlOEtha09rK2Z2d0lnS0FlOW9aQ2dNZVlnd0JLTERRWFI2VXNNR2V5ZwpUTEFzdlNQTW1DZjNGcEU9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0=" \
                   --cert "LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJrRENDQVRlZ0F3SUJBZ0lJRnhydW9NYXQ4VzB3Q2dZSUtvWkl6ajBFQXdJd0l6RWhNQjhHQTFVRUF3d1kKYXpOekxXTnNhV1Z1ZEMxallVQXhOakUwT0RFek56WTBNQjRYRFRJeE1ETXdNekl6TWpJME5Gb1hEVEl5TURNdwpNekl6TWpJME5Gb3dNREVYTUJVR0ExVUVDaE1PYzNsemRHVnRPbTFoYzNSbGNuTXhGVEFUQmdOVkJBTVRESE41CmMzUmxiVHBoWkcxcGJqQlpNQk1HQnlxR1NNNDlBZ0VHQ0NxR1NNNDlBd0VIQTBJQUJNQWY2Rldvbm1OLzlRM3EKZE93ZGRCeXpDS0tGZi9yeFVTUDVLRFRqcjdkRE1pUStDM0Q2QWw5RGxsR2RBYkdMOFZveDB3TisrMnRWTWlRRQpvMmZ1YjkralNEQkdNQTRHQTFVZER3RUIvd1FFQXdJRm9EQVRCZ05WSFNVRUREQUtCZ2dyQmdFRkJRY0RBakFmCkJnTlZIU01FR0RBV2dCVEJuUmFvOVFiWVJUTG5QYk9kNjZsdWNYUFhuREFLQmdncWhrak9QUVFEQWdOSEFEQkUKQWlBdi9OaURpMlJGbGRrMzMrQlZRZHhyYmdrdGJRRUUxeXJSMHVnSE5hWmRkUUlnUm0rVTAzVUZrelhLdFVGYgpmQUtQYyt6cmFYRm8zY1FmbEpaQUNQR0RSUW89Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0KLS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJkekNDQVIyZ0F3SUJBZ0lCQURBS0JnZ3Foa2pPUFFRREFqQWpNU0V3SHdZRFZRUUREQmhyTTNNdFkyeHAKWlc1MExXTmhRREUyTVRRNE1UTTNOalF3SGhjTk1qRXdNekF6TWpNeU1qUTBXaGNOTXpFd016QXhNak15TWpRMApXakFqTVNFd0h3WURWUVFEREJock0zTXRZMnhwWlc1MExXTmhRREUyTVRRNE1UTTNOalF3V1RBVEJnY3Foa2pPClBRSUJCZ2dxaGtqT1BRTUJCd05DQUFTdUl5QzdCMFJ5Q2Ixejd1UlJZdzV5QXJ6ci9uYjA1ZTNPLzBzWUhDYnUKOEh6aUlIenZnUkZqTG4xV0ZEd0FnVVEzU2xjUWZpRUN5SmY3UkU0ME9EL1lvMEl3UURBT0JnTlZIUThCQWY4RQpCQU1DQXFRd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZEJnTlZIUTRFRmdRVXdaMFdxUFVHMkVVeTV6MnpuZXVwCmJuRnoxNXd3Q2dZSUtvWkl6ajBFQXdJRFNBQXdSUUlnVjNrQlEwc0lpY2dQclFaTXRUVllBZW9OYnU0VGRiYXMKVUR2b3lCZHdpWWtDSVFDSkROUDlqWDZqWnpPbXRKdDlIaUVoS1YrLzdzOVNGeTBsc0xxdGVlWWdrQT09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K" \
                   --key "LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSURjelVCU3huY1BhVFA5VUVXcjZaMy9BUk1WUUdQUGl2NE80ZHNKejZSbWpvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFd0Ivb1ZhaWVZMy8xRGVwMDdCMTBITE1Jb29WLyt2RlJJL2tvTk9PdnQwTXlKRDRMY1BvQwpYME9XVVowQnNZdnhXakhUQTM3N2ExVXlKQVNqWis1djN3PT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo=" \
                   --harvester-url "https://172.16.17.16:6443" \
                   --cpus 3 \
                   --disk-size "42Gi" \
                   --keypair "k8s-local" \
                   --ssh-key-path "~/.ssh/id_rsa.pub" \
                   --ssh-user "gregj" \
                   --mem-size "4Gi" \
                   --vm-description "This is a hoooooot" \
                   --vm-name "gg-whatahoot" \
                   --image-id "image-mfz99" \
                   gorto
  #+end_src
** Work getting the certs from KUBECONFIG
   + Get the contexts to present to the user
      #+begin_src bash
        k config get-contexts -o name
      #+end_src
   + get cluster and user from selected context (e.g. =rancher=)
     #+begin_src bash :results output raw
       kubectl config view -o json|jq -r '.contexts[] | select(.name == "rancher") | .context'
     #+end_src

     #+RESULTS:
     {
       "cluster": "rancher",
       "user": "rancher"
     }
   + get cluster CA
     #+begin_src bash :results output raw
       kubectl config view --flatten -o json | jq -r '.clusters[] | select(.name == "rancher")' | sed 's,LS0.\+,blah-blah,'
     #+end_src

     #+RESULTS:
     {
       "name": "rancher",
       "cluster": {
         "server": "https://rancher.hypecyclist.org/k8s/clusters/local",
         "certificate-authority-data": "blah-blah
       }
     }
     {
       "name": "rancher",
       "cluster": {
         "server": "https://rancher.hypecyclist.org/k8s/clusters/local",
         "certificate-authority-data": "LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJoekNDQVM2Z0F3SUJBZ0lCQURBS0JnZ3Foa2pPUFFRREFqQTdNUnd3R2dZRFZRUUtFeE5rZVc1aGJXbGoKYkdsemRHVnVaWEl0YjNKbk1Sc3dHUVlEVlFRREV4SmtlVzVoYldsamJHbHpkR1Z1WlhJdFkyRXdIaGNOTWpFdwpNekU0TURFeU9UTTVXaGNOTXpFd016RTJNREV5T1RNNVdqQTdNUnd3R2dZRFZRUUtFeE5rZVc1aGJXbGpiR2x6CmRHVnVaWEl0YjNKbk1Sc3dHUVlEVlFRREV4SmtlVzVoYldsamJHbHpkR1Z1WlhJdFkyRXdXVEFUQmdjcWhrak8KUFFJQkJnZ3Foa2pPUFFNQkJ3TkNBQVFSdEM1ZXEzWEZSSUY2alE1V1JjOXNhZGliU2NjaWxnVWh5S2x5VSttbgphNEY1WktLS05zQnQxT1dWcmpRbEFtYkRWdUVrejN1WkF2aGN0cnA2Zm5TOW95TXdJVEFPQmdOVkhROEJBZjhFCkJBTUNBcVF3RHdZRFZSMFRBUUgvQkFVd0F3RUIvekFLQmdncWhrak9QUVFEQWdOSEFEQkVBaUIyMEhEaTJhVzAKVnc2SVgvV25sT1VIelFvZm51UUsySlZlOEtha09rK2Z2d0lnS0FlOW9aQ2dNZVlnd0JLTERRWFI2VXNNR2V5ZwpUTEFzdlNQTW1DZjNGcEU9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0="
       }
     }
   + some elisp
     #+begin_src emacs-lisp
       (defun gjg/get-cluster-credentials ()
         "Oh yeah"
         (interactive)
         (let ((contexts (shell-command)

                )))
         (message "Well that was fun")
         )
     #+end_src
   + The contexts using org-babel
     #+name: gorto
     #+begin_src bash :results output table
       kubectl config get-contexts
     #+end_src

     #+RESULTS: gorto
     | CURRENT         | NAME             | CLUSTER          | AUTHINFO         | NAMESPACE |
     | *               | gg-harvester-lab | gg-harvester-lab | gg-harvester-lab |           |
     | gg-one-and-done | gg-one-and-done  | gg-one-and-done  |                  |           |
     | gg-rke-gcp      | gg-rke-gcp       | gg-rke-gcp       |                  |           |
     | rancher         | rancher          | rancher          |                  |           |

   + ohai
     #+begin_src emacs-lisp :var table=gorto
       (completing-read "Contexts: "
                        (mapcar (lambda (x) (nth 1 x)) table)
                        )
     #+end_src
   + A magic intermediate command
     #+begin_src bash
       HARVESTER_CONTEXT="gg-harvester-lab"
       HARVESTER_CLUSTER=`kubectl config view -o json|jq -r ".contexts[]|select(.name == \"${HARVESTER_CONTEXT}\") | .context.cluster"`
       HARVESTER_USER=`kubectl config view -o json|jq -r ".contexts[]|select(.name == \"${HARVESTER_CONTEXT}\") | .context.user"`
       HARVESTER_CLUSTER_CA=`kubectl config view --flatten -o json | jq -r ".clusters[] | select(.name == \"${HARVESTER_CLUSTER}\") | .cluster[\"certificate-authority-data\"]"`
       HARVESTER_URL=`kubectl config view --flatten -o json | jq -r ".clusters[] | select(.name == \"${HARVESTER_CLUSTER}\") | .cluster.server"`
       HARVESTER_USER_CERT=`kubectl config view --flatten -o json | jq -r ".users[] | select(.name == \"${HARVESTER_USER}\") | .user[\"client-certificate-data\"]"`
       HARVESTER_USER_KEY=`kubectl config view --flatten -o json | jq -r ".users[] | select(.name == \"${HARVESTER_USER}\") | .user[\"client-key-data\"]"`
       docker-machine create -d harvester \
                      --ca-cert ${HARVESTER_CLUSTER_CA} \
                      --cert ${HARVESTER_USER_CERT} \
                      --key  ${HARVESTER_USER_KEY} \
                      --harvester-url ${HARVESTER_URL} \
                      --cpus 3 \
                      --disk-size "42Gi" \
                      --keypair "k8s-local" \
                      --ssh-key-path "~/.ssh/k8s-local.pub" \
                      --ssh-user "gregj" \
                      --mem-size "4Gi" \
                      --vm-description "This is a hoooooot" \
                      --vm-name "gg-whatahoot" \
                      --image-id "image-m2d7r" \
                      gorto
     #+end_src
* Create multi-VM bootable image in Harvester from start to finish
** Install from =.iso= using Ubuntu
   Image: [[http://archive.ubuntu.com/ubuntu/dists/focal/main/installer-amd64/current/legacy-images/netboot/mini.iso][ubuntu-mini 20.04]]
   + Do the needful to install the basic server in Harvester (just SSH server)
   + Virtual Machines -> Create
     + Name: gg-ubuntu-4-k8s
     + Description: Minimal boot disk ready for RKE usage
     + Use VM Template: iso-image-base-template
     + 1 CPU / 2GB RAM (just for building)
     + Image: ubuntu-20-04-mini.iso
     + SSHKey: k8s-local
     + Volumes:
       + cdrom-disk - boot order 1
       + Root disk: Size 42GB, boot order 2
     + defaults for the rest
   + Watch events from UI or CLI
     #+begin_src bash
       k get ev -w
     #+end_src
   + When running, connect to console and complete the OS install
     + =gregj= user
     + no auto updates
     + install OpenSSH server
   + After install, if the VM boots back into the CDROM menu - eject CD-ROM from Harvester
** Prepare the installed OS
   + Log in to the VM
   + You may have to hit =Alt-F1= to get to a TTY
   + Run through all the steps for prep in [[https://octetz.com/docs/2020/2020-10-19-machine-images/][Preparing Machine Images for qemu/KVM | octetz]]
     + I SSHed into the VM by multi-hop from the master node
     #+begin_src bash
       sudo apt update
       sudo apt -y install docker.io
       cat <<EOF | sudo tee /etc/docker/daemon.json
       {
         "exec-opts": ["native.cgroupdriver=systemd"],
         "log-driver": "json-file",
         "log-opts": {
           "max-size": "100m"
         },
         "storage-driver": "overlay2"
       }
       EOF
       sudo mkdir -p /etc/systemd/system/docker.service.d
       sudo usermod -G docker -a gregj
       sudo systemctl daemon-reload
       sudo systemctl restart docker
       sudo systemctl enable docker
     #+end_src
   + Disable swap in =/etc/fstab= after =sudo swapoff /swapfile=
** Prepare the image for cloning
   Again following steps in [[https://octetz.com/docs/2020/2020-10-19-machine-images/][Preparing Machine Images for qemu/KVM | octetz]]

   + Flush the contents of =/etc/machine-id=
     #+begin_src bash
       echo -n | sudo tee /etc/machine-id
     #+end_src
   + Deal with hostname
     [ ] Actually, don't - I think Harvester populates hostname - let's find out
     - but *do* remove =/etc/hostname=
       #+begin_src bash
         sudo rm -v /etc/hostname
       #+end_src
   + Clean up =.bash_history=
     #+begin_src bash
       cat /dev/null > ~/.bash_history && history -c && exit
     #+end_src
   + Power off the host from Harvester (since we exited our shell!)

** Save the root disk image to a path on your workstation
   + Find the Volume in Harvester: gg-ubuntu-4-k8s-rootdisk-3xt9y
     From CLI:
     #+begin_src bash
       k -n longhorn-system get volumes.longhorn.io \
         -o custom-columns=NAME:.metadata.name,STATE:.status.state,SIZE:.spec.size,USED:.status.actualSize,PVCNAME:.status.kubernetesStatus.pvcName
     #+end_src
   + Get the Minio URL storing the PVC
     #+begin_src bash
       k get pvc gg-ubuntu-4-k8s-rootdisk-3xt9y \
         -o custom-columns=NAME:.metadata.name,ENDPOINT:.metadata.annotations.cdi\\.kubevirt\\.io/storage\\.import\\.endpoint
     #+end_src

** Save a Longhorn backup!
   Ref: [[https://longhorn.io/docs/1.0.0/advanced-resources/data-recovery/recover-without-system/][Recovering from a Longhorn Backup without System Installed | Documentation]]
   In =longhorn-system= deploy the Pod to do the save
   #+begin_src yaml
     apiVersion: v1
     kind: Pod
     metadata:
       name: restore-to-file
       namespace: longhorn-system
     spec:
       # nodeName: <NODE_NAME>
       containers:
       - name: restore-to-file
         command:
         # set restore-to-file arguments here
         - /bin/sh
         - -c
         - longhorn backup restore-to-file
           's3://backups@us-east-1/?backup=backup-93af5ede3f8b4ca6&volume=pvc-3d37f78c-2c4b-4c3a-ac2e-69732291dae2'
           --output-file '/tmp/restore/gg-ubuntu-4-k8s-rootdisk.qcow2'
           --output-format qcow2
         # the version of longhorn engine should be v0.4.1 or higher
         image: rancher/longhorn-engine:v0.4.1
         imagePullPolicy: IfNotPresent
         securityContext:
           privileged: true
         volumeMounts:
         - name: disk-directory
           mountPath: /tmp/restore  # the argument <output-file> should be in this directory
         env:
         # set Backup Target Credential Secret here.
         - name: AWS_ACCESS_KEY_ID
           valueFrom:
             secretKeyRef:
               name: harvester-minio
               key: AWS_ACCESS_KEY_ID
         - name: AWS_SECRET_ACCESS_KEY
           valueFrom:
             secretKeyRef:
               name: harvester-minio
               key: AWS_SECRET_ACCESS_KEY
         - name: AWS_ENDPOINTS
           valueFrom:
             secretKeyRef:
               name: harvester-minio
               key: AWS_ENDPOINTS
       volumes:
         # the output file can be found on this host path
         - name: disk-directory
           hostPath:
             path: /tmp/restore
       restartPolicy: Never
   #+end_src
* Virtualization - prepare bootable image using QEMU
  Ref: [[https://docs.openstack.org/image-guide/centos-image.html][OpenStack Docs: Example: CentOS image]]

  #+begin_src bash
    qemu-img create -f qcow2 ./centos.qcow2 10g

    sudo virsh net-start default
    sudo virt-install --virt-type kvm --name centos --ram 1024 \
                --disk ./centos.qcow2,format=qcow2 \
                --network network=default \
                --graphics vnc,listen=0.0.0.0 --noautoconsole \
                --os-type=linux --os-variant=centos7.0 \
                --location=/data/data-files/isos/CentOS-7-x86_64-Minimal-1611.iso
  #+end_src
  + Install OS
  #+begin_src bash
    sudo virsh dumpxml centos |grep cdrom -A 5
    sudo virsh attach-disk --type cdrom --mode readonly centos "" sda
    sudo virsh reboot centos
  #+end_src
* Issues
** Submitted / in progress
*** [[https://github.com/rancher/harvester/issues/646][#646 {BUG} Specifying an internet proxy during installation of agent node makes node unusable]]
*** [[https://github.com/rancher/harvester/issues/647][#647 {BUG} UI Reports total available on root file systems, not total Longhorn storage]]
** Cooking
*** Add grouping label to instance when creating multiple instances of VM
    + Use =Name/Prefix= as value
*** CLI get settings should show value
    + Currently, =kubectl get settings.harvester.cattle.io= shows only NAME and AGE
      + it would be useful to show =.value= also for each settings
      + This would make it work the same as =get settings.longhorn.io=
* Mikhail's app mode tutorial
  + [[https://www.suse.com/c/meet-harvester-an-hci-solution-for-the-edge-src/][Meet Harvester, an HCI Solution for the Edge | SUSE Communities]]
