#+options: ':nil *:t -:t ::t <:t H:3 \n:nil ^:nil arch:headline author:t broken-links:nil
#+options: c:nil creator:nil d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t num:t
#+options: p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t timestamp:t title:t toc:t
#+options: todo:t |:t
#+title: harvester-readme
#+date: <2021-02-08 Mon>
#+author: Gregory Grubbs
#+email: gregory@dynapse.com
#+language: en
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 28.0.50 (Org mode 9.4.4)
#+setupfile: ~/projects/emacs/org-html-themes/org/theme-readtheorg-local.setup

   Official site: [[https://harvesterhci.io/][Harvester - Open-source hyperconverged infrastructure]]

* Harvester notes
** Latest dev =.iso=
   + https://releases.rancher.com/harvester/master/harvester-amd64.iso
** Installation considerations
   + Size of Minio buckets
     The default is 20Gi which iw way too small
     Consider setting =--set helm.persistence.size=200Gi=
** My GitHub issues
   + [[https://github.com/rancher/harvester/issues/646][rancher/harvester#646 {BUG} Specifying an internet proxy during installtion o...]]
** Getting details about vms
   + Jsonpath for getting the CD image IDs
     #+begin_src bash
       kubectl -n default get vm \
               -o jsonpath='{.items[*].spec.dataVolumeTemplates[0].metadata.annotations.harvester\.cattle\.io/imageId}' ; echo
     #+end_src


   + Then use that to find the image used to create the VM
     #+begin_src bash
       kubectl -n default get vmimage image-q8l5g \
         -o custom-columns=NAME:.metadata.name,DISPLAY_NAME:.spec.displayName
     #+end_src
   + Info for the running VM instances
     #+begin_src bash :results table replace
       kubectl -n default get virtualmachineinstances \
               -o custom-columns=NAME:.metadata.name,CPU:.spec.domain.cpu.cores,MAC:.status.interfaces[].mac,IP:.status.interfaces[].ipAddress,HOSTNAME:.spec.hostname,NETWORK:.spec.networks[].name,DISK1:.spec.volumes[0].dataVolume.name
     #+end_src

   + Simpler list of hostnames and IPs, just running =k get pod=
     #+begin_src bash :results table replace
       kubectl get pods -l kubevirt.io=virt-launcher \
         -o custom-columns=LAUNCHER_NAME:.metadata.name,HOSTNAME:.spec.hostname,IP:.status.podIP,STATUS:.status.phase \
         --sort-by=.spec.hostname
     #+end_src

** Setting root password on a =.qcow2= image

   + Install the basics - we're looking for =virsh= and =virt-customize=
     #+begin_src bash
       sudo zypper install guestfs-tools libvirt libvirt-client qemu-tools
     #+end_src
   + Set root password - make it random, since setting it explicitly is apparently harder
     to specify than it appears.  Note down what the generated random password is!
     #+begin_src bash
       sudo virt-customize \
            -a root-openSUSE-Leap-42.1-OpenStack.x86_64-0.0.4-Build2.225.qcow2 \
            --root-password random
     #+end_src
   + Once the new image is booted, login as root in order to:
     1. Change the root password to something sane
     2. Create a new user
        #+begin_src bash
          # (as root user)
          passwd
          useradd -m -s /bin/bash -g users -G docker,wheel gregj
          passwd gregj
        #+end_src
   +

** Getting access to Minio
   + Grab the minio client =mc= from [[https://dl.min.io/client/mc/release/linux-amd64/mc][here]]
   + Get the Access+Secret keys from =harvester-system= secret =minio=
     + If unmodified, they're YOURACCESSKEY/ YOURSECRETKEY
   + Configure and test the client, possibly in a container shell on the cluster
     #+begin_src bash
       mc alias set harvester http://minio.harvester-system:9000 YOURACCESSKEY YOURSECRETKEY
       mc ls harvester
     #+end_src

** Show/Change number of replicas on Longhorn volumes
   #+begin_src bash
     kubectl get volumes.longhorn.io -o custom-columns="NAME:metadata.name,STATE:status.state,NUMREPLICAS:spec.numberOfReplicas,ROBUSTNESS:status.robustness,SIZE:spec.size,ACTUAL:status.actualSize,NODE:status.currentNodeID"
   #+end_src

   #+begin_src bash
     kubectl -n longhorn-system patch  volume pvc-057fd8cd-6e7a-43e0-b9d3-0ebb8a34b838 --type merge -p '{"spec":{"numberOfReplicas": 2}}'
   #+end_src

** Grabbing volumes from Longhorn
   + Get Longhorn volumes with info of disk size, actual used size and PVC name
     #+begin_src bash
       k -n longhorn-system get volumes.longhorn.io \
         -o custom-columns=NAME:.metadata.name,STATE:.status.state,SIZE:.spec.size,USED:.status.actualSize,PVCNAME:.status.kubernetesStatus.pvcName
     #+end_src
   + Get Minio URL for Harvester
     #+begin_src bash
       k get pvc \
         -o custom-columns=NAME:.metadata.name,ENDPOINT:.metadata.annotations.cdi\\.kubevirt\\.io/storage\\.import\\.endpoint
     #+end_src
   + Copy disk image
     #+begin_src bash
       mc cp  harvester/vm-images/image-pj8k8 .
     #+end_src
   + Convert to =.qcow2= format
     #+begin_src bash
       qemu-img convert -f raw -O qcow2 image-pj8k8 image-pj8k8.qcow2
     #+end_src

** Prepare machine image for QEMU/KVM
   ref: [[https://octetz.com/docs/2020/2020-10-19-machine-images/][Preparing Machine Images for qemu/KVM | octetz]]
   For use with Kubernetes - installing and configuring Docker set us up for use with RKE
   + install Docker
   + configure Docker
     #+begin_src bash
       cat <<EOF | sudo tee /etc/docker/daemon.json
       {
         "exec-opts": ["native.cgroupdriver=systemd"],
         "log-driver": "json-file",
         "log-opts": {
           "max-size": "100m"
         },
         "storage-driver": "overlay2"
       }
       EOF
       sudo mkdir -p /etc/systemd/system/docker.service.d
       sudo usermod -G docker -a gregj
       sudo systemctl daemon-reload
       sudo systemctl restart docker
       sudo systemctl enable docker
     #+end_src
   + Disable sw2ap if enabled
** Prepare image for cloning
   + Get rid of machine ID so that it will be generated anew for each clone
     #+begin_src bash
       echo -n | sudo tee /etc/machine-id
     #+end_src
   + Host name - give a random one if none set
     Place in /usr/local/bin
     #+begin_src bash
       #!/bin/sh
       SN="hostname-init"

       # do nothing if /etc/hostname exists
       if [ -f "/etc/hostname" ]; then
         echo "${SN}: /etc/hostname exists; noop"
         exit
       fi

       echo "${SN}: creating hostname"

       # set hostname
       HN=$(head -60 /dev/urandom | tr -dc 'a-z' | fold -w 3 | head -n 1)
       echo ${HN} > /etc/hostname
       echo "${SN}: hostname (${HN}) created"

       # sort of dangerous, but works.
       if [ -f "/etc/hostname" ]; then
         /sbin/reboot
       fi
     #+end_src
   + Make executable
     #+begin_src bash
       sudo chmod a+rx /usr/local/bin/hostname-init.sh
     #+end_src
   + Add to =/etc/systemd/system/hostname-init.service=
     #+begin_src conf
       [Unit]
       Description=Set a random hostname.
       ConditionPathExists=!/etc/hostname

       [Service]
       ExecStart=/usr/local/bin/hostname-init.sh

       [Install]
       WantedBy=multi-user.target
     #+end_src
     #+begin_src bash
       sudo chmod 644 /etc/systemd/system/hostname-init.service
       sudo systemctl enable hostname-init
     #+end_src
     #+begin_src bash
       sudo rm -v /etc/hostname
     #+end_src
   + Clean up things like =.bash_history=
     #+begin_src bash
       cat /dev/null > ~/.bash_history && history -c && exit
     #+end_src
** Using cloud-config
   + Remember to literally start the config with =#cloud-config=
     #+begin_src yaml
       #cloud-config
       groups:
         - sudo
         - wheel
       ssh_authorized_keys:
         - >
           ssh-rsa
           AAAAB3NzaC1yc2EAAAADAQABAAABgQDIyhdKNeZnl0+nm5ApMVXCjbSvU/dEtFCU2+32GYBzyw6d8OtH7zs219/0ebsGpzPyIPcltWG/hn93A19feT7h/iZ0ZOl+TpdzvK0ExiEaqolZgiLavKcZyG6pVenfg7OF8HhI47XmjzgeVlFCP818TJF/LyA+eJGHumetAi+w7N34JVGz71gZridii1oWeNbzTC6oouBxZIu4+IVANnyTKYwMRzGdd7/iyOyJ1nvO88uedyD/KEZ4ow4tD1OOZE74VepxhbSEDjPu2Z++KQWQ7Ohjy5DZ8WagF/rgTbP0+wGX6AJSZ3S0p1+iXVjgYPx1kw8pPB1Ay7nLGHsgSSreBzxmiyX5rFmj1LtDti+Cy1m2tdnF+bFDT361j0JRgBGqD1R6AE2xCyizHgO6wLmgSzob0y7FzzafrIjFu64QkfAKuzJLXK02j1MdNPTlIuOc9vw6iKkIh3g4N55uPwyCOfhVZDPUgnmq1UPFr3hK5rHyQEpgeVOy5cia303au88=
           iamk8s-local
       users:
         - default
         - name: gregj
           groups: users, sudo, wheel
           sudo: ['ALL=(ALL) NOPASSWD:ALL']
           # password: >-
           #   ## mkpasswd --method=SHA-512 --rounds=4096
           ssh_authorized_keys:
             - >
               ssh-rsa
               AAAAB3NzaC1yc2EAAAADAQABAAABgQDIyhdKNeZnl0+nm5ApMVXCjbSvU/dEtFCU2+32GYBzyw6d8OtH7zs219/0ebsGpzPyIPcltWG/hn93A19feT7h/iZ0ZOl+TpdzvK0ExiEaqolZgiLavKcZyG6pVenfg7OF8HhI47XmjzgeVlFCP818TJF/LyA+eJGHumetAi+w7N34JVGz71gZridii1oWeNbzTC6oouBxZIu4+IVANnyTKYwMRzGdd7/iyOyJ1nvO88uedyD/KEZ4ow4tD1OOZE74VepxhbSEDjPu2Z++KQWQ7Ohjy5DZ8WagF/rgTbP0+wGX6AJSZ3S0p1+iXVjgYPx1kw8pPB1Ay7nLGHsgSSreBzxmiyX5rFmj1LtDti+Cy1m2tdnF+bFDT361j0JRgBGqD1R6AE2xCyizHgO6wLmgSzob0y7FzzafrIjFu64QkfAKuzJLXK02j1MdNPTlIuOc9vw6iKkIh3g4N55uPwyCOfhVZDPUgnmq1UPFr3hK5rHyQEpgeVOy5cia303au88=
     #+end_src
** Getting a cluster running in the cluster
   One key thing is to assure there are not overlapping Pod network CIDR ranges
*** k3s
    #+begin_src bash
      #!/usr/bin/env bash

      CONTEXT_NAME=inception
      LOCAL_DOMAIN=magichome
      dbuser=<FILLIN>
      dbpass=<FILLIN>
      export SERVER1_HOST=10.42.0.132
      export AGENT1_HOST=10.42.0.130
      export AGENT2_HOST=10.42.0.131
      export DATASTORE="mysql://${dbuser}:${dbpass}@tcp(<FILLIN>.${LOCAL_DOMAIN})/k3sup"

      # install the first server
      k3sup install --ip ${SERVER1_HOST} \
            --user gregj \
            --ssh-key ~/.ssh/k8s-local \
            --datastore ${DATASTORE} \
            --context ${CONTEXT_NAME} \
            --no-extras \
            --k3s-extra-args '--cluster-cidr 10.52.0.0/16 --service-cidr 10.53.0.0/16 --cluster-dns 10.53.0.10' \
            --print-command

      # Join the agents - to either server
      k3sup join --ip ${AGENT1_HOST} \
            --user gregj \
            --ssh-key ~/.ssh/k8s-local \
            --server-ip ${SERVER1_HOST} \
            --print-command


      k3sup join --ip ${AGENT2_HOST} \
            --user gregj \
            --ssh-key ~/.ssh/k8s-local \
            --server-ip ${SERVER1_HOST} \
            --print-command
    #+end_src
* Mohammed's Harvester Node driver
   AKA node-driver NodeDriver

  + [[https://github.com/belgaied2/docker-machine-driver-harvester/releases/][Releases · belgaied2/docker-machine-driver-harvester · GitHub]]
    All you need to do is to :
    + [[https://docs.docker.com/machine/install-machine/][Install Docker Machine]]: using brew or downloading the version for linux
    + download the [[https://github.com/belgaied2/docker-machine-driver-harvester/releases/][harvester driver]]
    + give the file execution rights and put in your PATH
    + then, run =docker-machine create -d harvester --help=
  The Driver will need Kubeconfig information (CA CERT, CLIENT CERT and CLIENT KEY + K3OS
  Kube proxy URL) and will create the VMs using the Kubernetes objects for
  KubeVirt/Harvester.

  #+begin_src bash
    docker-machine create -d harvester \
                   --ca-cert "LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJoekNDQVM2Z0F3SUJBZ0lCQURBS0JnZ3Foa2pPUFFRREFqQTdNUnd3R2dZRFZRUUtFeE5rZVc1aGJXbGoKYkdsemRHVnVaWEl0YjNKbk1Sc3dHUVlEVlFRREV4SmtlVzVoYldsamJHbHpkR1Z1WlhJdFkyRXdIaGNOTWpFdwpNekU0TURFeU9UTTVXaGNOTXpFd016RTJNREV5T1RNNVdqQTdNUnd3R2dZRFZRUUtFeE5rZVc1aGJXbGpiR2x6CmRHVnVaWEl0YjNKbk1Sc3dHUVlEVlFRREV4SmtlVzVoYldsamJHbHpkR1Z1WlhJdFkyRXdXVEFUQmdjcWhrak8KUFFJQkJnZ3Foa2pPUFFNQkJ3TkNBQVFSdEM1ZXEzWEZSSUY2alE1V1JjOXNhZGliU2NjaWxnVWh5S2x5VSttbgphNEY1WktLS05zQnQxT1dWcmpRbEFtYkRWdUVrejN1WkF2aGN0cnA2Zm5TOW95TXdJVEFPQmdOVkhROEJBZjhFCkJBTUNBcVF3RHdZRFZSMFRBUUgvQkFVd0F3RUIvekFLQmdncWhrak9QUVFEQWdOSEFEQkVBaUIyMEhEaTJhVzAKVnc2SVgvV25sT1VIelFvZm51UUsySlZlOEtha09rK2Z2d0lnS0FlOW9aQ2dNZVlnd0JLTERRWFI2VXNNR2V5ZwpUTEFzdlNQTW1DZjNGcEU9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0=" \
                   --cert "LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJrRENDQVRlZ0F3SUJBZ0lJRnhydW9NYXQ4VzB3Q2dZSUtvWkl6ajBFQXdJd0l6RWhNQjhHQTFVRUF3d1kKYXpOekxXTnNhV1Z1ZEMxallVQXhOakUwT0RFek56WTBNQjRYRFRJeE1ETXdNekl6TWpJME5Gb1hEVEl5TURNdwpNekl6TWpJME5Gb3dNREVYTUJVR0ExVUVDaE1PYzNsemRHVnRPbTFoYzNSbGNuTXhGVEFUQmdOVkJBTVRESE41CmMzUmxiVHBoWkcxcGJqQlpNQk1HQnlxR1NNNDlBZ0VHQ0NxR1NNNDlBd0VIQTBJQUJNQWY2Rldvbm1OLzlRM3EKZE93ZGRCeXpDS0tGZi9yeFVTUDVLRFRqcjdkRE1pUStDM0Q2QWw5RGxsR2RBYkdMOFZveDB3TisrMnRWTWlRRQpvMmZ1YjkralNEQkdNQTRHQTFVZER3RUIvd1FFQXdJRm9EQVRCZ05WSFNVRUREQUtCZ2dyQmdFRkJRY0RBakFmCkJnTlZIU01FR0RBV2dCVEJuUmFvOVFiWVJUTG5QYk9kNjZsdWNYUFhuREFLQmdncWhrak9QUVFEQWdOSEFEQkUKQWlBdi9OaURpMlJGbGRrMzMrQlZRZHhyYmdrdGJRRUUxeXJSMHVnSE5hWmRkUUlnUm0rVTAzVUZrelhLdFVGYgpmQUtQYyt6cmFYRm8zY1FmbEpaQUNQR0RSUW89Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0KLS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJkekNDQVIyZ0F3SUJBZ0lCQURBS0JnZ3Foa2pPUFFRREFqQWpNU0V3SHdZRFZRUUREQmhyTTNNdFkyeHAKWlc1MExXTmhRREUyTVRRNE1UTTNOalF3SGhjTk1qRXdNekF6TWpNeU1qUTBXaGNOTXpFd016QXhNak15TWpRMApXakFqTVNFd0h3WURWUVFEREJock0zTXRZMnhwWlc1MExXTmhRREUyTVRRNE1UTTNOalF3V1RBVEJnY3Foa2pPClBRSUJCZ2dxaGtqT1BRTUJCd05DQUFTdUl5QzdCMFJ5Q2Ixejd1UlJZdzV5QXJ6ci9uYjA1ZTNPLzBzWUhDYnUKOEh6aUlIenZnUkZqTG4xV0ZEd0FnVVEzU2xjUWZpRUN5SmY3UkU0ME9EL1lvMEl3UURBT0JnTlZIUThCQWY4RQpCQU1DQXFRd0R3WURWUjBUQVFIL0JBVXdBd0VCL3pBZEJnTlZIUTRFRmdRVXdaMFdxUFVHMkVVeTV6MnpuZXVwCmJuRnoxNXd3Q2dZSUtvWkl6ajBFQXdJRFNBQXdSUUlnVjNrQlEwc0lpY2dQclFaTXRUVllBZW9OYnU0VGRiYXMKVUR2b3lCZHdpWWtDSVFDSkROUDlqWDZqWnpPbXRKdDlIaUVoS1YrLzdzOVNGeTBsc0xxdGVlWWdrQT09Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K" \
                   --key "LS0tLS1CRUdJTiBFQyBQUklWQVRFIEtFWS0tLS0tCk1IY0NBUUVFSURjelVCU3huY1BhVFA5VUVXcjZaMy9BUk1WUUdQUGl2NE80ZHNKejZSbWpvQW9HQ0NxR1NNNDkKQXdFSG9VUURRZ0FFd0Ivb1ZhaWVZMy8xRGVwMDdCMTBITE1Jb29WLyt2RlJJL2tvTk9PdnQwTXlKRDRMY1BvQwpYME9XVVowQnNZdnhXakhUQTM3N2ExVXlKQVNqWis1djN3PT0KLS0tLS1FTkQgRUMgUFJJVkFURSBLRVktLS0tLQo=" \
                   --harvester-url "https://172.16.17.16:6443" \
                   --cpus 3 \
                   --disk-size "42Gi" \
                   --keypair "k8s-local" \
                   --ssh-key-path "~/.ssh/id_rsa.pub" \
                   --ssh-user "gregj" \
                   --mem-size "4Gi" \
                   --vm-description "This is a hoooooot" \
                   --vm-name "gg-whatahoot" \
                   --image-id "image-mfz99" \
                   gorto
  #+end_src
** Work getting the certs from KUBECONFIG
   + Get the contexts to present to the user
      #+begin_src bash
        k config get-contexts -o name
      #+end_src
   + get cluster and user from selected context (e.g. =rancher=)
     #+begin_src bash :results output raw
       kubectl config view -o json|jq -r '.contexts[] | select(.name == "rancher") | .context'
     #+end_src
     {
       "cluster": "rancher",
       "user": "rancher"
     }
   + get cluster CA
     #+begin_src bash :results output raw
       kubectl config view --flatten -o json | jq -r '.clusters[] | select(.name == "rancher")' | sed 's,LS0.\+,blah-blah,'
     #+end_src
     {
       "name": "rancher",
       "cluster": {
         "server": "https://rancher.hypecyclist.org/k8s/clusters/local",
         "certificate-authority-data": "blah-blah
       }
     }
     {
       "name": "rancher",
       "cluster": {
         "server": "https://rancher.hypecyclist.org/k8s/clusters/local",
         "certificate-authority-data": "LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUJoekNDQVM2Z0F3SUJBZ0lCQURBS0JnZ3Foa2pPUFFRREFqQTdNUnd3R2dZRFZRUUtFeE5rZVc1aGJXbGoKYkdsemRHVnVaWEl0YjNKbk1Sc3dHUVlEVlFRREV4SmtlVzVoYldsamJHbHpkR1Z1WlhJdFkyRXdIaGNOTWpFdwpNekU0TURFeU9UTTVXaGNOTXpFd016RTJNREV5T1RNNVdqQTdNUnd3R2dZRFZRUUtFeE5rZVc1aGJXbGpiR2x6CmRHVnVaWEl0YjNKbk1Sc3dHUVlEVlFRREV4SmtlVzVoYldsamJHbHpkR1Z1WlhJdFkyRXdXVEFUQmdjcWhrak8KUFFJQkJnZ3Foa2pPUFFNQkJ3TkNBQVFSdEM1ZXEzWEZSSUY2alE1V1JjOXNhZGliU2NjaWxnVWh5S2x5VSttbgphNEY1WktLS05zQnQxT1dWcmpRbEFtYkRWdUVrejN1WkF2aGN0cnA2Zm5TOW95TXdJVEFPQmdOVkhROEJBZjhFCkJBTUNBcVF3RHdZRFZSMFRBUUgvQkFVd0F3RUIvekFLQmdncWhrak9QUVFEQWdOSEFEQkVBaUIyMEhEaTJhVzAKVnc2SVgvV25sT1VIelFvZm51UUsySlZlOEtha09rK2Z2d0lnS0FlOW9aQ2dNZVlnd0JLTERRWFI2VXNNR2V5ZwpUTEFzdlNQTW1DZjNGcEU9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0="
       }
     }
   + some elisp
     #+begin_src emacs-lisp
       (defun gjg/get-cluster-credentials ()
         "Oh yeah"
         (interactive)
         (let ((contexts (shell-command)

                )))
         (message "Well that was fun")
         )
     #+end_src
   + The contexts using org-babel
     #+name: gorto
     #+begin_src bash :results output table
       kubectl config get-contexts
     #+end_src

   + ohai
     #+begin_src emacs-lisp :var table=gorto
       (completing-read "Contexts: "
                        (mapcar (lambda (x) (nth 1 x)) table)
                        )
     #+end_src
   + A magic intermediate command
     #+begin_src bash
       export HARVESTER_CONTEXT="protomolecule"
       export HARVESTER_CLUSTER=`kubectl config view -o json|jq -r ".contexts[]|select(.name == \"${HARVESTER_CONTEXT}\") | .context.cluster"`
       export HARVESTER_USER=`kubectl config view -o json|jq -r ".contexts[]|select(.name == \"${HARVESTER_CONTEXT}\") | .context.user"`
       export HARVESTER_CLUSTER_CA=`kubectl config view --flatten -o json | jq -r ".clusters[] | select(.name == \"${HARVESTER_CLUSTER}\") | .cluster[\"certificate-authority-data\"]"`
       export HARVESTER_URL=`kubectl config view --flatten -o json | jq -r ".clusters[] | select(.name == \"${HARVESTER_CLUSTER}\") | .cluster.server"`
       export HARVESTER_USER_CERT=`kubectl config view --flatten -o json | jq -r ".users[] | select(.name == \"${HARVESTER_USER}\") | .user[\"client-certificate-data\"]"`
       export HARVESTER_USER_KEY=`kubectl config view --flatten -o json | jq -r ".users[] | select(.name == \"${HARVESTER_USER}\") | .user[\"client-key-data\"]"`
       docker-machine create -d harvester \
                      --ca-cert ${HARVESTER_CLUSTER_CA} \
                      --cert ${HARVESTER_USER_CERT} \
                      --key  ${HARVESTER_USER_KEY} \
                      --harvester-url ${HARVESTER_URL} \
                      --cpus 3 \
                      --disk-size "42Gi" \
                      --keypair "k8s-local" \
                      --ssh-key-path "~/.ssh/k8s-local.pub" \
                      --ssh-user "gregj" \
                      --mem-size "4Gi" \
                      --vm-description "This is a hoooooot" \
                      --vm-name "gg-whatahoot" \
                      --image-id "image-m2d7r" \
                      gorto
     #+end_src
* The official node driver
  #+begin_src bash
    docker-machine create -d harvester \
                   --harvester-username admin \
                   --harvester-password=${HARVESTER_PASSWORD} \
                   --harvester-image-name image-opensuse-leap-15sp2.qcow2 \
                   --harvester-network-name vlan \
                   --harvester-host protomolecule.magichome \
                   --harvester-namespace default \
                   --harvester-port 30443 \
                   mynewvm
  #+end_src
* Create multi-VM bootable image in Harvester from start to finish
** Install from =.iso= using Ubuntu
   Image: [[http://archive.ubuntu.com/ubuntu/dists/focal/main/installer-amd64/current/legacy-images/netboot/mini.iso][ubuntu-mini 20.04]]
   + Do the needful to install the basic server in Harvester (just SSH server)
   + Virtual Machines -> Create
     + Name: gg-ubuntu-4-k8s
     + Description: Minimal boot disk ready for RKE usage
     + Use VM Template: iso-image-base-template
     + 1 CPU / 2GB RAM (just for building)
     + Image: ubuntu-20-04-mini.iso
     + SSHKey: k8s-local
     + Volumes:
       + cdrom-disk - boot order 1
       + Root disk: Size 42GB, boot order 2
     + defaults for the rest
   + Watch events from UI or CLI
     #+begin_src bash
       k get ev -w
     #+end_src
   + When running, connect to console and complete the OS install
     + =gregj= user
     + no auto updates
     + install OpenSSH server
   + After install, if the VM boots back into the CDROM menu - eject CD-ROM from Harvester
** Prepare the installed OS
   + Log in to the VM
   + You may have to hit =Alt-F1= to get to a TTY
   + Run through all the steps for prep in [[https://octetz.com/docs/2020/2020-10-19-machine-images/][Preparing Machine Images for qemu/KVM | octetz]]
     + I SSHed into the VM by multi-hop from the master node
     #+begin_src bash
       sudo apt update
       sudo apt -y install docker.io
       cat <<EOF | sudo tee /etc/docker/daemon.json
       {
         "exec-opts": ["native.cgroupdriver=systemd"],
         "log-driver": "json-file",
         "log-opts": {
           "max-size": "100m"
         },
         "storage-driver": "overlay2"
       }
       EOF
       sudo mkdir -p /etc/systemd/system/docker.service.d
       sudo usermod -G docker -a gregj
       sudo systemctl daemon-reload
       sudo systemctl restart docker
       sudo systemctl enable docker
     #+end_src
   + Disable swap in =/etc/fstab= after =sudo swapoff /swapfile=
** Prepare the image for cloning
   Again following steps in [[https://octetz.com/docs/2020/2020-10-19-machine-images/][Preparing Machine Images for qemu/KVM | octetz]]

   + Flush the contents of =/etc/machine-id=
     #+begin_src bash
       echo -n | sudo tee /etc/machine-id
     #+end_src
   + Deal with hostname (by removing it)
     + Harvester populates hostname using cloud-init
     + but *do* remove =/etc/hostname=
       #+begin_src bash
         sudo rm -v /etc/hostname
       #+end_src
   + Clean up =.bash_history=
     #+begin_src bash
       cat /dev/null > ~/.bash_history && history -c && exit
     #+end_src
   + Power off the host from Harvester (since we exited our shell!)

** Save the root disk image to a path on your workstation
   + Find the Volume in Harvester: gg-ubuntu-4-k8s-rootdisk-3xt9y
     From CLI:
     #+begin_src bash
       k -n longhorn-system get volumes.longhorn.io \
         -o custom-columns=NAME:.metadata.name,STATE:.status.state,SIZE:.spec.size,USED:.status.actualSize,PVCNAME:.status.kubernetesStatus.pvcName
     #+end_src
   + Get the Minio URL storing the PVC
     #+begin_src bash
       k get pvc gg-ubuntu-4-k8s-rootdisk-3xt9y \
         -o custom-columns=NAME:.metadata.name,ENDPOINT:.metadata.annotations.cdi\\.kubevirt\\.io/storage\\.import\\.endpoint
     #+end_src

** Save a Longhorn backup!
   Ref: [[https://longhorn.io/docs/1.0.0/advanced-resources/data-recovery/recover-without-system/][Recovering from a Longhorn Backup without System Installed | Documentation]]
   In =longhorn-system= deploy the Pod to do the save
   #+begin_src yaml
     apiVersion: v1
     kind: Pod
     metadata:
       name: restore-to-file
       namespace: longhorn-system
     spec:
       # nodeName: <NODE_NAME>
       containers:
       - name: restore-to-file
         command:
         # set restore-to-file arguments here
         - /bin/sh
         - -c
         - longhorn backup restore-to-file
           's3://backups@us-east-1/?backup=backup-93af5ede3f8b4ca6&volume=pvc-3d37f78c-2c4b-4c3a-ac2e-69732291dae2'
           --output-file '/tmp/restore/gg-ubuntu-4-k8s-rootdisk.qcow2'
           --output-format qcow2
         # the version of longhorn engine should be v0.4.1 or higher
         image: rancher/longhorn-engine:v0.4.1
         imagePullPolicy: IfNotPresent
         securityContext:
           privileged: true
         volumeMounts:
         - name: disk-directory
           mountPath: /tmp/restore  # the argument <output-file> should be in this directory
         env:
         # set Backup Target Credential Secret here.
         - name: AWS_ACCESS_KEY_ID
           valueFrom:
             secretKeyRef:
               name: harvester-minio
               key: AWS_ACCESS_KEY_ID
         - name: AWS_SECRET_ACCESS_KEY
           valueFrom:
             secretKeyRef:
               name: harvester-minio
               key: AWS_SECRET_ACCESS_KEY
         - name: AWS_ENDPOINTS
           valueFrom:
             secretKeyRef:
               name: harvester-minio
               key: AWS_ENDPOINTS
       volumes:
         # the output file can be found on this host path
         - name: disk-directory
           hostPath:
             path: /tmp/restore
       restartPolicy: Never
   #+end_src
* Steps to install HA k3s on Harvester VMs
** TODO still need to automate
*** Figure out storage class to use with image

      #+begin_src bash
      kubectl get sc -o custom-columns=NAME:metadata.name,PROVISIONER:provisioner,BACKINGIMAGE:parameters.backingImage
      #+end_src

      #+begin_src bash
      kubectl get virtualmachineimages.harvester.cattle.io -o custom-columns=NAME:metadata.name,DISPLAY_NAME:spec.displayName
      #+end_src


** Initialize settings used for code execution and tangling
   + Set our cluster prefix / VM group name
   #+name: cluster-prefix
   #+begin_src text
   leapy
   #+end_src
   + Set namespace where the VMs will run
   #+name: vmnamespace
   #+begin_src text
   default
   #+end_src
   #+name: vmuser
      #+begin_src text
     gregj
   #+end_src
   + Figure out correct storage class


** Upload bootable image
   #+begin_src yaml :tangle /tmp/opensuse-leap-vmimage.yaml :noweb yes
     apiVersion: harvester.cattle.io/v1alpha1
     kind: VirtualMachineImage
     metadata:
       name: image-opensuse-leap-15sp2.qcow2
       generateName: image-opensuse-leap-15sp2.qcow2
       namespace: <<vmnamespace>>
     spec:
       displayName: image-opensuse-leap-15sp2.qcow2
       url: http://gorto.magichome/Linux/distros/opensuse/openSUSE-Leap-15.2.x86_64-NoCloud.qcow2
   #+end_src
   #+begin_src bash
     kubectl apply -f /tmp/opensuse-leap-vmimage.yaml
   #+end_src
   #+begin_src bash
     kubectl get sc -o custom-columns=NAME:metadata.name,PROVISIONER:provisioner,BACKINGIMAGE:parameters.backingImage,BACKINGURL:parameters.backingImageURL
   #+end_src

** In Harvester, create our cluster node VMs
   + Write our template - using Org tangle
     #+begin_src yaml :tangle /tmp/peeps-1.vm.yaml :noweb yes
       apiVersion: kubevirt.io/v1
       kind: VirtualMachine
       metadata:
         labels:
           harvester.cattle.io/creator: harvester
           vmgroup: <<cluster-prefix>>
         name: peeps-1
         namespace: <<vmnamespace>>
       spec:
         running: true
         dataVolumeTemplates:
         - apiVersion: cdi.kubevirt.io/v1beta1
           kind: DataVolume
           metadata:
             # annotations:
             #   harvester.cattle.io/imageId: vm/image-opensuse-leap-15sp2
            name: peeps-1-disk-0
           spec:
             pvc:
               accessModes:
               - ReadWriteMany
               resources:
                 requests:
                   storage: 10Gi
               storageClassName: longhorn-image-fnvgt
               volumeMode: Block
             source:
               blank: {}
         template:
           metadata:
             annotations:
               harvester.cattle.io/sshNames: '["k8s-local"]'
             labels:
               harvester.cattle.io/creator: harvester
               harvester.cattle.io/vmName: peeps-1
               vmgroup: peeps
           spec:
             domain:
               cpu:
                 cores: 4
                 sockets: 1
                 threads: 1
               devices:
                 disks:
                 - bootOrder: 1
                   disk:
                     bus: virtio
                   name: disk-0
                 - disk:
                     bus: virtio
                   name: cloudinitdisk
                 inputs:
                 - bus: usb
                   name: tablet
                   type: tablet
                 interfaces:
                 - bridge: {}
                   model: virtio
                   name: default
               machine:
                 type: q35
               resources:
                 requests:
                   memory: 4Gi
             evictionStrategy: LiveMigrate
             hostname: peeps-1
             subdomain: gorto
             networks:
             - name: default
               multus:
                 networkName: vlan1
             volumes:
             - dataVolume:
                 name: peeps-1-disk-0
               name: disk-0
             - cloudInitNoCloud:
                 userData: |
                   #cloud-config
                   groups:
                     - sudo
                     - wheel
                   ssh_authorized_keys:
                     - >
                       ssh-rsa
                       AAAAB3NzaC1yc2EAAAADAQABAAABgQDIyhdKNeZnl0+nm5ApMVXCjbSvU/dEtFCU2+32GYBzyw6d8OtH7zs219/0ebsGpzPyIPcltWG/hn93A19feT7h/iZ0ZOl+TpdzvK0ExiEaqolZgiLavKcZyG6pVenfg7OF8HhI47XmjzgeVlFCP818TJF/LyA+eJGHumetAi+w7N34JVGz71gZridii1oWeNbzTC6oouBxZIu4+IVANnyTKYwMRzGdd7/iyOyJ1nvO88uedyD/KEZ4ow4tD1OOZE74VepxhbSEDjPu2Z++KQWQ7Ohjy5DZ8WagF/rgTbP0+wGX6AJSZ3S0p1+iXVjgYPx1kw8pPB1Ay7nLGHsgSSreBzxmiyX5rFmj1LtDti+Cy1m2tdnF+bFDT361j0JRgBGqD1R6AE2xCyizHgO6wLmgSzob0y7FzzafrIjFu64QkfAKuzJLXK02j1MdNPTlIuOc9vw6iKkIh3g4N55uPwyCOfhVZDPUgnmq1UPFr3hK5rHyQEpgeVOy5cia303au88=
                       iamk8s-local
                   users:
                     - default
                     - name: gregj
                       groups: [users, sudo, wheel]
                       sudo:
                         - 'ALL=(ALL) NOPASSWD:ALL'
                       # password: '## mkpasswd --method=SHA-512 --rounds=4096'
                       ssh_authorized_keys:
                         - >
                           ssh-rsa
                           AAAAB3NzaC1yc2EAAAADAQABAAABgQDIyhdKNeZnl0+nm5ApMVXCjbSvU/dEtFCU2+32GYBzyw6d8OtH7zs219/0ebsGpzPyIPcltWG/hn93A19feT7h/iZ0ZOl+TpdzvK0ExiEaqolZgiLavKcZyG6pVenfg7OF8HhI47XmjzgeVlFCP818TJF/LyA+eJGHumetAi+w7N34JVGz71gZridii1oWeNbzTC6oouBxZIu4+IVANnyTKYwMRzGdd7/iyOyJ1nvO88uedyD/KEZ4ow4tD1OOZE74VepxhbSEDjPu2Z++KQWQ7Ohjy5DZ8WagF/rgTbP0+wGX6AJSZ3S0p1+iXVjgYPx1kw8pPB1Ay7nLGHsgSSreBzxmiyX5rFmj1LtDti+Cy1m2tdnF+bFDT361j0JRgBGqD1R6AE2xCyizHgO6wLmgSzob0y7FzzafrIjFu64QkfAKuzJLXK02j1MdNPTlIuOc9vw6iKkIh3g4N55uPwyCOfhVZDPUgnmq1UPFr3hK5rHyQEpgeVOy5cia303au88=
                   package_update: true
                   packages:
                     - qemu-guest-agent
                   runcmd:
                     - - systemctl
                       - enable
                       - '--now'
                       - qemu-guest-agent
               name: cloudinitdisk
     #+end_src
   + Create the set of VMs based on the cluster prefix
   #+begin_src bash  :results output :async no :noweb yes
     for i in $(seq 1 6); do
         cat /tmp/peeps-1.vm.yaml | sed "s,peeps-1,<<cluster-prefix>>-${i}," | kubectl apply -f -
     done
   #+end_src

   + Label our VM instancess with label key =vmgroup= (if required) and show status
   #+begin_src bash :results output table replace :noweb yes
     for i in $(kubectl -n <<vmnamespace>> get vmi -o name -l vmgroup!=<<cluster-prefix>>| grep <<cluster-prefix>>); do
         kubectl -n <<vmnamespace>> label --overwrite ${i} vmgroup=<<cluster-prefix>> > /dev/null
     done
     kubectl -n <<vmnamespace>> get vmi -l vmgroup=<<cluster-prefix>> --sort-by=metadata.name
   #+end_src



** Create node lists for k3s or rke AND start bastion container
   + Use my utils image with 'gregj' user
     #+begin_src bash :results none
       kubectl -n default run mysh --image=gregoryg/sh-net-utils -- sleep 24d 2&> /dev/null
     #+end_src

   + all-nodes, master-nodes, worker-nodes
     #+begin_src bash :noweb yes
       kubectl -n <<vmnamespace>> get vmi \
               -l vmgroup=<<cluster-prefix>> \
               -o custom-columns=IP:status.interfaces[0].ipAddress \
               --sort-by metadata.name \
           | tail -n +2 \
           | tee /tmp/all-nodes.txt
       head -3 /tmp/all-nodes.txt > /tmp/master-nodes.txt
       tail -n +4 /tmp/all-nodes.txt > /tmp/worker-nodes.txt
     #+end_src


** Open bastion container shell
   + open a shell to the bastion container
       #+begin_src emacs-lisp
         (cd "/kube:default@mysh:")
         (shell "bastion-sh")
       #+end_src
   + User will need to use SSH key to set up VM hosts with k3s
       #+begin_src bash :session bastion-sh
         mkdir -p ~/.ssh
       #+end_src

       #+begin_src emacs-lisp
         (copy-file "~/.ssh/k8s-local" "/kube:default@mysh:.ssh/")
         (copy-file "~/.ssh/k8s-local.pub" "/kube:default@mysh:.ssh/")
       #+end_src

   + copy the node files to the bastion
     #+begin_src emacs-lisp :results none
       (copy-file "/tmp/all-nodes.txt" "/kube:default@mysh:/tmp/" t)
       (copy-file "/tmp/master-nodes.txt" "/kube:default@mysh:/tmp/" t)
       (copy-file "/tmp/worker-nodes.txt" "/kube:default@mysh:/tmp/" t)
     #+end_src
   + Set up pre-requisites for k3s installation on bastion
     #+begin_src bash :session bastion-sh :noweb yes
       # Get the keys all settled in known_hosts
       cd /tmp
       for i in `cat all-nodes.txt`
       do
           ssh-keygen -R ${i}
           ssh -i ~/.ssh/k8s-local -o StrictHostKeyChecking=no  <<vmuser>>@${i} hostname
       done

         #+end_src

       + install k3sup on bastion
       #+begin_src bash :session bastion-sh
         curl -sLS https://get.k3sup.dev | sh
         sudo install k3sup /usr/local/bin/
         k3sup --help
   #+end_src

   #+RESULTS:

   + Generate cluster hosts file for the halibut
     #+begin_src bash :noweb yes :results output replace
       kubectl -n <<vmnamespace>> \
               get vmi \
               -l vmgroup=<<cluster-prefix>> \
               -o custom-columns=IP:status.interfaces[0].ipAddress,NAME:metadata.name \
               --sort-by=metadata.name \
       | tail -n +2
     #+end_src



   #+begin_src bash :session bastion-sh
     # install the first server
     k3sup install \
           --ip $(head -1 master-nodes.txt) \
           --user gregj \
           --ssh-key ~/.ssh/k8s-local \
           --cluster \
           --context k3s-vms \
           --k3s-channel v1.19 \
           --no-extras \
           --k3s-extra-args '--cluster-cidr 10.72.0.0/16 --service-cidr 10.73.0.0/16 --cluster-dns 10.73.0.10'

     # join the second server
     k3sup join \
           --ip $(sed '2q;d' master-ndes.txt) \
           --user gregj \
           --server-user gregj \
           --ssh-key ~/.ssh/k8s-local \
           --server-ip $(head -1 master-nodes.txt) \
           --server \
           --k3s-channel v1.19 \
           --k3s-extra-args '--cluster-cidr 10.72.0.0/16 --service-cidr 10.73.0.0/16 --cluster-dns 10.73.0.10'

     # join the third server
     k3sup join \
           --ip $(sed '3q;d' master-nodes.txt) \
           --user gregj \
           --server-user gregj \
           --ssh-key ~/.ssh/k8s-local \
           --server-ip $(head -1 master-nodes.txt) \
           --server \
           --k3s-channel v1.19
   #+end_src


* Prep to access k3os
  #+name: harvester-ingress-hostname
  |protomolecule|
  #+name: harvester-ingress-domain
  |magichome|
  #+name: harvester-ingress-ip
  |172.16.17.16|

  #+begin_src bash :session sh1 :var hhostname=harvester-ingress-hostname :var hdomain=harvester-ingress-domain :var hip=harvester-ingress-ip
    ssh-keygen -R ${hhostname}
    ssh-keygen -R ${hhostname}.${hdomain}
    ssh-keygen -R ${hip}
    cat ~/.ssh/k8s-local.pub | ssh -o StrictHostKeyChecking=no rancher@${hhostname} "cat - >> .ssh/authorized_keys"
    cat ~/.ssh/id_rsa.pub    | ssh -o StrictHostKeyChecking=no rancher@${hhostname}.${hdomain} "cat - >> .ssh/authorized_keys"
    ssh -o StrictHostKeyChecking=no ${hip} hostname

    echo "export PROMPT_DIRTRIM=2
    if [ -z \"${SSH_CONNECTION}\" ] ; then
        PS1=\"\u@\h \w $ \"
    else
        PS1=\"/ssh:\u@\h \w $ \"
    fi
    export PS1
    source <(kubectl completion bash)
    complete -F __start_kubectl k
    alias k='kubectl'
    alias kn='kubectl config set-context --current --namespace '
    alias kx='kubectl config get-contexts'
    " | ssh rancher@${hhostname} "cat - >> .bashrc"

    echo 'source ~/.bashrc' | ssh rancher@${hhostname} "cat - >> .bash_profile"

  #+end_src
  #+begin_src bash :session sh1 :var hhostname=harvester-ingress-hostname :var hdomain=harvester-ingress-domain :var hip=harvester-ingress-ip
    scp -o StrictHostKeyChecking=no rancher@${hhostname}:/etc/rancher/k3s/k3s.yaml gregj@glados:/tmp/
    sed -i.bak 's,default,protomolecule,g' /tmp/k3s.yaml
    sed -i.bak "s,127.0.0.1,${hip},g" /tmp/k3s.yaml
    kubectl config delete-cluster protomolecule
    kubectl config delete-context protomolecule
    kubectl config unset current-context
    # rename the user that remains
    sed -i.bak "s,protomolecule,pleaseDELETEME"
    KUBECONFIG=~/.kube/config:/tmp/k3s.yaml kubectl config view --flatten > /tmp/config
    mv -v /tmp/config ~/.kube/
    kubectl get nodes
  #+end_src

* Virtualization - prepare bootable image using QEMU
  Ref: [[https://docs.openstack.org/image-guide/centos-image.html][OpenStack Docs: Example: CentOS image]]

  #+begin_src bash
    qemu-img create -f qcow2 ./centos.qcow2 10g

    sudo virsh net-start default
    sudo virt-install --virt-type kvm --name centos --ram 1024 \
                --disk ./centos.qcow2,format=qcow2 \
                --network network=default \
                --graphics vnc,listen=0.0.0.0 --noautoconsole \
                --os-type=linux --os-variant=centos7.0 \
                --location=/data/data-files/isos/CentOS-7-x86_64-Minimal-1611.iso
  #+end_src
  + Install OS
  #+begin_src bash
    sudo virsh dumpxml centos |grep cdrom -A 5
    sudo virsh attach-disk --type cdrom --mode readonly centos "" sda
    sudo virsh reboot centos
  #+end_src
* Issues
** How to grab relevant logs
*** RKE/k3s
    #+begin_src bash
      wget -O- https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v2.x/logs-collector/rancher2_logs_collector.sh | sudo bash -s
    #+end_src
*** Longhorn
    + Use UI "Generate Support Bundle" button at bottom
    + Joshua Moody's gist:
    [[https://gist.github.com/joshimoo/cdf2995e5c517a74b588bab59dbf3a1f][how to create a longhorn support bundle when the ui is not working · GitHub]]
    + Simplified instructions:
      + Create access to =longhorn-backend= service
        #+begin_src bash
          # Change type to NodePort
          kubectl -n longhorn-system edit svc longhorn-backend
        #+end_src

    #+begin_src bash
      # specify NodePort to service longhorn-backend
      HOSTPORT=`hostname`:30487
      # initiate bundle generation
      bundleinfo=$(curl -s -X POST \
                        -H 'Content-Type: application/json' -d '{ "issueURL": "https://github.com/longhorn/longhorn/issues/2118", "description": "2118" }' \
                        http://${HOSTPORT}/v1/supportbundles)
      bundleid=$(echo $bundleinfo | jq -r '.id')
      bundlename=$(echo $bundleinfo | jq -r '.name')

      # check download readiness
      bundlestate=$(curl -s  -X GET \
                         http://${HOSTPORT}/v1/supportbundles/${bundleid}/${bundlename} | jq -r '.state')
      # When ready, download the beast
      if [ ${bundlestate} == 'ReadyForDownload' ] ; then
          curl -s -X GET \
               http://${HOSTPORT}/v1/supportbundles/${bundleid}/${bundlename}/download --output /tmp/support-bundle.zip
          echo "Wrote Longhorn support bundle to /tmp/support-bundle.zip"
      fi
    #+end_src


** Submitted / in progress
*** [[https://github.com/rancher/harvester/issues/646][#646 {BUG} Specifying an internet proxy during installation of agent node makes node unusable]]
*** [[https://github.com/rancher/harvester/issues/647][#647 {BUG} UI Reports total available on root file systems, not total Longhorn storage]]
*** #666 Rebooting a node makes VM unable to start
   + [[https://github.com/rancher/harvester/issues/666][rancher/harvester#666 {BUG} Rebooting a node makes VMs unable to start]]

** Cooking
*** Harvester node driver node pool template should include cloud config drop-down
*** Harvester node driver node pool template should include SSH public key name
*** Add grouping label to instance when creating multiple instances of VM
    + Use =Name/Prefix= as value
*** CLI get settings should show value
    + Currently, =kubectl get settings.harvester.cattle.io= shows only NAME and AGE
      + it would be useful to show =.value= also for each settings
      + This would make it work the same as =get settings.longhorn.io=
* Mikhail's app mode tutorial
  + [[https://www.suse.com/c/meet-harvester-an-hci-solution-for-the-edge-src/][Meet Harvester, an HCI Solution for the Edge | SUSE Communities]]
