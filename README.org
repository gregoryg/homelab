#+options: ':nil *:t -:t ::t <:t H:3 \n:nil ^:nil arch:headline author:t broken-links:nil
#+options: c:nil creator:nil d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t num:t
#+options: p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t timestamp:t title:t toc:t
#+options: todo:t |:t
#+title: Homelab scripts and config for Kubernetes clusters
#+date: <2020-06-13 Sat>
#+author: Gregory Grubbs
#+email: gregory@dynapse.com
#+language: en
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 27.0.91 (Org mode 9.3.6)
#+setupfile: ~/projects/emacs/org-html-themes/org/theme-readtheorg.setup
# #+setupfile: https://raw.githubusercontent.com/fniessen/org-html-themes/master/setup/theme-readtheorg.setup
#+PROPERTY: header-args:bash :comments org :shebang #!/usr/bin/env bash :tangle deploy-all.sh :eval never-export

* Literate document setup
  This README is a literate programming document.  When this document is loaded in Emacs
  with Org Mode, it can generate scripts and configuration for the documented steps.


** Set up shell session
   This starts up or resuses a shell session named =homelab-sh= for interactive use.  Some
   setup in this document may use this session for stateful operations.

   #+begin_src emacs-lisp :keep-windows t :results none
     (switch-to-buffer (shell "homelab-sh"))
     (switch-to-buffer "README.org")
     (delete-other-windows  )
     (switch-to-buffer-other-window "homelab-sh")
     (switch-to-buffer-other-window "README.org")
   #+end_src

* Binaries and config
** Cow me, baby
   Assure =fortune= and =cowsay= are installed

   The cownonical Cow Me! script
   #+begin_src bash :tangle ~/bin/cowme :results replace output
     # Optionally specify a cow - whether or not in the safe list
     mycow=$1
     # cowfiles are in diff paths on openSUSE and Ubuntu, so include them both
     export COWPATH=/usr/share/cows:/usr/share/cowsay/cows:~/projects/homelab/cows
     if [ -z ${mycow} ] ; then
         IFS=',' read -r -a safe_cows <<< 'default,default,default,default,bud-frogs,elephant,small,three-eyes,tux,rancher,rancher-trademarked,kitten,robotfindskitten,owl,hellokitty'
         mycow=${safe_cows[$((RANDOM % ${#safe_cows[@]}))]}
     fi
     case ${mycow} in
         rancher*|tux|chameleon)
             db='linux computers debian science startrek steven-wright'
             ;;
         owl|satanic)
             db='cookie definitions steven-wright deep-thoughts'
             ;;
         ,*)
             db=
     esac
     cowcmd='cowsay'
     if [[ $((RANDOM % 2)) == 0 ]]; then
         cowcmd='cowthink'
     fi

     if [[ $(command -v fortune 2>/dev/null) && $(command -v cowsay 2>/dev/null) ]] ; then
         IFS=',' read -r -a cowopts <<< "b,g,p,s,t,w,y,d"
         fortune -s ${db} | ${cowcmd} -f ${mycow} -${cowopts[$((RANDOM % ${#cowopts[@]}))]}
         echo
     fi
   #+end_src
* Squid caching proxy
** Server
  Instructions for setting up on Debian
  #+begin_src bash
    sudo apt install squid
  #+end_src

  Add caching for large objects! Put this in =/etc/squid/conf.d/gregs-cache.conf=
  #+begin_src conf
    # http_port 3128 transparent
    http_access allow all
    # ref https://superuser.com/a/972702/74209
    # we want to cache large objects
    maximum_object_size 6 GB
    cache_dir ufs /var/spool/squid 30720 16 256
    cache_mem 256 MB
    maximum_object_size_in_memory 512 KB
    cache_replacement_policy heap LFUDA
    range_offset_limit -1
    quick_abort_min -1 KB
  #+end_src
** Client
*** Apt for Ubuntu and Debian
    in =/etc/apt/apt.conf.d/proxy.conf=
    #+begin_src conf
      Acquire::http::Proxy "http://172.16.17.5:3128/";
      Acquire::https::Proxy "http://172.16.17.5:3128/";
    #+end_src
*** Everything on openSUSE
    Change the following in =/etc/sysconfig/proxy=
    #+begin_src conf
      PROXY_ENABLED="yes"
      HTTP_PROXY="http://172.16.17.5:3128/"
      HTTPS_PROXY="http://172.16.17.5:3128/"
      NO_PROXY="localhost,127.0.0.1,172.16.17.0/24,.magichome"
    #+end_src
* Docker registry for caching images

** The problem
    When setting up a Kubernetes RKE cluster, the same Docker image gets pulled on
    separate connections to each of the nodes.  A pull-through Docker registry would solve
    the problem, acting as a caching server for Docker images.

    However, Docker's built-in support will only work with images in the primary Docker
    registry.
     + [[https://dev.to/mayeu/saving-time-and-bandwidth-by-caching-docker-images-with-a-local-registry-98b][Saving Time and Bandwidth by Caching Docker Images With a Local Registry - DEV]]

** Solution
     =docker-registry-proxy= works with multiple registries.
     + [[https://github.com/rpardini/docker-registry-proxy][GitHub - rpardini/docker-registry-proxy: An HTTPS Proxy for Docker providing ...]]

** Server setup
   This is a proxy that also defaults to 3128 (already used by Squid) - so I'm forwarding
   to port 6128
    #+begin_src bash :tangle ~/bin/start-docker-registry-proxy.sh
      docker run -d --rm --name docker_registry_proxy -it \
             -p 0.0.0.0:6128:3128 \
             -v /data/docker_mirror_cache:/docker_mirror_cache \
             -v /data/docker_mirror_certs:/ca \
             -e REGISTRIES="k8s.gcr.io gcr.io quay.io" \
             -e AUTH_REGISTRIES="auth.docker.io:gregoryg:NLCsEKtk6cNeE5 quay.io:gregoryg:AJYgeUXbfjiRFNPiyM5Wrc+NiEBkIPe1lpjkp2erB6xaETMZowuaU6qLEkbFB7h+Rr4ExAoRrstcpLSt4c3zJtEJM/+mLQ3GCaQ9OeQ1Plc=" \
             rpardini/docker-registry-proxy:latest
             # tiangolo/docker-registry-proxy:latest


             # -e REGISTRIES="k8s.gcr.io gcr.io quay.io your.own.registry another.public.registry" \
             # -e AUTH_REGISTRIES="auth.docker.io:dockerhub_username:dockerhub_password your.own.registry:username:password" \
    #+end_src
** Client setup

    Create file =/etc/systemd/system/docker.service.d/http-proxy.conf=
    #+begin_src bash
      sudo mkdir -p /etc/systemd/system/docker.service.d
    #+end_src

    #+begin_src conf :tangle /sudo::/etc/systemd/system/docker.service.d/http-proxy.conf
      [Service]
      Environment="HTTP_PROXY=http://172.16.17.5:6128/"
      Environment="HTTPS_PROXY=http://172.16.17.5:6128/"
      Environment="NO_PROXY=localhost,127.0.0.1,docker-registry.example.com,.corp,quay.io"
    #+end_src

    Get the CA certificate from the proxy and make it a trusted root.  The directory for
    the certificate differs on OpenSUSE and Ubuntu
    #+begin_src bash
      if [ -d "/etc/pki/trust/anchors" ] ; then
          certdir=/etc/pki/trust/anchors
      else
          certdir=/usr/share/ca-certificates
      fi
      curl http://172.16.17.5:6128/ca.crt | sudo tee ${certdir}/docker_registry_proxy.crt
      echo "docker_registry_proxy.crt" | sudo tee -a /etc/ca-certificates.conf
      sudo update-ca-certificates --fresh
    #+end_src

    Reload and restart
    #+begin_src bash
      sudo systemctl daemon-reload
      sudo systemctl restart docker
    #+end_src

** Testing the clients

   + Clear =dockerd= of everything not currently running:
     #+begin_src bash
       docker system prune -a -f beware.
     #+end_src
   + Pull something, like
     #+begin_src bash
       docker pull ubuntu:20.04
     #+end_src
   + Watch the caching proxy logs on Lab-Server1
     #+begin_src bash
       docker logs docker_registry_proxy --follow
     #+end_src

    Then do, for example, docker pull k8s.gcr.io/kube-proxy-amd64:v1.10.4 and watch the
     logs on the caching proxy, it should list a lot of MISSes.

    Then, clean again, and pull again. You should see HITs! Success.

    Do the same for docker pull ubuntu and rejoice.

    Test your own registry caching and authentication the same way; you don't need docker
     login, or .docker/config.json anymore.
* Setting up Kubernetes distributions
** Set up initial barebones cluster
*** Apache Kubernetes using Kubeadm
    + [[file+emacs:01-k8s-distribution/apache-kubeadm/]]
*** Rancher Labs' RKE
    + [[file+emacs:01-k8s-distribution/rancher-rke/]]
    + Have a look at the two canonical starter configs from Rancher:
      + [[file+emacs:01-k8s-distribution/rancher-rke/templates/cluster-full-example.yaml-TEMPLATE][Cluster full example config]]
      + [[file+emacs:01-k8s-distribution/rancher-rke/templates/cluster-minimal-example.yaml-TEMPLATE][Cluster minimal example config]]
    + [[file+emacs:01-k8s-distribution/rancher-rke/cluster.yaml][My specific homelab config]]
    + Once you have the config to your liking, run
      #+begin_src bash
        # rke up
        rke up --config cluster.yaml --ssh-agent-auth
        KUBECONFIG=kube_config_cluster.yaml kubectl get nodes
      #+end_src
**** Install Rancher on the cluster
     #+begin_src bash
       kubectl create ns cattle-system
       helm install rancher rancher-latest/rancher \
            --namespace cattle-system \
            --set hostname=rancher.example.com
     #+end_src
     + To get the =cattle-cluster-agent= Deployment to resolve my =rancher.example.com=
       server URL, I had to add the following to =Deployment.spec.template.spec=
       #+begin_src yaml
         hostAliases:
         - hostnames:
           - rancher.example.com
           ip: 172.16.17.14
       #+end_src
*** Platform9 PMK
** Initialize Pod networking
   + SKIP THIS for RKE - Canal is already installed and configured
   + At the end of this step you should see all nodes reporting =ready= status
       #+begin_src bash
       kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml
       #+end_src
** Consider a cluster management solution
   + There are some options for getting a Web UI overview of either a single cluster or
     multiple clusters.  These will usually offer the ability to display resource usage,
     view and edit running resources, and create new resources.  Some allow higher level
     options like setting workloads to run on multiple clusters, deploying secrets and
     config maps across clusters, etc.
   + A great choice for this is Rancher (not RKE or K3s, which are Kubernetes distributions
     offered by Rancher Labs).  All you have to do to get started is to follow the guide at
     [[https://rancher.com/docs/rancher/v2.x/en/quick-start-guide/deployment/quickstart-manual-setup/][Rancher Docs: Manual Quick Start]].  The TL;DR is here.
     #+begin_src bash
     docker run --name rancher -d --restart=unless-stopped -p 0.0.0.0:80:80 -p 0.0.0.0:443:443 rancher/rancher
     #+end_src
   + Run this on any server you wish that can be seen by your cluster.  It can also be run
     on one of your cluster nodes, of course.
** Establish storage solution
   + I'm putting this step ahead of higher-level networking or *any* new objects that might
     create persistent volume claims
*** Longhorn
    + OSS project created by Rancher Labs
      #+begin_src bash
        kubectl apply -f  https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/longhorn.yaml
      #+end_src
    + If you want to create easy access to the Longhorn UI, change the =longhorn-frontend=
      service to either NodePort or LoadBalancer.  If the latter, you will need to
      implement a load balancer solution such as MetalLB (see below)
*** Optionally make one storage class the default
    + Add annotation to the desired StorageClass resource
     #+begin_src yaml
       annotations:
         storageclass.kubernetes.io/is-default-class: "true"
     #+end_src
    + Check with =kubectl get sc=
    + Note that you can also install Longhorn using the Rancher UI if you are using that:
      Rancher -> Apps -> Launch -> Longhorn
*** Longhorn Service Monitor to feed Prometheus
    #+begin_src yaml
      apiVersion: monitoring.coreos.com/v1
      kind: ServiceMonitor
      metadata:
        name: longhorn-prometheus-servicemonitor
        namespace: cattle-monitoring-system
        labels:
          name: longhorn-prometheus-servicemonitor
      spec:
        selector:
          matchLabels:
            app: longhorn-manager
        namespaceSelector:
          matchNames:
          - longhorn-system
        endpoints:
        - port: manager
    #+end_src
** Set up Load Balancing and Ingress Controller
   + First step, let's make it possible to create =LoadBalancer= resources
   + On our bare metal cluster, we'll use MetalLB - be sure to check [[https://github.com/metallb/metallb/releases][releases]] to get the
     right URL
   + TODO: Investigate reserving host network IPs
     #+begin_src bash :session homelab-sh
       # use new namespace metallb-system
       kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.5/manifests/namespace.yaml
       kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.5/manifests/metallb.yaml
       # On first install only
       kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey="$(openssl rand -base64 128)"
     #+end_src

   + Give MetalLB a pool of IPs
     + Here I'm using a pool from the primary home network
       #+begin_src yaml :tangle manifests/metallb-pool-cm.yaml
         apiVersion: v1
         kind: ConfigMap
         metadata:
           namespace: metallb-system
           name: config
         data:
           config: |
             address-pools:
             - name: default
               protocol: layer2
               addresses:
               - 172.16.17.230-172.16.17.250
       #+end_src
** Prepare for complex apps - Helm and Kudo
*** Helm
    + Install the Helm 3.x client from [[https://github.com/helm/helm/releases][Helm releases]]
    + That's all there is to do!  Installing a Helm chart will put required resources on
      the server
*** KUDO
    + Install the KUDO client from [[https://github.com/kudobuilder/kudo/releases][Kudo releases]]
    + This is a =kubectl= plugin; the binary is named =kubectl-kudo=.  It can be invoked
      as-is, but is meant to be used in conjunction with =kubectl=.  Place it in the Path
      and test it with
      #+begin_src bash
        kubectl kudo version
      #+end_src
    + Install server components with
      #+begin_src bash
        kubectl kudo init
      #+end_src
** Install a relational DB - MySQL
   + We will do this with the mature Helm chart
   + Change the root password below
     #+begin_src bash
       # Create the namespace we will use
       kubectl create ns sunshine
       helm install mysql stable/mysql \
            -n sunshine \
            --set mysqlRootPassword=adminpass,persistence.storageClass=longhorn,persistence.size=20Gi
     #+end_src
   + Note that the Longhorn UI should show a 20Gi volume.
   + To use the =mysql= CLI or other client, figure out whether you want to forward the
     port, use a NodePort or create a load balancer
** Install Apache Kafka
   + For this we will use KUDO, which offers a mature, purely declarative operator
   + Zookeeper first
     #+begin_src bash
       kubectl kudo install zookeeper --instance=zk
     #+end_src
   + Wait until all Zookeeper pods in your chosen namespace are ready, then
     #+begin_src bash
       kubectl kudo install kafka \
               --instance=kafka \
               -p ZOOKEEPER_URI=zk-zookeeper-0.zk-hs:2181,zk-zookeeper-1.zk-hs:2181,zk-zookeeper-2.zk-hs:2181
     #+end_src
** Tear down your cluster
*** Apache K8s with Kubeadm
*** Rancher's RKE
    #+begin_src bash
      rke remove --config cluster.yaml --ssh-agent-auth
    #+end_src
*** Platform 9 Systems' PMK
    #+begin_src bash
      sudo apt purge `dpkg -l | grep pf9|cut -d' ' -f3`
      sudo rm -rf /var/opt/pf9/ /opt/pf9/ /var/log/pf9/ /var/log/pods
    #+end_src
*** Cleanup after removal of any distribution
**** Some components may need manual removal
     #+begin_src bash
       sudo rm -rf /var/lib/longhorn
       sudo rm -rf /etc/cni/net.d/
     #+end_src
** Kubernetes notes
*** Create volume that persists between multiple pod restarts
    :LOGBOOK:
    - State "DONE"       from              [2020-02-23 Sun 12:10]
    :END:
    + A volume that handles persistent storage using a PersistentVolumeClaim will survive
      Pod restarts.  This is true of Konvoy's default storage class on any cloud platform,
      and is true of persistent storage providers such as Portworx and Mayadata.
**** To show this on AWS
     + Define a PersistentVolumeClaim using the =awsebscsiprovisioner= storage class
       #+caption: hello-pvc.yaml
       #+begin_src yaml :tangle manifests/hello-pvc.yaml
         apiVersion: v1
         kind: PersistentVolumeClaim
         metadata:
           labels:
             app: hello-world
           name: hello-pvc
         spec:
           accessModes:
           - ReadWriteOnce
           resources:
             requests:
               storage: 10Gi
           storageClassName: awsebscsiprovisioner
           volumeMode: Filesystem
       #+end_src
       + Create a PVC using the above manifest.  List the resulting PVC resource and see
         that it is created and in a =Pending= state:

         #+begin_src bash :results table replace
           kubectl create -f manifests/hello-pvc.yaml
           kubectl get pvc -o wide
         #+end_src


       + Define a Pod that makes use of the PVC
         #+caption: myhello-pod.yaml
         #+begin_src yaml :tangle manifests/myhello-pod.yaml
           apiVersion: v1
           kind: Pod
           metadata:
             creationTimestamp: null
             labels:
               app: myhello
             name: myhello
           spec:
             containers:
             - image: nginxdemos/hello
               name: myhello
               resources: {}
               volumeMounts:
               - name: myhellovol
                 mountPath: /data
             dnsPolicy: ClusterFirst
             restartPolicy: Never
             volumes:
             - name: myhellovol
               persistentVolumeClaim:
                 claimName: hello-pvc
           status: {}
         #+end_src
         + Create the Pod, then list both the pod and the PersistentVolume that was created
           from the PVC.
           #+begin_src bash :results output replace
             kubectl create -f manifests/myhello-pod.yaml
             until [ $(kubectl get pods myhello -o jsonpath='{.status.phase}') == 'Running' ]; do sleep 1; done
             kubectl get pod,pv,pvc
           #+end_src

         + Create a file on the mounted volume, delete the pod, recreate the pod and verify
           that the file is still there
           #+begin_src bash  :results output replace
             kubectl exec myhello -- sh -c "touch /data/persistent.flag && ls /data/"
             kubectl delete pod myhello && kubectl create -f manifests/myhello-pod.yaml
             until [ $(kubectl get pods myhello -o jsonpath='{.status.phase}') == 'Running' ]; do sleep 1; done
             kubectl exec myhello -- sh -c "ls /data/"
           #+end_src

*** Create volume per instance of pod
    :LOGBOOK:
    - State "DONE"       from              [2020-02-23 Sun 12:10]
    :END:
    + This would be a volume used by each pod, and valid for the life of the individual
      Pod.  One reason to have this would be for multiple containers in the pod to indicate
      readiness and "liveness".  For this reason, the example will be a multi-container pod
      with an Init container writing a file to indicate readiness, and a container that
      periodically writes status for a liveness probe.

      #+caption: multivol-deployment.yaml
      #+begin_src yaml :tangle manifests/multivol-deployment.yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          creationTimestamp: null
          labels:
            app: multivol
          name: multivol
        spec:
          replicas: 3
          selector:
            matchLabels:
              app: multivol
          strategy: {}
          template:
            metadata:
              creationTimestamp: null
              labels:
                app: multivol
            spec:
              initContainers:
              - name: init1
                image: busybox
                command: ["sh", "-c", "touch /status/running"]
                volumeMounts:
                - name: statusdir
                  mountPath: /status
              containers:
              - name: nginx
                image: nginx
                resources: {}
                readinessProbe:
                  exec:
                    command: ["sh", "-c", "ls /opt/status/running && true"]
                volumeMounts:
                - name: statusdir
                  mountPath: /opt/status
              volumes:
              - name: statusdir
                emptyDir: {}
        status: {}
      #+end_src

      + Create, then describe the deployment.  Note the same volume is deployed at
        different mount points in each container
        #+begin_src bash  :results output replace
           kubectl create -f manifests/multivol-deployment.yaml
           until [ $(kubectl get pods -l app=multivol -o jsonpath='{.items[*].status.phase}' | grep 'Running' -o | wc -l) -eq 3  ]; do sleep 1; done
           kubectl describe deployment multivol | grep Mounts: -A 4
        #+end_src

*** Create volume shared between pods                                  :NEXT:
    :LOGBOOK:
    - State "MAYBE"      from "STARTED"    [2020-02-24 Mon 08:36]
    - State "STARTED"    from              [2020-02-23 Sun 12:10]
    :END:
    + This is a matter of mounting the volume as ReadWriteMany.  The underlying file system
      must support sharing across multiple nodes.  Examples of this type of file system
      include NFS and cloud implementations such as AWS EFS.

**** Example on AWS
     + Create an EFS file system in the AWS Console or CLI
     + Konvoy comes pre-installed with Helm and Tiller.  Install the EFS Provisioner using
       a Helm chart.  You will need the EFS file system ID and the AWS region it's in.  Use
       the below as a guide
       #+begin_src bash :results output replace
         helm install --name efs-provisioner \
              --namespace default \
              --set  efsProvisioner.efsFileSystemId=fs-d7a62e7d \
              --set efsProvisioner.awsRegion=us-west-2 \
              stable/efs-provisioner
       #+end_src

    + We will define a deployment with 3 replicas.  Each pod will mount the same persistent
      volume.  As before, the pods will mount a volume based on a PersistentVolumeClaim.

      #+caption: diskshare-pvc.yaml
      #+begin_src yaml :tangle manifests/diskshare-pvc.yaml
        apiVersion: v1
        kind: PersistentVolumeClaim
        metadata:
          labels:
            app: diskshare
          name: diskshare-pvc
        spec:
          accessModes:
          - ReadWriteMany
          resources:
            requests:
              storage: 6Ki
          storageClassName: aws-efs
          volumeMode: Filesystem
      #+end_src
      #+caption: diskshare-deployment.yaml
      #+begin_src yaml :tangle manifests/diskshare-deployment.yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          creationTimestamp: null
          labels:
            app: diskshare
          name: diskshare
        spec:
          replicas: 3
          selector:
            matchLabels:
              app: diskshare
          strategy: {}
          template:
            metadata:
              creationTimestamp: null
              labels:
                app: diskshare
            spec:
              containers:
              - name: nginx
                image: nginx
                command: ["sh", "-c", "echo 'Wondrous Disk Content at WDC!' > /usr/share/nginx/html/index.html"]
                resources: {}
                volumeMounts:
                - name: sharevol
                  mountPath: /usr/share/nginx/html
              volumes:
              - name: sharevol
                persistentVolumeClaim:
                  claimName: diskshare-pvc
        status: {}
      #+end_src

    + Create PVC and Deployment, verify all pods share the disk
      #+begin_src bash :results output replace
      #+end_src
*** Resize existing volume in-place
    + [[https://kubernetes.io/docs/concepts/storage/persistent-volumes/#csi-volume-expansion][CSI Volume Expansion]] (k8s.io)
    + Resizing in-use volumes can only be done on specific storage classes that support
      dynamic resizing.  It is effected by editing the PersistentVolumeClaim object.
* Rancher Server
** Test full certificate chain of Rancher Server
   #+begin_src bash
     docker run superseb/ranchercheck https://rancher.hypecyclist.org:8443
   #+end_src
** Clean up after RKE
* RKE
** Clean up after RKE
   #+begin_src bash
     # on all nodes - get rid of ALL docker containers -- too general if anything else may be running
     WCOLL=~/projects/homelab/pdsh/all-nodes.txt pdsh -R ssh 'docker stop `docker ps -aq`'
     WCOLL=~/projects/homelab/pdsh/all-nodes.txt pdsh -R ssh 'docker rm `docker ps -aq`'
     WCOLL=~/projects/homelab/pdsh/all-nodes.txt pdsh -R ssh '# remove CNI and Longhorn remnants'
     WCOLL=~/projects/homelab/pdsh/all-nodes.txt pdsh -R ssh 'sudo rm -rfv /var/lib/longhorn/* /data/longhorn/* /etc/cni/* /var/lib/kubelet/cpu_manager_state'
     WCOLL=~/projects/homelab/pdsh/all-nodes.txt pdsh -R ssh '# iptables'
     WCOLL=~/projects/homelab/pdsh/all-nodes.txt pdsh -R ssh 'sudo iptables -F -t nat'
     WCOLL=~/projects/homelab/pdsh/all-nodes.txt pdsh -R ssh 'sudo iptables -X -t nat'
     WCOLL=~/projects/homelab/pdsh/all-nodes.txt pdsh -R ssh 'sudo iptables -F -t mangle'
     WCOLL=~/projects/homelab/pdsh/all-nodes.txt pdsh -R ssh 'sudo iptables -X -t mangle'
     WCOLL=~/projects/homelab/pdsh/all-nodes.txt pdsh -R ssh 'sudo iptables -F'
     WCOLL=~/projects/homelab/pdsh/all-nodes.txt pdsh -R ssh 'sudo iptables -X'
     WCOLL=~/projects/homelab/pdsh/all-nodes.txt pdsh -R ssh 'sudo systemctl restart docker'

   #+end_src
* Networking
  :PROPERTIES:
  :tangle-dir: 02-pod-networking/
  :END:
** Network policies
   Working directory (from base of the =homelab= directory)
   #+name: 02-pod-networking
*** Experiment with pure Kubernetes network policy
    + ref [[https://docs.projectcalico.org/security/kubernetes-network-policy][Get started with Kubernetes network policy]]

      This requires running pods with color labels (blue, red) and namespace labels*
*** Start pods
     #+begin_src bash
       kubectl create namespace k8snetpol
       kubectl -n k8snetpol run blue --image=nginx --labels app=blue,color=blue
       kubectl -n k8snetpol run red  --image=nginx --labels app=red,color=red
     #+end_src

     In this first example, inoming traffic to pods with label =color=blue= are allowed only
     if they come from a pod with =color=red= on port =80=
     #+begin_src yaml :tangle (concat (org-entry-get nil "tangle-dir" t) "/manifests/k8s-red-is-cool-for-blue-netpol.yaml")
       kind: NetworkPolicy
       apiVersion: networking.k8s.io/v1
       metadata:
         name: allow-same-namespace
         namespace: k8snetpol
       spec:
         podSelector:
           matchLabels:
             color: blue
         ingress:
         - from:
           - podSelector:
               matchLabels:
                 color: red
           ports:
             - port: 80
     #+end_src

     #+begin_src bash :dir 02-pod-networking/manifests/
       kubectl apply -f k8s-red-is-cool-for-blue-netpol.yaml
     #+end_src

*** Experiment with Calico network policies
    Following along with [[https://medium.com/flant-com/calico-for-kubernetes-networking-792b41e19d69][Calico for Kubernetes networking: the basics & examples]]

    One =nodejs= app deployment that has access to =redis=.  A php deployment that has no
     access to =redis=

     1058  k create deploy redis --image=redis
   1059  kx
   1060  k -n default get all
   1061  k scale deployment redis --replicas=3
   1062  k get all
   1063  k get pods -o wide
   1064  k get pods -o wide -w
   1065  k create deploy redis --image=readytalk/nodejs
   1066  k create deploy nodejs --image=readytalk/nodejs
   1067  k get all
   1068  k get ev
   1069  k get all
   1070  docker search php
   1071  k create deploy phpmyadmin --image=phpmyadmin/phpmyadmin
   1072  k get all
   1073  k get deployment --show-labels
   1074  pwd
   1075  history

**** Create deployments
***** Redis
      Created using
      + =kubectl create deploy redis --image=readytalk/nodejs -o yaml --dry-run=client=
      + =kubectl expose deployment redis --port=6379 --target-port=6379 -o yaml --dry-run=client=
      #+begin_src yaml :tangle (concat (org-entry-get nil "tangle-dir" t) "/manifests/redis-deployment.yaml")
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          creationTimestamp: null
          labels:
            app: redis
          name: redis
        spec:
          replicas: 1
          selector:
            matchLabels:
              app: redis
          strategy: {}
          template:
            metadata:
              creationTimestamp: null
              labels:
                app: redis
            spec:
              containers:
              - image: readytalk/nodejs
                name: nodejs
                resources: {}
        status: {}
        ---
        apiVersion: v1
        kind: Service
        metadata:
          creationTimestamp: null
          labels:
            app: redis
          name: redis
        spec:
          ports:
          - port: 6379
            protocol: TCP
            targetPort: 6379
          selector:
            app: redis
        status:
          loadBalancer: {}
      #+end_src
***** Fake nodejs app
      Created using
      + =kubectl create deploy nodejs --image=readytalk/nodejs -o yaml --dry-run=client=
        #+begin_src yaml
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            creationTimestamp: null
            labels:
              app: nodejs
            name: nodejs
          spec:
            replicas: 1
            selector:
              matchLabels:
                app: nodejs
            strategy: {}
            template:
              metadata:
                creationTimestamp: null
                labels:
                  app: nodejs
              spec:
                containers:
                - image: readytalk/nodejs
                  name: nodejs
                  resources: {}
          status: {}
        #+end_src
***** Nodejs app
      Created using
      + =kubectl create deploy nodejs-hello --image=heroku/nodejs-hello-world -o yaml --dry-run=client=
        #+begin_src yaml :tangle (concat (org-entry-get nil "tangle-dir" t) "/manifests/nodejs-hello-deploy.yaml")
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            creationTimestamp: null
            labels:
              app: nodejs-hello
            name: nodejs-hello
          spec:
            replicas: 1
            selector:
              matchLabels:
                app: nodejs-hello
            strategy: {}
            template:
              metadata:
                creationTimestamp: null
                labels:
                  app: nodejs-hello
              spec:
                containers:
                - image: heroku/nodejs-hello-world
                  name: nodejs-hello-world
                - image: gregoryg/sh-net-utils
                  name: utils
                  command: ["sleep"]
                  args: ["1d"]
        #+end_src
***** PHP app
      Created using
      + =kubectl  create deploy phpmyadmin --image=phpmyadmin/phpmyadmin -o yaml --dry-run=client=
      + =kubectl expose deploy phpmyadmin --port=80 --target-port=80 -o yaml --dry-run=client=
        #+begin_src yaml
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            creationTimestamp: null
            labels:
              app: phpmyadmin
            name: phpmyadmin
          spec:
            replicas: 1
            selector:
              matchLabels:
                app: phpmyadmin
            strategy: {}
            template:
              metadata:
                creationTimestamp: null
                labels:
                  app: phpmyadmin
              spec:
                containers:
                - image: phpmyadmin/phpmyadmin
                  name: phpmyadmin
                  resources: {}
          status: {}
          ---
          apiVersion: v1
          kind: Service
          metadata:
            creationTimestamp: null
            labels:
              app: phpmyadmin
            name: phpmyadmin
          spec:
            ports:
            - port: 80
              protocol: TCP
              targetPort: 80
            selector:
              app: phpmyadmin
          status:
            loadBalancer: {}
        #+end_src

**** Network policy
     #+begin_src yaml :tangle (concat (org-entry-get nil "tangle-dir" t) "/manifests/calico-block-redis-netpol.yaml")
       kind: NetworkPolicy
       apiVersion: networking.k8s.io/v1
       metadata:
         name: allow-redis-nodejs
       spec:
         podSelector:
           matchLabels:
             service: redis
         ingress:
         - from:
           - podSelector:
               matchLabels:
                 service: nodejs
           ports:
           - protocol: TCP
             port: 6379
     #+end_src
** Illuminatio - a tool to validate Network Policy
   Ref: [[https://www.inovex.de/blog/illuminatio-kubernetes-network-policy-validator/][illuminatio: the Kubernetes Network Policy Validator - inovex Blog]]

*** Install Illuminatio
   #+begin_src bash
   pip3 install illuminatio
   #+end_src
*** Get a deployment with service running
    #+begin_src bash
      kubectl create deployment web --image=nginx
      kubectl expose deployment web --port=80 --target-port=80
    #+end_src

*** Create and apply network policy to prohibit ingress to our deployment
    #+begin_src yaml :dir 02-pod-networking/ :tangle (concat (org-entry-get nil "tangle-dir" t) "/manifests/illuminatio-example-deny-all-netpol.yaml")
      apiVersion: networking.k8s.io/v1
      kind: NetworkPolicy
      metadata:
        name: web-deny-all
      spec:
        podSelector:
          matchLabels:
            app: web
        ingress: []
    #+end_src

    #+begin_src bash :dir 02-pod-networking/manifests/
      kubectl apply -f illuminatio-example-deny-all-netpol.yaml
    #+end_src

*** Test with Illuminatio
    #+begin_src bash
      illuminatio run
    #+end_src
* Observability
** Kubevious
   [[https://github.com/kubevious/kubevious][GitHub - kubevious/kubevious: Kubevious - application centric Kubernetes UI a...]]

   #+begin_src bash
     kubectl create namespace kubevious
     helm repo add kubevious https://helm.kubevious.io
     helm upgrade --atomic -i kubevious kubevious/kubevious --version 0.7.26 -n kubevious
     kubectl port-forward $(kubectl get pods -n kubevious -l "app.kubernetes.io/component=kubevious-ui" -o jsonpath="{.items[0].metadata.name}") 8080:80 -n kubevious
   #+end_src
* Storage
** NFS
   #+begin_src yaml
     apiVersion: v1
     kind: PersistentVolume
     metadata:
       name: data-k8s-pv
     spec:
       storageClassName: ""  # ignore default storage class
       capacity:
         storage: 1Gi
       accessModes:
         - ReadWriteMany
       persistentVolumeReclaimPolicy: Retain
       nfs:
         path: /data/data-files/k8s
         server: glados.magichome
         readOnly: false
   #+end_src
   #+begin_src yaml
     apiVersion: v1
     kind: PersistentVolumeClaim
     metadata:
       name: data-k8s-pv
     spec:
       storageClassName: ""  # ignore default storage class
       accessModes:
       - ReadWriteMany
       resources:
          requests:
            storage: 1Gi
   #+end_src
   #+begin_src yaml
     apiVersion: v1
     kind: Pod
     metadata:
       name: nginx-nfs-pod
       labels:
         name: nginx-nfs-pod
     spec:
       containers:
         - name: nginx-nfs-pod
           image: fedora/nginx
           ports:
             - name: web
               containerPort: 80
           volumeMounts:
             - name: nfsvol
               mountPath: /usr/share/nginx/html
       securityContext:
           supplementalGroups: [1000]
           # privileged: false
       volumes:
         - name: nfsvol
           persistentVolumeClaim:
             claimName: data-k8s-pv
   #+end_src
   #+begin_src yaml
     apiVersion: v1
     kind: Pod
     metadata:
       name: busybox-nfs-pod
       labels:
         name: busybox-nfs-pod
     spec:
       containers:
       - name: busybox-nfs-pod
         image: busybox
         command: ["sleep", "60000"]
         volumeMounts:
         - name: nfsvol-2
           mountPath: /usr/share/busybox
           readOnly: false
       securityContext:
         supplementalGroups: [1000]
         # privileged: false
       volumes:
       - name: nfsvol-2
         persistentVolumeClaim:
           claimName: data-k8s-pv
   #+end_src

* Cracking the structure of Rancher application ingress URLs
  + Examples of working parent-authenticated URLs
    + Rancher Server
      + Grafana dashboard
        + https://rancher.hypecyclist.org:8443/api/v1/namespaces/cattle-monitoring-system/services/http:rancher-monitoring-grafana:80/proxy/?orgId=1

    + Managed Cluster
      + Grafana dashboard
        + https://rancher.hypecyclist.org:8443/k8s/clusters/c-r5dj9/api/v1/namespaces/cattle-monitoring-system/services/http:rancher-monitoring-grafana:80/proxy/?orgId=1
        + https://rancher.hypecyclist.org:8443/k8s/clusters/c-r5dj9/api/v1/namespaces/longhorn-system/services/http:longhorn-frontend:80/proxy/
      + Longhorn UI
        + https://rancher.hypecyclist.org:8443/k8s/clusters/c-r5dj9/api/v1/namespaces/longhorn-system/services/http:longhorn-frontend:80/proxy/dashboard
    + Could it be this
      | L/M | Rancher URL                          | local vs managed     | cluster ID      | API    | namespace                | services   | URL to service                     | etc            |
      |-----+--------------------------------------+----------------------+-----------------+--------+--------------------------+------------+------------------------------------+----------------|
      | M   | https://rancher.hypecyclist.org:8443 | null or k8s/clusters | null or c-r5dj9 | api/v1 | longhorn-system          | 'services' | http:longhorn-frontend:80          | proxy          |
      | M   | https://rancher.hypecyclist.org:8443 | k8s/clusters         | c-r5dj9         | api/v1 | cattle-monitoring-system | 'services' | http:rancher-monitoring-grafana:80 | proxy/+orgId=1 |
      | L   | https://rancher.hypecyclist.org:8443 | null                 | null            | api/v1 | cattle-monitoring-system | 'services' | http:rancher-monitoring-grafana:80 | proxy/+orgId=1 |
** Use the Rancher API to get URLS
*** Authentication
    set env vars =rancher_access= and =rancher_secret= to the Access Key and Secret Key
    values from the Rancher UI API Keys

    For now, do this manually in the =homelab-sh= session
    #+begin_src bash
      read -p "Password: " rancher_access
      read -p "Password: " rancher_secret
      export rancher_access rancher_secret
    #+end_src
*** Get list of clusters known to Rancher Server with relevant info
    #+begin_src bash :session homelab-sh
      curl -s -k \
           -u "${rancher_access}:${rancher_secret}" \
           -X GET \
           -H 'Accept: application/json' \
           -H 'Content-Type: application/json' \
           'https://rancher.hypecyclist.org:8443/v3/clusters/' > /tmp/rancher-clusters.json
    #+end_src

    #+begin_src bash :results table
      cat /tmp/rancher-clusters.json | \
          jq -r '.data[] | "\(.name)\t\(.id)"'
    #+end_src
*** Get nice list of convenient API links for a specific cluster
    #+begin_src bash :results replace raw
      cat /tmp/rancher-clusters.json | \
          jq -r '.data[] | select (.name == "goozilla") | {"name": .name, "id": .id, "links": .links}'
    #+end_src
    {
      "name": "goozilla",
      "id": "c-vb78v",
      "links": {
        "apiServices": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/apiservices",
        "clusterAlertGroups": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/clusteralertgroups",
        "clusterAlertRules": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/clusteralertrules",
        "clusterAlerts": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/clusteralerts",
        "clusterCatalogs": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/clustercatalogs",
        "clusterLoggings": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/clusterloggings",
        "clusterMonitorGraphs": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/clustermonitorgraphs",
        "clusterRegistrationTokens": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/clusterregistrationtokens",
        "clusterRoleTemplateBindings": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/clusterroletemplatebindings",
        "clusterScans": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/clusterscans",
        "etcdBackups": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/etcdbackups",
        "namespaces": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/namespaces",
        "nodePools": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/nodepools",
        "nodes": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/nodes",
        "notifiers": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/notifiers",
        "persistentVolumes": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/persistentvolumes",
        "projects": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/projects",
        "remove": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v",
        "self": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v",
        "shell": "wss://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v?shell=true",
        "storageClasses": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/storageclasses",
        "subscribe": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/subscribe",
        "templates": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/templates",
        "tokens": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/tokens",
        "update": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v"
      }
    }
* Dell XPS 13" 2 in 1 Laptop - Linux stuff
** Buzzing from headphone jack when nothing is playing
   The key is to set an option in the =snd-hda-intel= module.  =power_save=0= means never
   go into power saving mode
   + On Ubuntu 20.4
     in =/etc/modprobe.d/alsa-base.conf=
     #+begin_src conf
       # GJG stop the buzzing from headphone jack
       options snd-hda-intel power_save=0 power_save_controller=N
     #+end_src
   + On openSUSE Tumbleweed
     in =/etc/modprobe.d/42-power-audio.conf=
     #+begin_src conf
       options snd_hda_intel power_save=0 power_save_controller=0
     #+end_src
* Monitoring for RabbitMQ
  It may be necessary to create a Longhorn Volume/PVC named =data-rabbit-rabbitmq-0=
#+begin_src bash
  helm install rabbit bitnami/rabbitmq \
       --set persistence.storageClass=longhorn \
       --namespace rabbit \
       --set metrics.enabled=true \
       --set metrics.serviceMonitor.enabled=true
#+end_src
