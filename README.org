#+options: ':nil *:t -:t ::t <:t H:3 \n:nil ^:nil arch:headline author:t broken-links:nil
#+options: c:nil creator:nil d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t num:t
#+options: p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t timestamp:t title:t toc:t
#+options: todo:t |:t
#+title: Homelab scripts and config for Kubernetes clusters
#+date: <2020-06-13 Sat>
#+author: Gregory Grubbs
#+email: gregory@dynapse.com
#+language: en
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 27.0.91 (Org mode 9.3.6)
#+setupfile: ~/projects/emacs/org-html-themes/setup/theme-readtheorg.setup
# #+setupfile: https://raw.githubusercontent.com/fniessen/org-html-themes/master/setup/theme-readtheorg.setup
#+PROPERTY: header-args:sh :comments org :shebang #!/usr/bin/env bash :tangle deploy-all.sh :eval never-export

* Literate document setup
  This README is a literate programming document.  When this document is loaded in Emacs
  with Org Mode, it can generate scripts and configuration for the documented steps.

* Set up initial barebones cluster
** Apache Kubernetes using Kubeadm
   + [[file+emacs:01-k8s-distribution/apache-kubeadm/]]
** Rancher Labs' RKE
   + [[file+emacs:01-k8s-distribution/rancher-rke/]]
   + Have a look at the two canonical starter configs from Rancher:
     + [[file+emacs:01-k8s-distribution/rancher-rke/templates/cluster-full-example.yaml-TEMPLATE][Cluster full example config]]
     + [[file+emacs:01-k8s-distribution/rancher-rke/templates/cluster-minimal-example.yaml-TEMPLATE][Cluster minimal example config]]
   + [[file+emacs:01-k8s-distribution/rancher-rke/cluster.yaml][My specific homelab config]]
   + Once you have the config to your liking, run
     #+begin_src bash
       # rke up
       rke up --config cluster.yaml --ssh-agent-auth
       KUBECONFIG=kube_config_cluster.yaml kubectl get nodes
     #+end_src
*** Install Rancher on the cluster
    #+begin_src bash
      kubectl create ns cattle-system
      helm install rancher rancher-latest/rancher \
           --namespace cattle-system \
           --set hostname=rancher.example.com
    #+end_src
    + To get the =cattle-cluster-agent= Deployment to resolve my =rancher.example.com=
      server URL, I had to add the following to =Deployment.spec.template.spec=
      #+begin_src yaml
        hostAliases:
        - hostnames:
          - rancher.example.com
          ip: 172.16.17.14
      #+end_src
** Platform9 PMK
* Initialize Pod networking
  + SKIP THIS for RKE - Canal is already installed and configured
  + At the end of this step you should see all nodes reporting =ready= status
      #+begin_src bash
      kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml
      #+end_src
* Consider a cluster management solution
  + There are some options for getting a Web UI overview of either a single cluster or
    multiple clusters.  These will usually offer the ability to display resource usage,
    view and edit running resources, and create new resources.  Some allow higher level
    options like setting workloads to run on multiple clusters, deploying secrets and
    config maps across clusters, etc.
  + A great choice for this is Rancher (not RKE or K3s, which are Kubernetes distributions
    offered by Rancher Labs).  All you have to do to get started is to follow the guide at
    [[https://rancher.com/docs/rancher/v2.x/en/quick-start-guide/deployment/quickstart-manual-setup/][Rancher Docs: Manual Quick Start]].  The TL;DR is here.
    #+begin_src bash
    docker run --name rancher -d --restart=unless-stopped -p 0.0.0.0:80:80 -p 0.0.0.0:443:443 rancher/rancher
    #+end_src
  + Run this on any server you wish that can be seen by your cluster.  It can also be run
    on one of your cluster nodes, of course.
* Establish storage solution
  + I'm putting this step ahead of higher-level networking or *any* new objects that might
    create persistent volume claims
** Longhorn
   + OSS project created by Rancher Labs
     #+begin_src bash
       kubectl apply -f  https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/longhorn.yaml
     #+end_src
   + If you want to create easy access to the Longhorn UI, change the =longhorn-frontend=
     service to either NodePort or LoadBalancer.  If the latter, you will need to
     implement a load balancer solution such as MetalLB (see below)
** Optionally make one storage class the default
   + Add annotation to the desired StorageClass resource
    #+begin_src yaml
      annotations:
        storageclass.kubernetes.io/is-default-class: "true"
    #+end_src
   + Check with =kubectl get sc=
   + Note that you can also install Longhorn using the Rancher UI if you are using that:
     Rancher -> Apps -> Launch -> Longhorn
* Set up Load Balancing and Ingress Controller
  + First step, let's make it possible to create =LoadBalancer= resources
  + On our bare metal cluster, we'll use MetalLB - be sure to check [[https://github.com/metallb/metallb/releases][releases]] to get the
    right URL
  + TODO: Investigate reserving host network IPs
    #+begin_src bash
      # use new namespace metallb-system
      kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/namespace.yaml
      kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.3/manifests/metallb.yaml
      # On first install only
      kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey="$(openssl rand -base64 128)"
    #+end_src
  + Give MetalLB a pool of IPs
    + Here I'm using a pool from the primary home network
      #+begin_src yaml :tangle manifests/metallb-pool-cm.yaml
        apiVersion: v1
        kind: ConfigMap
        metadata:
          namespace: metallb-system
          name: config
        data:
          config: |
            address-pools:
            - name: default
              protocol: layer2
              addresses:
              - 172.16.17.230-172.16.17.250
      #+end_src
* Prepare for complex apps - Helm and Kudo
** Helm
   + Install the Helm 3.x client from [[https://github.com/helm/helm/releases][Helm releases]]
   + That's all there is to do!  Installing a Helm chart will put required resources on
     the server
** KUDO
   + Install the KUDO client from [[https://github.com/kudobuilder/kudo/releases][Kudo releases]]
   + This is a =kubectl= plugin; the binary is named =kubectl-kudo=.  It can be invoked
     as-is, but is meant to be used in conjunction with =kubectl=.  Place it in the Path
     and test it with
     #+begin_src bash
       kubectl kudo version
     #+end_src
   + Install server components with
     #+begin_src bash
       kubectl kudo init
     #+end_src
* Install a relational DB - MySQL
  + We will do this with the mature Helm chart
  + Change the root password below
    #+begin_src bash
      # Create the namespace we will use
      kubectl create ns sunshine
      helm install mysql stable/mysql \
           -n sunshine \
           --set mysqlRootPassword=adminpass,persistence.storageClass=longhorn,persistence.size=20Gi
    #+end_src
  + Note that the Longhorn UI should show a 20Gi volume.
  + To use the =mysql= CLI or other client, figure out whether you want to forward the
    port, use a NodePort or create a load balancer
* Install Apache Kafka
  + For this we will use KUDO, which offers a mature, purely declarative operator
  + Zookeeper first
    #+begin_src bash
      kubectl kudo install zookeeper --instance=zk
    #+end_src
  + Wait until all Zookeeper pods in your chosen namespace are ready, then
    #+begin_src bash
      kubectl kudo install kafka \
              --instance=kafka \
              -p ZOOKEEPER_URI=zk-zookeeper-0.zk-hs:2181,zk-zookeeper-1.zk-hs:2181,zk-zookeeper-2.zk-hs:2181
    #+end_src
* Tear down your cluster
** Apache K8s with Kubeadm
** Rancher's RKE
   #+begin_src bash
     rke remove --config cluster.yaml --ssh-agent-auth
   #+end_src
** Cleanup after removal of any distribution
*** Some components may need manual removal
    #+begin_src bash
      sudo rm -rf /var/lib/longhorn
      sudo rm -rf /etc/cni/net.d/
    #+end_src
* Kubernetes notes
** Create volume that persists between multiple pod restarts
   :LOGBOOK:
   - State "DONE"       from              [2020-02-23 Sun 12:10]
   :END:
   + A volume that handles persistent storage using a PersistentVolumeClaim will survive
     Pod restarts.  This is true of Konvoy's default storage class on any cloud platform,
     and is true of persistent storage providers such as Portworx and Mayadata.
*** To show this on AWS
    + Define a PersistentVolumeClaim using the =awsebscsiprovisioner= storage class
      #+caption: hello-pvc.yaml
      #+begin_src yaml :tangle manifests/hello-pvc.yaml
        apiVersion: v1
        kind: PersistentVolumeClaim
        metadata:
          labels:
            app: hello-world
          name: hello-pvc
        spec:
          accessModes:
          - ReadWriteOnce
          resources:
            requests:
              storage: 10Gi
          storageClassName: awsebscsiprovisioner
          volumeMode: Filesystem
      #+end_src
      + Create a PVC using the above manifest.  List the resulting PVC resource and see
        that it is created and in a =Pending= state:

        #+begin_src bash :results table replace
          kubectl create -f manifests/hello-pvc.yaml
          kubectl get pvc -o wide
        #+end_src


      + Define a Pod that makes use of the PVC
        #+caption: myhello-pod.yaml
        #+begin_src yaml :tangle manifests/myhello-pod.yaml
          apiVersion: v1
          kind: Pod
          metadata:
            creationTimestamp: null
            labels:
              app: myhello
            name: myhello
          spec:
            containers:
            - image: nginxdemos/hello
              name: myhello
              resources: {}
              volumeMounts:
              - name: myhellovol
                mountPath: /data
            dnsPolicy: ClusterFirst
            restartPolicy: Never
            volumes:
            - name: myhellovol
              persistentVolumeClaim:
                claimName: hello-pvc
          status: {}
        #+end_src
        + Create the Pod, then list both the pod and the PersistentVolume that was created
          from the PVC. 
          #+begin_src bash :results output replace
            kubectl create -f manifests/myhello-pod.yaml
            until [ $(kubectl get pods myhello -o jsonpath='{.status.phase}') == 'Running' ]; do sleep 1; done
            kubectl get pod,pv,pvc
          #+end_src

        + Create a file on the mounted volume, delete the pod, recreate the pod and verify
          that the file is still there
          #+begin_src bash  :results output replace
            kubectl exec myhello -- sh -c "touch /data/persistent.flag && ls /data/"
            kubectl delete pod myhello && kubectl create -f manifests/myhello-pod.yaml
            until [ $(kubectl get pods myhello -o jsonpath='{.status.phase}') == 'Running' ]; do sleep 1; done
            kubectl exec myhello -- sh -c "ls /data/"
          #+end_src

** Create volume per instance of pod
   :LOGBOOK:
   - State "DONE"       from              [2020-02-23 Sun 12:10]
   :END:
   + This would be a volume used by each pod, and valid for the life of the individual
     Pod.  One reason to have this would be for multiple containers in the pod to indicate
     readiness and "liveness".  For this reason, the example will be a multi-container pod
     with an Init container writing a file to indicate readiness, and a container that
     periodically writes status for a liveness probe.

     #+caption: multivol-deployment.yaml
     #+begin_src yaml :tangle manifests/multivol-deployment.yaml
       apiVersion: apps/v1
       kind: Deployment
       metadata:
         creationTimestamp: null
         labels:
           app: multivol
         name: multivol
       spec:
         replicas: 3
         selector:
           matchLabels:
             app: multivol
         strategy: {}
         template:
           metadata:
             creationTimestamp: null
             labels:
               app: multivol
           spec:
             initContainers:
             - name: init1
               image: busybox
               command: ["sh", "-c", "touch /status/running"]
               volumeMounts:
               - name: statusdir
                 mountPath: /status
             containers:
             - name: nginx
               image: nginx
               resources: {}
               readinessProbe:
                 exec:
                   command: ["sh", "-c", "ls /opt/status/running && true"]
               volumeMounts:
               - name: statusdir
                 mountPath: /opt/status
             volumes:
             - name: statusdir
               emptyDir: {}
       status: {}
     #+end_src

     + Create, then describe the deployment.  Note the same volume is deployed at
       different mount points in each container
       #+begin_src bash  :results output replace
          kubectl create -f manifests/multivol-deployment.yaml
          until [ $(kubectl get pods -l app=multivol -o jsonpath='{.items[*].status.phase}' | grep 'Running' -o | wc -l) -eq 3  ]; do sleep 1; done
          kubectl describe deployment multivol | grep Mounts: -A 4
       #+end_src
     
** Create volume shared between pods                                   :NEXT:
   :LOGBOOK:
   - State "MAYBE"      from "STARTED"    [2020-02-24 Mon 08:36]
   - State "STARTED"    from              [2020-02-23 Sun 12:10]
   :END:
   + This is a matter of mounting the volume as ReadWriteMany.  The underlying file system
     must support sharing across multiple nodes.  Examples of this type of file system
     include NFS and cloud implementations such as AWS EFS.

*** Example on AWS
    + Create an EFS file system in the AWS Console or CLI
    + Konvoy comes pre-installed with Helm and Tiller.  Install the EFS Provisioner using
      a Helm chart.  You will need the EFS file system ID and the AWS region it's in.  Use
      the below as a guide
      #+begin_src bash :results output replace
        helm install --name efs-provisioner \
             --namespace default \
             --set  efsProvisioner.efsFileSystemId=fs-d7a62e7d \
             --set efsProvisioner.awsRegion=us-west-2 \
             stable/efs-provisioner
      #+end_src
      
   + We will define a deployment with 3 replicas.  Each pod will mount the same persistent
     volume.  As before, the pods will mount a volume based on a PersistentVolumeClaim.

     #+caption: diskshare-pvc.yaml
     #+begin_src yaml :tangle manifests/diskshare-pvc.yaml
       apiVersion: v1
       kind: PersistentVolumeClaim
       metadata:
         labels:
           app: diskshare
         name: diskshare-pvc
       spec:
         accessModes:
         - ReadWriteMany
         resources:
           requests:
             storage: 6Ki
         storageClassName: aws-efs
         volumeMode: Filesystem
     #+end_src
     #+caption: diskshare-deployment.yaml
     #+begin_src yaml :tangle manifests/diskshare-deployment.yaml
       apiVersion: apps/v1
       kind: Deployment
       metadata:
         creationTimestamp: null
         labels:
           app: diskshare
         name: diskshare
       spec:
         replicas: 3
         selector:
           matchLabels:
             app: diskshare
         strategy: {}
         template:
           metadata:
             creationTimestamp: null
             labels:
               app: diskshare
           spec:
             containers:
             - name: nginx
               image: nginx
               command: ["sh", "-c", "echo 'Wondrous Disk Content at WDC!' > /usr/share/nginx/html/index.html"]
               resources: {}
               volumeMounts:
               - name: sharevol
                 mountPath: /usr/share/nginx/html
             volumes:
             - name: sharevol
               persistentVolumeClaim:
                 claimName: diskshare-pvc
       status: {}
     #+end_src

   + Create PVC and Deployment, verify all pods share the disk
     #+begin_src bash :results output replace
     #+end_src
** Resize existing volume in-place
   + [[https://kubernetes.io/docs/concepts/storage/persistent-volumes/#csi-volume-expansion][CSI Volume Expansion]] (k8s.io)
   + Resizing in-use volumes can only be done on specific storage classes that support
     dynamic resizing.  It is effected by editing the PersistentVolumeClaim object.
