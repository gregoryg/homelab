#+options: ':nil *:t -:t ::t <:t H:3 \n:nil ^:nil arch:headline author:t broken-links:nil
#+options: c:nil creator:nil d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t num:t
#+options: p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t timestamp:t title:t toc:t
#+options: todo:t |:t
#+title: Homelab scripts and config for Kubernetes clusters
#+date: <2020-06-13 Sat>
#+author: Gregory Grubbs
#+email: gregory@dynapse.com
#+language: en
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 27.0.91 (Org mode 9.3.6)
#+setupfile: ~/projects/emacs/org-html-themes/org/./theme-readtheorg-local.setup
# #+setupfile: https://raw.githubusercontent.com/fniessen/org-html-themes/master/setup/theme-readtheorg.setup
#+PROPERTY: header-args:bash :comments org :shebang #!/usr/bin/env bash :eval never-export

* Literate document setup
  This README is a literate programming document.  When this document is loaded in Emacs
  with Org Mode, it can generate scripts and configuration for the documented steps.


** Set up shell session
   This starts up or resuses a shell session named =homelab-sh= for interactive use.  Some
   setup in this document may use this session for stateful operations.

   #+begin_src emacs-lisp :keep-windows t :results none
     (switch-to-buffer (shell "homelab-sh"))
     (switch-to-buffer "README.org")
     (delete-other-windows  )
     (switch-to-buffer-other-window "homelab-sh")
     (switch-to-buffer-other-window "README.org")
   #+end_src

* Binaries and config
** Cow me, baby
   Assure =fortune= and =cowsay= are installed

   + Some additional cows
     + [[https://github.com/piuccio/cowsay/tree/master/cows][cowsay/cows at master · piuccio/cowsay · GitHub]]

   The cownonical Cow Me! script
   #+begin_src bash :tangle ~/bin/cowme :results replace output
     # Optionally specify a cow - whether or not in the safe list
     mycow=$1
     # cowfiles are in diff paths on openSUSE and Ubuntu, so include them both
     export COWPATH=/usr/share/cows:/usr/share/cowsay/cows:~/projects/homelab/cows
     if [ -z ${mycow} ] ; then
         IFS=',' read -r -a safe_cows <<< 'default,default,default,default,bud-frogs,elephant,small,three-eyes,tux,rancher,rancher-trademarked,rancher-k3os,rancher-k3os-trademarked,kitten,robotfindskitten,owl,hellokitty'
         mycow=${safe_cows[$((RANDOM % ${#safe_cows[@]}))]}
     fi
     case ${mycow} in
         rancher*|tux|chameleon)
             db=linux computers debian science startrek steven-wright
             ;;
         owl|satanic)
             db=cookie definitions steven-wright deep-thoughts off/zippy
             ;;
         ,*)
             db=
     esac
     cowcmd='cowsay'
     if [[ $((RANDOM % 2)) == 0 ]]; then
         cowcmd='cowthink'
     fi

     if [[ $(command -v fortune 2>/dev/null) && $(command -v cowsay 2>/dev/null) ]] ; then
         IFS=',' read -r -a cowopts <<< "b,g,p,s,t,w,y,d"
         fortune -s ${db} | ${cowcmd} -f ${mycow} -${cowopts[$((RANDOM % ${#cowopts[@]}))]}
         echo
     fi
   #+end_src
* For every Chroot there is a season
  #+begin_src bash
    cd /mnt/arch # or where you are preparing the chroot dir
    mount -t proc /proc proc/
    mount --rbind /sys sys/
    mount --rbind /dev dev/
  #+end_src
* Squid caching proxy
  TODO: update how-to for Synology DSM
** Server
  Instructions for setting up on Debian
  #+begin_src bash
    sudo apt install squid
  #+end_src

  Add caching for large objects! Put this in =/etc/squid/conf.d/gregs-cache.conf=
  #+begin_src conf
    # http_port 3128 transparent
    http_access allow all
    # ref https://superuser.com/a/972702/74209
    # we want to cache large objects
    maximum_object_size 6 GB
    cache_dir ufs /var/spool/squid 30720 16 256
    cache_mem 256 MB
    maximum_object_size_in_memory 512 KB
    cache_replacement_policy heap LFUDA
    range_offset_limit -1
    quick_abort_min -1 KB
  #+end_src
** Client
*** Proxy Apt for Ubuntu and Debian
    in =/etc/apt/apt.conf.d/proxy.conf=
    #+begin_src conf
      Acquire::http::Proxy "http://172.16.17.5:3128/";
      Acquire::https::Proxy "http://172.16.17.5:3128/";
    #+end_src
*** Proxy all the things on openSUSE
    Change the following in =/etc/sysconfig/proxy=
    #+begin_src conf
      PROXY_ENABLED="yes"
      HTTP_PROXY="http://172.16.17.5:3128/"
      HTTPS_PROXY="http://172.16.17.5:3128/"
      NO_PROXY="localhost,127.0.0.1,172.16.17.0/24,.magichome"
    #+end_src
* Docker registry for caching images

** The problem
    When setting up a Kubernetes RKE cluster, the same Docker image gets pulled on
    separate connections to each of the nodes.  A pull-through Docker registry would solve
    the problem, acting as a caching server for Docker images.

    However, Docker's built-in support will only work with images in the primary Docker
    registry.
     + [[https://dev.to/mayeu/saving-time-and-bandwidth-by-caching-docker-images-with-a-local-registry-98b][Saving Time and Bandwidth by Caching Docker Images With a Local Registry - DEV]]

** Solution
     =docker-registry-proxy= works with multiple registries.
     + [[https://github.com/rpardini/docker-registry-proxy][GitHub - rpardini/docker-registry-proxy: An HTTPS Proxy for Docker providing ...]]

** Server setup
   This is a proxy that also defaults to 3128 (already used by Squid) - so I'm forwarding
   to port 6128
    #+begin_src bash :tangle ~/bin/start-docker-registry-proxy.sh
      docker run -d --rm --name docker_registry_proxy -it \
             -p 0.0.0.0:6128:3128 \
             -v /data/docker_mirror_cache:/docker_mirror_cache \
             -v /data/docker_mirror_certs:/ca \
             -e REGISTRIES="k8s.gcr.io gcr.io quay.io" \
             -e AUTH_REGISTRIES="auth.docker.io:gregoryg:NLCsEKtk6cNeE5 quay.io:gregoryg:AJYgeUXbfjiRFNPiyM5Wrc+NiEBkIPe1lpjkp2erB6xaETMZowuaU6qLEkbFB7h+Rr4ExAoRrstcpLSt4c3zJtEJM/+mLQ3GCaQ9OeQ1Plc=" \
             rpardini/docker-registry-proxy:latest
             # tiangolo/docker-registry-proxy:latest


             # -e REGISTRIES="k8s.gcr.io gcr.io quay.io your.own.registry another.public.registry" \
             # -e AUTH_REGISTRIES="auth.docker.io:dockerhub_username:dockerhub_password your.own.registry:username:password" \
    #+end_src
** Client setup

    Create file =/etc/systemd/system/docker.service.d/http-proxy.conf=
    #+begin_src bash
      sudo mkdir -p /etc/systemd/system/docker.service.d
    #+end_src

    #+begin_src conf :tangle /sudo::/etc/systemd/system/docker.service.d/http-proxy.conf
      [Service]
      Environment="HTTP_PROXY=http://172.16.17.5:6128/"
      Environment="HTTPS_PROXY=http://172.16.17.5:6128/"
      Environment="NO_PROXY=localhost,127.0.0.1,docker-registry.example.com,.corp,quay.io"
    #+end_src

    Get the CA certificate from the proxy and make it a trusted root.  The directory for
    the certificate differs on OpenSUSE and Ubuntu
    #+begin_src bash
      if [ -d "/etc/pki/trust/anchors" ] ; then
          certdir=/etc/pki/trust/anchors
      else
          certdir=/usr/share/ca-certificates
      fi
      curl http://172.16.17.5:6128/ca.crt | sudo tee ${certdir}/docker_registry_proxy.crt
      echo "docker_registry_proxy.crt" | sudo tee -a /etc/ca-certificates.conf
      sudo update-ca-certificates --fresh
    #+end_src

    Reload and restart
    #+begin_src bash
      sudo systemctl daemon-reload
      sudo systemctl restart docker
    #+end_src

** Testing the clients

   + Clear =dockerd= of everything not currently running:
     #+begin_src bash
       docker system prune -a -f beware.
     #+end_src
   + Pull something, like
     #+begin_src bash
       docker pull ubuntu:20.04
     #+end_src
   + Watch the caching proxy logs on Lab-Server1
     #+begin_src bash
       docker logs docker_registry_proxy --follow
     #+end_src

    Then do, for example, docker pull k8s.gcr.io/kube-proxy-amd64:v1.10.4 and watch the
     logs on the caching proxy, it should list a lot of MISSes.

    Then, clean again, and pull again. You should see HITs! Success.

    Do the same for docker pull ubuntu and rejoice.

    Test your own registry caching and authentication the same way; you don't need docker
     login, or .docker/config.json anymore.
* Setting up Kubernetes distributions
** Set up initial barebones cluster
*** Apache Kubernetes using Kubeadm
    + [[file+emacs:01-k8s-distribution/apache-kubeadm/]]
*** Rancher Labs' RKE
    + [[file+emacs:01-k8s-distribution/rancher-rke/]]
    + Have a look at the two canonical starter configs from Rancher:
      + [[file+emacs:01-k8s-distribution/rancher-rke/templates/cluster-full-example.yaml-TEMPLATE][Cluster full example config]]
      + [[file+emacs:01-k8s-distribution/rancher-rke/templates/cluster-minimal-example.yaml-TEMPLATE][Cluster minimal example config]]
    + [[file+emacs:01-k8s-distribution/rancher-rke/cluster.yaml][My specific homelab config]]
    + Once you have the config to your liking, run
      #+begin_src bash
        # rke up
        rke up --config cluster.yaml --ssh-agent-auth
        KUBECONFIG=kube_config_cluster.yaml kubectl get nodes
      #+end_src
**** Install Rancher on the cluster
     #+begin_src bash
       kubectl create ns cattle-system
       helm install rancher rancher-latest/rancher \
            --namespace cattle-system \
            --set hostname=rancher.example.com
     #+end_src
     + To get the =cattle-cluster-agent= Deployment to resolve my =rancher.example.com=
       server URL, I had to add the following to =Deployment.spec.template.spec=
       #+begin_src yaml
         hostAliases:
         - hostnames:
           - rancher.example.com
           ip: 172.16.17.14
       #+end_src
*** Platform9 PMK
** Initialize Pod networking
   + SKIP THIS for RKE - Canal is already installed and configured
   + At the end of this step you should see all nodes reporting =ready= status
       #+begin_src bash
       kubectl apply -f https://docs.projectcalico.org/manifests/calico.yaml
       #+end_src
** Consider a cluster management solution
   + There are some options for getting a Web UI overview of either a single cluster or
     multiple clusters.  These will usually offer the ability to display resource usage,
     view and edit running resources, and create new resources.  Some allow higher level
     options like setting workloads to run on multiple clusters, deploying secrets and
     config maps across clusters, etc.
   + A great choice for this is Rancher (not RKE or K3s, which are Kubernetes distributions
     offered by Rancher Labs).  All you have to do to get started is to follow the guide at
     [[https://rancher.com/docs/rancher/v2.x/en/quick-start-guide/deployment/quickstart-manual-setup/][Rancher Docs: Manual Quick Start]].  The TL;DR is here.
     #+begin_src bash
     docker run --name rancher -d --restart=unless-stopped -p 0.0.0.0:80:80 -p 0.0.0.0:443:443 rancher/rancher
     #+end_src
   + Run this on any server you wish that can be seen by your cluster.  It can also be run
     on one of your cluster nodes, of course.
** Establish storage solution
   + I'm putting this step ahead of higher-level networking or *any* new objects that might
     create persistent volume claims
*** Longhorn
    + OSS project created by Rancher Labs
      #+begin_src bash
        kubectl apply -f  https://raw.githubusercontent.com/longhorn/longhorn/master/deploy/longhorn.yaml
      #+end_src
    + If you want to create easy access to the Longhorn UI, change the =longhorn-frontend=
      service to either NodePort or LoadBalancer.  If the latter, you will need to
      implement a load balancer solution such as MetalLB (see below)
*** Optionally make one storage class the default
    + Add annotation to the desired StorageClass resource
     #+begin_src yaml
       annotations:
         storageclass.kubernetes.io/is-default-class: "true"
     #+end_src
    + Check with =kubectl get sc=
    + Note that you can also install Longhorn using the Rancher UI if you are using that:
      Rancher -> Apps -> Launch -> Longhorn
*** Longhorn Service Monitor to feed Prometheus
    #+begin_src yaml
      apiVersion: monitoring.coreos.com/v1
      kind: ServiceMonitor
      metadata:
        name: longhorn-prometheus-servicemonitor
        namespace: cattle-monitoring-system
        labels:
          name: longhorn-prometheus-servicemonitor
      spec:
        selector:
          matchLabels:
            app: longhorn-manager
        namespaceSelector:
          matchNames:
          - longhorn-system
        endpoints:
        - port: manager
    #+end_src
** Set up Load Balancing and Ingress Controller
   + First step, let's make it possible to create =LoadBalancer= resources
   + On our bare metal cluster, we'll use MetalLB - be sure to check [[https://github.com/metallb/metallb/releases][releases]] to get the
     right URL
   + TODO: Investigate reserving host network IPs
     #+begin_src bash :session homelab-sh
       # use new namespace metallb-system
       kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.5/manifests/namespace.yaml
       kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.9.5/manifests/metallb.yaml
       # On first install only
       kubectl create secret generic -n metallb-system memberlist --from-literal=secretkey="$(openssl rand -base64 128)"
     #+end_src

   + Give MetalLB a pool of IPs
     + Here I'm using a pool from the primary home network
       #+begin_src yaml :tangle manifests/metallb-pool-cm.yaml
         apiVersion: v1
         kind: ConfigMap
         metadata:
           namespace: metallb-system
           name: config
         data:
           config: |
             address-pools:
             - name: default
               protocol: layer2
               addresses:
               - 172.16.17.230-172.16.17.250
       #+end_src
** Prepare for complex apps - Helm and Kudo
*** Helm
    + Install the Helm 3.x client from [[https://github.com/helm/helm/releases][Helm releases]]
    + That's all there is to do!  Installing a Helm chart will put required resources on
      the server
*** KUDO
    + Install the KUDO client from [[https://github.com/kudobuilder/kudo/releases][Kudo releases]]
    + This is a =kubectl= plugin; the binary is named =kubectl-kudo=.  It can be invoked
      as-is, but is meant to be used in conjunction with =kubectl=.  Place it in the Path
      and test it with
      #+begin_src bash
        kubectl kudo version
      #+end_src
    + Install server components with
      #+begin_src bash
        kubectl kudo init
      #+end_src
** Install a relational DB - MySQL
   + We will do this with the mature Helm chart
   + Change the root password below
     #+begin_src bash
       # Create the namespace we will use
       kubectl create ns sunshine
       helm install mysql stable/mysql \
            -n sunshine \
            --set mysqlRootPassword=adminpass,persistence.storageClass=longhorn,persistence.size=20Gi
     #+end_src
   + Note that the Longhorn UI should show a 20Gi volume.
   + To use the =mysql= CLI or other client, figure out whether you want to forward the
     port, use a NodePort or create a load balancer
** Install Apache Kafka
   + For this we will use KUDO, which offers a mature, purely declarative operator
   + Zookeeper first
     #+begin_src bash
       kubectl kudo install zookeeper --instance=zk
     #+end_src
   + Wait until all Zookeeper pods in your chosen namespace are ready, then
     #+begin_src bash
       kubectl kudo install kafka \
               --instance=kafka \
               -p ZOOKEEPER_URI=zk-zookeeper-0.zk-hs:2181,zk-zookeeper-1.zk-hs:2181,zk-zookeeper-2.zk-hs:2181
     #+end_src
** Tear down your cluster
*** Apache K8s with Kubeadm
*** Rancher's RKE
    #+begin_src bash
      rke remove --config cluster.yaml --ssh-agent-auth
    #+end_src
*** Platform 9 Systems' PMK
    #+begin_src bash
      sudo apt purge `dpkg -l | grep pf9|cut -d' ' -f3`
      sudo rm -rf /var/opt/pf9/ /opt/pf9/ /var/log/pf9/ /var/log/pods
    #+end_src
*** Cleanup after removal of any distribution
**** Some components may need manual removal
     #+begin_src bash
       sudo rm -rf /var/lib/longhorn
       sudo rm -rf /etc/cni/net.d/
     #+end_src
** Kubernetes notes
*** Add specific hosts to cluster DNS
    + ref: [[https://stackoverflow.com/a/40595719/457574][amazon web services - Create specific A record entry to Kubernetes local DNS ...]]
    #+begin_src yaml
      # Add config to dnsmasq used by kube-dns
      apiVersion: v1
      kind: ConfigMap
      metadata:
        name: kube-dns
        namespace: kube-system
      data:
        myhosts: |
          172.16.17.5 gorto gorto.magichome
    #+end_src
    #+begin_src yaml
      spec:
        template:
          spec:
            volumes:
            - name: extra-hosts
              configMap:
                name: kube-dns
            volumeMounts:
            - name: extra-hosts
              mountPath: /etc/hosts.d
            args:
            - --hostsdir=/etc/hosts.d
    #+end_src
*** Create volume that persists between multiple pod restarts
    :LOGBOOK:
    - State "DONE"       from              [2020-02-23 Sun 12:10]
    :END:
    + A volume that handles persistent storage using a PersistentVolumeClaim will survive
      Pod restarts.  This is true of Konvoy's default storage class on any cloud platform,
      and is true of persistent storage providers such as Portworx and Mayadata.
**** To show this on AWS
     + Define a PersistentVolumeClaim using the =awsebscsiprovisioner= storage class
       #+caption: hello-pvc.yaml
       #+begin_src yaml :tangle manifests/hello-pvc.yaml
         apiVersion: v1
         kind: PersistentVolumeClaim
         metadata:
           labels:
             app: hello-world
           name: hello-pvc
         spec:
           accessModes:
           - ReadWriteOnce
           resources:
             requests:
               storage: 10Gi
           storageClassName: awsebscsiprovisioner
           volumeMode: Filesystem
       #+end_src
       + Create a PVC using the above manifest.  List the resulting PVC resource and see
         that it is created and in a =Pending= state:

         #+begin_src bash :results table replace
           kubectl create -f manifests/hello-pvc.yaml
           kubectl get pvc -o wide
         #+end_src


       + Define a Pod that makes use of the PVC
         #+caption: myhello-pod.yaml
         #+begin_src yaml :tangle manifests/myhello-pod.yaml
           apiVersion: v1
           kind: Pod
           metadata:
             creationTimestamp: null
             labels:
               app: myhello
             name: myhello
           spec:
             containers:
             - image: nginxdemos/hello
               name: myhello
               resources: {}
               volumeMounts:
               - name: myhellovol
                 mountPath: /data
             dnsPolicy: ClusterFirst
             restartPolicy: Never
             volumes:
             - name: myhellovol
               persistentVolumeClaim:
                 claimName: hello-pvc
           status: {}
         #+end_src
         + Create the Pod, then list both the pod and the PersistentVolume that was created
           from the PVC.
           #+begin_src bash :results output replace
             kubectl create -f manifests/myhello-pod.yaml
             until [ $(kubectl get pods myhello -o jsonpath='{.status.phase}') == 'Running' ]; do sleep 1; done
             kubectl get pod,pv,pvc
           #+end_src

         + Create a file on the mounted volume, delete the pod, recreate the pod and verify
           that the file is still there
           #+begin_src bash  :results output replace
             kubectl exec myhello -- sh -c "touch /data/persistent.flag && ls /data/"
             kubectl delete pod myhello && kubectl create -f manifests/myhello-pod.yaml
             until [ $(kubectl get pods myhello -o jsonpath='{.status.phase}') == 'Running' ]; do sleep 1; done
             kubectl exec myhello -- sh -c "ls /data/"
           #+end_src

*** Create volume per instance of pod
    :LOGBOOK:
    - State "DONE"       from              [2020-02-23 Sun 12:10]
    :END:
    + This would be a volume used by each pod, and valid for the life of the individual
      Pod.  One reason to have this would be for multiple containers in the pod to indicate
      readiness and "liveness".  For this reason, the example will be a multi-container pod
      with an Init container writing a file to indicate readiness, and a container that
      periodically writes status for a liveness probe.

      #+caption: multivol-deployment.yaml
      #+begin_src yaml :tangle manifests/multivol-deployment.yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          creationTimestamp: null
          labels:
            app: multivol
          name: multivol
        spec:
          replicas: 3
          selector:
            matchLabels:
              app: multivol
          strategy: {}
          template:
            metadata:
              creationTimestamp: null
              labels:
                app: multivol
            spec:
              initContainers:
              - name: init1
                image: busybox
                command: ["sh", "-c", "touch /status/running"]
                volumeMounts:
                - name: statusdir
                  mountPath: /status
              containers:
              - name: nginx
                image: nginx
                resources: {}
                readinessProbe:
                  exec:
                    command: ["sh", "-c", "ls /opt/status/running && true"]
                volumeMounts:
                - name: statusdir
                  mountPath: /opt/status
              volumes:
              - name: statusdir
                emptyDir: {}
        status: {}
      #+end_src

      + Create, then describe the deployment.  Note the same volume is deployed at
        different mount points in each container
        #+begin_src bash  :results output replace
           kubectl create -f manifests/multivol-deployment.yaml
           until [ $(kubectl get pods -l app=multivol -o jsonpath='{.items[*].status.phase}' | grep 'Running' -o | wc -l) -eq 3  ]; do sleep 1; done
           kubectl describe deployment multivol | grep Mounts: -A 4
        #+end_src

*** Create volume shared between pods                                  :NEXT:
    :LOGBOOK:
    - State "MAYBE"      from "STARTED"    [2020-02-24 Mon 08:36]
    - State "STARTED"    from              [2020-02-23 Sun 12:10]
    :END:
    + This is a matter of mounting the volume as ReadWriteMany.  The underlying file system
      must support sharing across multiple nodes.  Examples of this type of file system
      include NFS and cloud implementations such as AWS EFS.

**** Example on AWS
     + Create an EFS file system in the AWS Console or CLI
     + Konvoy comes pre-installed with Helm and Tiller.  Install the EFS Provisioner using
       a Helm chart.  You will need the EFS file system ID and the AWS region it's in.  Use
       the below as a guide
       #+begin_src bash :results output replace
         helm install --name efs-provisioner \
              --namespace default \
              --set  efsProvisioner.efsFileSystemId=fs-d7a62e7d \
              --set efsProvisioner.awsRegion=us-west-2 \
              stable/efs-provisioner
       #+end_src

    + We will define a deployment with 3 replicas.  Each pod will mount the same persistent
      volume.  As before, the pods will mount a volume based on a PersistentVolumeClaim.

      #+caption: diskshare-pvc.yaml
      #+begin_src yaml :tangle manifests/diskshare-pvc.yaml
        apiVersion: v1
        kind: PersistentVolumeClaim
        metadata:
          labels:
            app: diskshare
          name: diskshare-pvc
        spec:
          accessModes:
          - ReadWriteMany
          resources:
            requests:
              storage: 6Ki
          storageClassName: aws-efs
          volumeMode: Filesystem
      #+end_src
      #+caption: diskshare-deployment.yaml
      #+begin_src yaml :tangle manifests/diskshare-deployment.yaml
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          creationTimestamp: null
          labels:
            app: diskshare
          name: diskshare
        spec:
          replicas: 3
          selector:
            matchLabels:
              app: diskshare
          strategy: {}
          template:
            metadata:
              creationTimestamp: null
              labels:
                app: diskshare
            spec:
              containers:
              - name: nginx
                image: nginx
                command: ["sh", "-c", "echo 'Wondrous Disk Content at WDC!' > /usr/share/nginx/html/index.html"]
                resources: {}
                volumeMounts:
                - name: sharevol
                  mountPath: /usr/share/nginx/html
              volumes:
              - name: sharevol
                persistentVolumeClaim:
                  claimName: diskshare-pvc
        status: {}
      #+end_src

    + Create PVC and Deployment, verify all pods share the disk
      #+begin_src bash :results output replace
      #+end_src
*** Resize existing volume in-place
    + [[https://kubernetes.io/docs/concepts/storage/persistent-volumes/#csi-volume-expansion][CSI Volume Expansion]] (k8s.io)
    + Resizing in-use volumes can only be done on specific storage classes that support
      dynamic resizing.  It is effected by editing the PersistentVolumeClaim object.
* Rancher Server
** Test full certificate chain of Rancher Server
   #+begin_src bash
     docker run superseb/ranchercheck https://rancher.hypecyclist.org:8443
   #+end_src
** Clean up after RKE
* RKE
** Clean up after RKE
   #+begin_src bash
     # on all nodes - get rid of ALL docker containers -- too general if anything else may be running
     WCOLL=~/projects/homelab/pdsh/all-nodes.txt pdsh -R ssh 'docker stop `docker ps -aq`'
     WCOLL=~/projects/homelab/pdsh/all-nodes.txt pdsh -R ssh 'docker rm `docker ps -aq`'
     WCOLL=~/projects/homelab/pdsh/all-nodes.txt pdsh -R ssh '# remove CNI and Longhorn remnants'
     WCOLL=~/projects/homelab/pdsh/all-nodes.txt pdsh -R ssh 'sudo rm -rfv /var/lib/longhorn/* /data/longhorn/* /etc/cni/* /var/lib/kubelet /etc/rancher /var/lib/rancher /etc/kubernetes'
     WCOLL=~/projects/homelab/pdsh/all-nodes.txt pdsh -R ssh '# iptables'
     WCOLL=~/projects/homelab/pdsh/all-nodes.txt pdsh -R ssh 'sudo iptables -F -t nat'
     WCOLL=~/projects/homelab/pdsh/all-nodes.txt pdsh -R ssh 'sudo iptables -X -t nat'
     WCOLL=~/projects/homelab/pdsh/all-nodes.txt pdsh -R ssh 'sudo iptables -F -t mangle'
     WCOLL=~/projects/homelab/pdsh/all-nodes.txt pdsh -R ssh 'sudo iptables -X -t mangle'
     WCOLL=~/projects/homelab/pdsh/all-nodes.txt pdsh -R ssh 'sudo iptables -F'
     WCOLL=~/projects/homelab/pdsh/all-nodes.txt pdsh -R ssh 'sudo iptables -X'
     WCOLL=~/projects/homelab/pdsh/all-nodes.txt pdsh -R ssh 'sudo systemctl restart docker'

   #+end_src
* Networking
  :PROPERTIES:
  :tangle-dir: 02-pod-networking/
  :END:
** Network policies
   Working directory (from base of the =homelab= directory)
   #+name: 02-pod-networking
*** Experiment with pure Kubernetes network policy
    + ref [[https://docs.projectcalico.org/security/kubernetes-network-policy][Get started with Kubernetes network policy]]

      This requires running pods with color labels (blue, red) and namespace labels*
*** Start pods
     #+begin_src bash
       kubectl create namespace k8snetpol
       kubectl -n k8snetpol run blue --image=nginx --labels app=blue,color=blue
       kubectl -n k8snetpol run red  --image=nginx --labels app=red,color=red
     #+end_src

     In this first example, inoming traffic to pods with label =color=blue= are allowed only
     if they come from a pod with =color=red= on port =80=
     #+begin_src yaml :tangle (concat (org-entry-get nil "tangle-dir" t) "/manifests/k8s-red-is-cool-for-blue-netpol.yaml")
       kind: NetworkPolicy
       apiVersion: networking.k8s.io/v1
       metadata:
         name: allow-same-namespace
         namespace: k8snetpol
       spec:
         podSelector:
           matchLabels:
             color: blue
         ingress:
         - from:
           - podSelector:
               matchLabels:
                 color: red
           ports:
             - port: 80
     #+end_src

     #+begin_src bash :dir 02-pod-networking/manifests/
       kubectl apply -f k8s-red-is-cool-for-blue-netpol.yaml
     #+end_src

*** Experiment with Calico network policies
    Following along with [[https://medium.com/flant-com/calico-for-kubernetes-networking-792b41e19d69][Calico for Kubernetes networking: the basics & examples]]

    One =nodejs= app deployment that has access to =redis=.  A php deployment that has no
     access to =redis=

     1058  k create deploy redis --image=redis
   1059  kx
   1060  k -n default get all
   1061  k scale deployment redis --replicas=3
   1062  k get all
   1063  k get pods -o wide
   1064  k get pods -o wide -w
   1065  k create deploy redis --image=readytalk/nodejs
   1066  k create deploy nodejs --image=readytalk/nodejs
   1067  k get all
   1068  k get ev
   1069  k get all
   1070  docker search php
   1071  k create deploy phpmyadmin --image=phpmyadmin/phpmyadmin
   1072  k get all
   1073  k get deployment --show-labels
   1074  pwd
   1075  history

**** Create deployments
***** Redis
      Created using
      + =kubectl create deploy redis --image=readytalk/nodejs -o yaml --dry-run=client=
      + =kubectl expose deployment redis --port=6379 --target-port=6379 -o yaml --dry-run=client=
      #+begin_src yaml :tangle (concat (org-entry-get nil "tangle-dir" t) "/manifests/redis-deployment.yaml")
        apiVersion: apps/v1
        kind: Deployment
        metadata:
          creationTimestamp: null
          labels:
            app: redis
          name: redis
        spec:
          replicas: 1
          selector:
            matchLabels:
              app: redis
          strategy: {}
          template:
            metadata:
              creationTimestamp: null
              labels:
                app: redis
            spec:
              containers:
              - image: readytalk/nodejs
                name: nodejs
                resources: {}
        status: {}
        ---
        apiVersion: v1
        kind: Service
        metadata:
          creationTimestamp: null
          labels:
            app: redis
          name: redis
        spec:
          ports:
          - port: 6379
            protocol: TCP
            targetPort: 6379
          selector:
            app: redis
        status:
          loadBalancer: {}
      #+end_src
***** Fake nodejs app
      Created using
      + =kubectl create deploy nodejs --image=readytalk/nodejs -o yaml --dry-run=client=
        #+begin_src yaml
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            creationTimestamp: null
            labels:
              app: nodejs
            name: nodejs
          spec:
            replicas: 1
            selector:
              matchLabels:
                app: nodejs
            strategy: {}
            template:
              metadata:
                creationTimestamp: null
                labels:
                  app: nodejs
              spec:
                containers:
                - image: readytalk/nodejs
                  name: nodejs
                  resources: {}
          status: {}
        #+end_src
***** Nodejs app
      Created using
      + =kubectl create deploy nodejs-hello --image=heroku/nodejs-hello-world -o yaml --dry-run=client=
        #+begin_src yaml :tangle (concat (org-entry-get nil "tangle-dir" t) "/manifests/nodejs-hello-deploy.yaml")
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            creationTimestamp: null
            labels:
              app: nodejs-hello
            name: nodejs-hello
          spec:
            replicas: 1
            selector:
              matchLabels:
                app: nodejs-hello
            strategy: {}
            template:
              metadata:
                creationTimestamp: null
                labels:
                  app: nodejs-hello
              spec:
                containers:
                - image: heroku/nodejs-hello-world
                  name: nodejs-hello-world
                - image: gregoryg/sh-net-utils
                  name: utils
                  command: ["sleep"]
                  args: ["1d"]
        #+end_src
***** PHP app
      Created using
      + =kubectl  create deploy phpmyadmin --image=phpmyadmin/phpmyadmin -o yaml --dry-run=client=
      + =kubectl expose deploy phpmyadmin --port=80 --target-port=80 -o yaml --dry-run=client=
        #+begin_src yaml
          apiVersion: apps/v1
          kind: Deployment
          metadata:
            creationTimestamp: null
            labels:
              app: phpmyadmin
            name: phpmyadmin
          spec:
            replicas: 1
            selector:
              matchLabels:
                app: phpmyadmin
            strategy: {}
            template:
              metadata:
                creationTimestamp: null
                labels:
                  app: phpmyadmin
              spec:
                containers:
                - image: phpmyadmin/phpmyadmin
                  name: phpmyadmin
                  resources: {}
          status: {}
          ---
          apiVersion: v1
          kind: Service
          metadata:
            creationTimestamp: null
            labels:
              app: phpmyadmin
            name: phpmyadmin
          spec:
            ports:
            - port: 80
              protocol: TCP
              targetPort: 80
            selector:
              app: phpmyadmin
          status:
            loadBalancer: {}
        #+end_src

**** Network policy
     #+begin_src yaml :tangle (concat (org-entry-get nil "tangle-dir" t) "/manifests/calico-block-redis-netpol.yaml")
       kind: NetworkPolicy
       apiVersion: networking.k8s.io/v1
       metadata:
         name: allow-redis-nodejs
       spec:
         podSelector:
           matchLabels:
             service: redis
         ingress:
         - from:
           - podSelector:
               matchLabels:
                 service: nodejs
           ports:
           - protocol: TCP
             port: 6379
     #+end_src
** Illuminatio - a tool to validate Network Policy
   Ref: [[https://www.inovex.de/blog/illuminatio-kubernetes-network-policy-validator/][illuminatio: the Kubernetes Network Policy Validator - inovex Blog]]

*** Install Illuminatio
   #+begin_src bash
   pip3 install illuminatio
   #+end_src
*** Get a deployment with service running
    #+begin_src bash
      kubectl create deployment web --image=nginx
      kubectl expose deployment web --port=80 --target-port=80
    #+end_src

*** Create and apply network policy to prohibit ingress to our deployment
    #+begin_src yaml :dir 02-pod-networking/ :tangle (concat (org-entry-get nil "tangle-dir" t) "/manifests/illuminatio-example-deny-all-netpol.yaml")
      apiVersion: networking.k8s.io/v1
      kind: NetworkPolicy
      metadata:
        name: web-deny-all
      spec:
        podSelector:
          matchLabels:
            app: web
        ingress: []
    #+end_src

    #+begin_src bash :dir 02-pod-networking/manifests/
      kubectl apply -f illuminatio-example-deny-all-netpol.yaml
    #+end_src

*** Test with Illuminatio
    #+begin_src bash
      illuminatio run
    #+end_src
* Observability
** Kubevious
   [[https://github.com/kubevious/kubevious][GitHub - kubevious/kubevious: Kubevious - application centric Kubernetes UI a...]]

   #+begin_src bash
     kubectl create namespace kubevious
     helm repo add kubevious https://helm.kubevious.io
     helm upgrade --atomic -i kubevious kubevious/kubevious --version 0.7.26 -n kubevious
     kubectl port-forward $(kubectl get pods -n kubevious -l "app.kubernetes.io/component=kubevious-ui" -o jsonpath="{.items[0].metadata.name}") 8080:80 -n kubevious
   #+end_src
* Storage
** NFS
   #+begin_src yaml
     apiVersion: v1
     kind: PersistentVolume
     metadata:
       name: data-k8s-pv
     spec:
       storageClassName: ""  # ignore default storage class
       capacity:
         storage: 1Gi
       accessModes:
         - ReadWriteMany
       persistentVolumeReclaimPolicy: Retain
       nfs:
         path: /data/data-files/k8s
         server: glados.magichome
         readOnly: false
   #+end_src
   #+begin_src yaml
     apiVersion: v1
     kind: PersistentVolumeClaim
     metadata:
       name: data-k8s-pv
     spec:
       storageClassName: ""  # ignore default storage class
       accessModes:
       - ReadWriteMany
       resources:
          requests:
            storage: 1Gi
   #+end_src
   #+begin_src yaml
     apiVersion: v1
     kind: Pod
     metadata:
       name: nginx-nfs-pod
       labels:
         name: nginx-nfs-pod
     spec:
       containers:
         - name: nginx-nfs-pod
           image: fedora/nginx
           ports:
             - name: web
               containerPort: 80
           volumeMounts:
             - name: nfsvol
               mountPath: /usr/share/nginx/html
       securityContext:
           supplementalGroups: [1000]
           # privileged: false
       volumes:
         - name: nfsvol
           persistentVolumeClaim:
             claimName: data-k8s-pv
   #+end_src
   #+begin_src yaml
     apiVersion: v1
     kind: Pod
     metadata:
       name: busybox-nfs-pod
       labels:
         name: busybox-nfs-pod
     spec:
       containers:
       - name: busybox-nfs-pod
         image: busybox
         command: ["sleep", "60000"]
         volumeMounts:
         - name: nfsvol-2
           mountPath: /usr/share/busybox
           readOnly: false
       securityContext:
         supplementalGroups: [1000]
         # privileged: false
       volumes:
       - name: nfsvol-2
         persistentVolumeClaim:
           claimName: data-k8s-pv
   #+end_src

* Cracking the structure of Rancher application ingress URLs
  + Examples of working parent-authenticated URLs
    + Rancher Server
      + Grafana dashboard
        + https://rancher.hypecyclist.org:8443/api/v1/namespaces/cattle-monitoring-system/services/http:rancher-monitoring-grafana:80/proxy/?orgId=1

    + Managed Cluster
      + Grafana dashboard
        + https://rancher.hypecyclist.org:8443/k8s/clusters/c-r5dj9/api/v1/namespaces/cattle-monitoring-system/services/http:rancher-monitoring-grafana:80/proxy/?orgId=1
        + https://rancher.hypecyclist.org:8443/k8s/clusters/c-r5dj9/api/v1/namespaces/longhorn-system/services/http:longhorn-frontend:80/proxy/
      + Longhorn UI
        + https://rancher.hypecyclist.org:8443/k8s/clusters/c-r5dj9/api/v1/namespaces/longhorn-system/services/http:longhorn-frontend:80/proxy/dashboard
    + Could it be this
      | L/M | Rancher URL                          | local vs managed     | cluster ID      | API    | namespace                | services   | URL to service                     | etc            |
      |-----+--------------------------------------+----------------------+-----------------+--------+--------------------------+------------+------------------------------------+----------------|
      | M   | https://rancher.hypecyclist.org:8443 | null or k8s/clusters | null or c-r5dj9 | api/v1 | longhorn-system          | 'services' | http:longhorn-frontend:80          | proxy          |
      | M   | https://rancher.hypecyclist.org:8443 | k8s/clusters         | c-r5dj9         | api/v1 | cattle-monitoring-system | 'services' | http:rancher-monitoring-grafana:80 | proxy/+orgId=1 |
      | L   | https://rancher.hypecyclist.org:8443 | null                 | null            | api/v1 | cattle-monitoring-system | 'services' | http:rancher-monitoring-grafana:80 | proxy/+orgId=1 |
** Use the Rancher API to get URLS
*** Authentication
    set env vars =rancher_access= and =rancher_secret= to the Access Key and Secret Key
    values from the Rancher UI API Keys

    For now, do this manually in the =homelab-sh= session
    #+begin_src bash
      read -p "Password: " rancher_access
      read -p "Password: " rancher_secret
      export rancher_access rancher_secret
    #+end_src
*** Get list of clusters known to Rancher Server with relevant info
    #+begin_src bash :session homelab-sh
      curl -s -k \
           -u "${rancher_access}:${rancher_secret}" \
           -X GET \
           -H 'Accept: application/json' \
           -H 'Content-Type: application/json' \
           'https://rancher.hypecyclist.org:8443/v3/clusters/' > /tmp/rancher-clusters.json
    #+end_src

    #+begin_src bash :results table
      cat /tmp/rancher-clusters.json | \
          jq -r '.data[] | "\(.name)\t\(.id)"'
    #+end_src
*** Get nice list of convenient API links for a specific cluster
    #+begin_src bash :results replace raw
      cat /tmp/rancher-clusters.json | \
          jq -r '.data[] | select (.name == "goozilla") | {"name": .name, "id": .id, "links": .links}'
    #+end_src
    {
      "name": "goozilla",
      "id": "c-vb78v",
      "links": {
        "apiServices": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/apiservices",
        "clusterAlertGroups": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/clusteralertgroups",
        "clusterAlertRules": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/clusteralertrules",
        "clusterAlerts": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/clusteralerts",
        "clusterCatalogs": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/clustercatalogs",
        "clusterLoggings": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/clusterloggings",
        "clusterMonitorGraphs": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/clustermonitorgraphs",
        "clusterRegistrationTokens": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/clusterregistrationtokens",
        "clusterRoleTemplateBindings": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/clusterroletemplatebindings",
        "clusterScans": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/clusterscans",
        "etcdBackups": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/etcdbackups",
        "namespaces": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/namespaces",
        "nodePools": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/nodepools",
        "nodes": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/nodes",
        "notifiers": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/notifiers",
        "persistentVolumes": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/persistentvolumes",
        "projects": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/projects",
        "remove": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v",
        "self": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v",
        "shell": "wss://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v?shell=true",
        "storageClasses": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/storageclasses",
        "subscribe": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/subscribe",
        "templates": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/templates",
        "tokens": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v/tokens",
        "update": "https://rancher.hypecyclist.org:8443/v3/clusters/c-vb78v"
      }
    }
* Dell XPS 13" 2 in 1 Laptop - Linux stuff
** Buzzing from headphone jack when nothing is playing
   The key is to set an option in the =snd-hda-intel= module.  =power_save=0= means never
   go into power saving mode
   + On Ubuntu 20.4
     in =/etc/modprobe.d/alsa-base.conf=
     #+begin_src conf
       # GJG stop the buzzing from headphone jack
       options snd-hda-intel power_save=0 power_save_controller=N
     #+end_src
   + On openSUSE Tumbleweed
     in =/etc/modprobe.d/42-power-audio.conf=
     #+begin_src conf
       options snd_hda_intel power_save=0 power_save_controller=0
     #+end_src
* Monitoring for RabbitMQ
  It may be necessary to create a Longhorn Volume/PVC named =data-rabbit-rabbitmq-0=
#+begin_src bash
  helm install rabbit bitnami/rabbitmq \
       --set persistence.storageClass=longhorn \
       --namespace rabbit \
       --set metrics.enabled=true \
       --set metrics.serviceMonitor.enabled=true
#+end_src

* Linux setup odds and ends
** Debian and Ubuntu deprecated =apt-key= follow these miserable steps instead
   Painful, but necessary for now
   ref: [[https://askubuntu.com/questions/1286545/what-commands-exactly-should-replace-the-deprecated-apt-key][What commands (exactly) should replace the deprecated apt-key? - Ask Ubuntu]]
   ref: [[https://www.linuxuprising.com/2021/01/apt-key-is-deprecated-how-to-add.html][apt-key Is Deprecated. How To Add OpenPGP Repository Signing Keys Without It ...]]
   + Set up the key url and file name, and open a shell in =/tmp=
   #+begin_src emacs-lisp
     (cd "/tmp")
     (shell "gg-tmp-sh")
   #+end_src

   #+name: key-url
   #+begin_src emacs-lisp :results table replace
     (read-string "URL for signing key: ")
   #+end_src

   + Download the key:
     Org-babel note: specify the org variables only on this first session definition to
     avoid being re-prompted for URL
   #+begin_src bash :session gg-tmp-sh :var keyurl=key-url :results none
     export keyfile=$(basename ${keyurl})
     curl -s -L -O ${keyurl}
   #+end_src


   + Verify that the filetype is =PGP public key block Public-Key (old)=
   #+begin_src bash :session gg-tmp-sh :results output replace
     file /tmp/${keyfile}
   #+end_src

   #+RESULTS:
   : /tmp/linux_signing_key.pub: PGP public key block Public-Key (old)

   + If your key is in a different format, convert it by importing it into a temp keyring,
     then exporting it again

     Here we go through the steps regardless because it doesn't hurt to "convert" in any case
   #+begin_src bash :session gg-tmp-sh :results output replace
     gpg --no-default-keyring --keyring ./temp-keyring.gpg --import ${keyfile}
     gpg --no-default-keyring --keyring ./temp-keyring.gpg --export --output ${keyfile}_keyring.gpg
     rm temp-keyring.gpg
   #+end_src

   #+RESULTS:
   #+begin_example
   gpg: keybox './temp-keyring.gpg' created
   " imported
   gpg: key 7721F63BD38B4796: 1 signature not checked due to a missing key
   " imported
   gpg: Total number processed: 2
   gpg:               imported: 2
   gpg: public key of ultimately trusted key 88F1B1F338F524AC not found
   gpg: public key of ultimately trusted key 8BED022043E6E8FC not found
   gpg: marginals needed: 3  completes needed: 1  trust model: pgp
   gpg: depth: 0  valid:   2  signed:   0  trust: 0-, 0q, 0n, 0m, 0f, 2u
   #+end_example

   + Now that you have your converted key, do *not* add it to apt's trusted keystore by
     copying it into =/etc/apt/trusted.gpg.d/=. Instead, put it somewhere like
     =/usr/local/share/keyrings/=. (You'll need to create that keyrings directory first.)
     #+begin_src bash :session gg-tmp-sh :results output replace
       sudo mkdir -p /usr/local/share/keyrings/
       sudo mv -iv ${keyfile}_keyring.gpg /usr/local/share/keyrings/
     #+end_src

     #+RESULTS:
     :
     : renamed 'gpgkey_keyring.gpg' -> '/usr/local/share/keyrings/gpgkey_keyring.gpg'

   + At this point, nothing has changed and apt doesn't know the key exists. The last step
     is to modify the specific =.list= file for the repository to tell apt where to find the
     key for that specific repo.

     Edit the file =/etc/apt/sources.list.d/<example>.list=, and in between deb and the url,
     add =[signed-by=/usr/local/share/keyrings/<your-keyfile-name>.gpg]=

     Now apt will accept that key's signature for all packages in that repo and only that
     repo.

* Graph databases
** Tigergraph
*** Tigergraph Download and Installation instructions
    + [[https://docs.tigergraph.com/admin/admin-guide/install-and-config/install][Detailed Instructions]]
    + Download the TigerGraph Enterprise Free Edition for Linux: https://dl.tigergraph.com/download.html
    + [[https://dl.tigergraph.com/enterprise-edition/tigergraph-3.1.6-offline-docker-image.tar.gz][Direct download for 3.1.6]]
    + [[http://gorto.magichome:8420/Linux/software/tigergraph-3.1.6-offline.tar][3.1.6 on erebor]] in =archive/Linux/software=
    + On Ubuntu 18.04, install these packages *prior to* running Tigergraph install
      #+begin_src bash
        sudo apt -y install net-tools ntp iptables-persistent
      #+end_src
    + The =iptables-persistent= package will prompt during install, preventing a scripted
      installation via =pdsh=

*** Cluster setup for non-interactive installs
    + Possible non-interactive config
       #+begin_src json
         {
           "BasicConfig": {
             "TigerGraph": {
               "Username": "gregj",
               "[comment]":"Provide password for tigergraph user, if the user already exists, we won't change the password. If the password is empty, we will set it to default value 'tigergraph'.",
               "Password": "tigergraph",
               "SSHPort": 22,
               "[comment]":"(Optional)Provide valid private key file below to replace tigergraph.rsa and tigergraph.pub, which will be generated by default.",
               "PrivateKeyFile": "",
               "PublicKeyFile": ""
             },
             "RootDir": {
               "AppRoot": "/data/tigergraph/app",
               "DataRoot": "/data/tigergraph/data",
               "LogRoot": "/data/tigergraph/log",
               "TempRoot": "/data/tigergraph/tmp"
             },
             "License": "eyJhbGciOiJSUzI1NiIsInR5cCI6IkpXVCJ9.eyJJc3N1ZXIiOiJUaWdlckdyYXBoIEluYy4iLCJBdWRpZW5jZSI6IlRpZ2VyR3JhcGggRnJlZSIsIlN0YXJ0VGltZSI6MTYyNTA0NTUyNCwiRW5kVGltZSI6MTY1OTE3NzEyNCwiSXNzdWVUaW1lIjoxNjI1MDQ5MTI0LCJFZGl0aW9uIjoiRW50ZXJwcmlzZSIsIlZlcnNpb24iOiJBbGwiLCJIb3N0Ijp7Ik1heENQVUNvcmUiOjEwMDAwMDAwMDAwMDAwMDAsIk1heFBoeXNpY2FsTWVtb3J5Qnl0ZXMiOjEwMDAwMDAwMDAwMDAwMDAsIk1heENsdXN0ZXJOb2RlTnVtYmVyIjoxMDI0fSwiVG9wb2xvZ3kiOnsiTWF4VmVydGV4TnVtYmVyIjoxMDAwMDAwMDAwMDAwMDAwLCJNYXhFZGdlTnVtYmVyIjoxMDAwMDAwMDAwMDAwMDAwLCJNYXhHcmFwaE51bWJlciI6MTAyNCwiTWF4VG9wb2xvZ3lCeXRlcyI6NTM2ODcwNjM3MTJ9LCJHU1QiOnsiRW5hYmxlIjp0cnVlLCJab29tQ2hhcnRzTGljZW5zZSI6IntcbiAgXCJsaWNlbnNlXCI6IFwiWkNCLTRtZmk3NDRsdjogWm9vbUNoYXJ0cyBFbnRlcnByaXNlIGxpY2VuY2UgZm9yIFRpZ2VyR3JhcGggZm9yIG9mZmxpbmUgdXNlOyB1cGdyYWRlcyB1bnRpbDogMjAyMi0xMi0zMVwiLFxuICBcImxpY2Vuc2VLZXlcIjogXCI4Zjg2ZWQzM2Y0ZWJmYmE4YTI4NjlkZGUzMmYzYTMwZGI4NGEzOTgxYmEzNmZhZWZlYTMxNDhhYzY4MzkxZTlhYzUzZDU3YmI5MjdmNjY5YWI1ZWJhYzJhYmQ3YTFkNDBiM2UxNDRmZjIwMDYyZWNiZmIwZjJiN2I0ZWFmZWIwYzU2NTc2NjBhZGExZDc1MWNhODU3NWZhYTE1ZWQwODI0NzkwZWQxMjJkY2Q4NjcyZTJiN2QzN2MwNmE3MzFhYTc2MDIwM2FhYmMwYjYzOWEzMjBhOGQxNmI0YmFiNGY5NTJiNTMwOTUzMWI4MDkxNjYwZDVjOGMzNGY0NmMyNjZiOGZiNzc2YzFmN2Y0MTdlMGQ5Y2JkZGFlOTExNTFlY2Y3YmMzZDlkNDgyNWE2MjAwYzk0MWMyMDE4ZDY4YjkyOWE5ZWY2MzQ2MDE5NjFhYmU1MGI0ZTk0ZmY5Y2VjMjA1ODEwYmVlZmRkM2NlZDU2YjM5NTVjYmE0YWIyMGNiNzc5MWE0NmQxNzIwMzNiZmI0ZDIyMDM4ODZhZTllZDFkOWMzOWIyOTM2ODc3Yjc2NzY4ZjQwNWQ5Y2MwY2JlODVjOTE2NDllMDI5YTA0NDFlOGFmYjA2MzY4MTMxZGM1YTc1NDEyOTc1NjFlMDRlMGM1MzE1ZmFjMDdhYzViOWViM1wiXG59In0sIlJ1bnRpbWVNZW1vcnkiOnsiTWF4VXNlclJlc2lkZW50U2V0Qnl0ZXMiOjEwMDAwMDAwMDAwMDAwMDB9fQ.dwuNMEMTKjn50PASHEGv4Pqd2uter7OtpEe2s3QUaw8h-h7cmcwTnZAzptbnvjIelrPUHeTwS-N3cayVGl1Hig2vU34bqp3CyhsE2s6AjVeTvEENrLXAZRSyf1OIEr22c2oo42TXiihCrntXkvsGO3H0AQERauSvKffR4CVtSb1AzrzObECuxggAwdoGDElEGXO2aS3X3j3pf3XIIt31eiVQklnAVLCcQKqc-oMUFrHIRf-SrGqLh6FWJs3vWTlC7pIj9NrcKIfwTLkbMAtj--5ZVsSBaGgIeAZ4YqziTj2ZgsYmVD_aBpgiKSUaYPm4HjTMPbgdKAH7nRGuXbjeog",
             "[comment]":"You can add more nodes by string 'node_id: IP', appending to the following json array. Otherwise, it installs single node locally by default.",
             "NodeList": [
                 "m1: 172.16.17.198",
                 "m2: 172.16.17.79",
                 "m3: 172.16.17.64"
             ]
           },
           "AdvancedConfig": {
             "[comment]": "Keep the default ClusterConfig if installing locally",
             "ClusterConfig": {
               "[comment]": "All nodes must have the same login configurations",
               "LoginConfig": {
                 "SudoUser": "gregj",
                 "[comment]": "choose login method: 'P' for SSH using password or 'K' for SSH using key file (e.g. ec2_key.pem)",
                 "Method": "K",
                 "P": "sudoUserPassword",
                 "K": "/home/gregj/.ssh/k8s-local"
               },
               "[comment]": "To install a high-availability cluster, please specify the ReplicationFactor greater than 1",
               "ReplicationFactor": 3
             }
           }
         }
      #+end_src

*** DONE Learn some Tigergraph
    DEADLINE: <2021-06-19 Sat 18:55>
    :LOGBOOK:
    - State "DONE"       from "STARTED"    [2021-06-27 Sun 15:45]
    CLOCK: [2021-06-27 Sun 12:14]--[2021-06-27 Sun 15:45] =>  3:31
    CLOCK: [2021-06-27 Sun 08:42]--[2021-06-27 Sun 08:57] =>  0:15
    CLOCK: [2021-06-20 Sun 15:02]--[2021-06-20 Sun 15:18] =>  0:16
    CLOCK: [2021-06-20 Sun 11:51]--[2021-06-20 Sun 14:46] =>  2:55
    CLOCK: [2021-06-20 Sun 08:10]--[2021-06-20 Sun 10:30] =>  2:20
    CLOCK: [2021-06-19 Sat 22:29]--[2021-06-19 Sat 23:20] =>  0:51
    CLOCK: [2021-06-19 Sat 22:21]--[2021-06-19 Sat 22:28] =>  0:07
    CLOCK: [2021-06-19 Sat 18:56]--[2021-06-19 Sat 20:34] =>  1:38
    - State "STARTED"    from "TODO"       [2021-06-19 Sat 18:56]
    CREATED: [2021-06-19 Sat 18:55]
    :END:

**** Pre-requisites and setup
     + Generate startup script - very large image (~3GB)
       #+begin_src bash :tangle ~/bin/start-tigergraph.sh
         docker run -d \
                -p 14022:22 \
                -p 3306:3306 \
                -p 9000:9000 \
                -p 14240:14240 \
                --name tigergraph_server \
                --ulimit nofile=1000000:1000000 \
                -v ~/data:/home/tigergraph/mydata \
                tigergraphbootcamp/tigergraph-image:latest
       #+end_src
     + If docker instance was stopped, just run
       #+begin_src bash
         docker start tigergraph_server
       #+end_src

**** Terminology
      #+begin_example
        In computing, a graph database is a database that uses graph
        structures for semantic queries with nodes (AKA vertices), edges, and
        attributes to represent and store data
      #+end_example
      + Nodes / Vertices
        + A vertex represents *anything*: a company, person, product etc
      + Edges
        + lines that connect vertices
      + Attributes
        + associated with vertices
      + Hops
        + distance between 2 vertices - the number of edges *traversed*
      + Directionality (?)
        + A single graph may have multiple vertex types (i.e. Person and Store), and
          multiple edge types (i.e. Friendship and Buys From)
        + Friendship goes both ways - we call this an undirected edge
        + Buys From would be a directed edge
        + Graph visualizations typically show an arrowhead on the edges between vertices
        +
      +

**** Database evolution
     + relational
       + acid compliant
       + great for transactions
       + good for analytics
       + standard SQL
     + nosql
       + unstructured - or ... less structured
       + limited ACID compliance
       + identifying relationships is hard
     + graph
       + relationships
       + AWS Neptune, Neo4j, Tigergraph
       + no standard query language has emerged
       + not well suited to traditional analytics

**** What is Tigergraph
     + speed
       + native graph storage
       + data compression
       + MPP
       + efficient distributed computation
     + Scale-out
       +
     + Deep-link analytics
       + Queries can traverse 10 or more hops
     + Graph Query Language
       + GraphSQL or GSQL
     + Multigraph
       + multiple groups can share the same master database
     + Visual Interface
     + Developer, Cloud, Enterprise

**** Running Tigergraph on Docker (for MacOS)
     + Run or start up  the script generated under Pre-requisites section
       #+begin_src bash
         sudo systemctl start docker
         # ~/bin/start-tigergraph.sh
         docker run tigergraph_server
       #+end_src
**** GSQL
     + We will use GSQL to
       + define a graph schema
       + create a graph
       + load data into the graph
       + run graph queries
     + define a graph schema: vertices and edges

**** GSQL Getting Ready
     + for data files to be loaded: drop them into =~/data= provided by the =docker run=
       volume mount
***** SSH to Tigergraph server
      #+begin_src bash
        ssh-add ~/.ssh/tigergraph_rsa
      #+end_src
      #+begin_src emacs-lisp
        (call-process-shell-command "ssh -o StrictHostKeyChecking=no -p 30022  tigergraph@protomolecule id")
        (cd "/ssh:tigergraph@protomolecule:#30022:")
        (shell "tigergraph-sh")
      #+end_src
***** Start Tigergraph
      #+begin_src bash :session tigergraph-sh :results value
        time gadmin start all
      #+end_src
***** Get GSQL goin'
      =gsql=

**** Create a graph
     #+begin_src sql
       CREATE VERTEX person (PRIMARY_ID id INT, first_name STRING, last_name STRING, age INT, email STRING, gender STRING, phone STRING)
     #+end_src
     #+begin_src sql
       CREATE UNDIRECTED EDGE friendship (FROM person, TO person, friendship_date DATETIME)
     #+end_src
     #+begin_src sql
       CREATE GRAPH friends (person, friendship)
     #+end_src
     + Shortcuts to create empty graph or a graph using all global edges and vertices
       and edges
       #+begin_src bash
         CREATE GRAPH GraphName()
         CREATE GRAPH GraphName(*)
       #+end_src
**** Create loading job
     + =USE GRAPH friends=
     + When we create a graph, we are techincally creating a "local graph" TigerGraph
       already has one graph named Global.  It can be leveraged by other graphs
     + To exit the scope of your graph: =USE GRAPH global=
     #+begin_src sql
       CREATE LOADING JOB friends_data FOR GRAPH friends {       DEFINE FILENAME people = "/home/tigergraph/mydata/people.csv";       DEFINE FILENAME friendship = "/home/tigergraph/mydata/friendship.csv";       LOAD people TO VERTEX person VALUES ($"id", $"first_name", $"last_name", $"age", $"email", $"gender", $"phone") USING header="true", separator=",";       LOAD friendship TO EDGE friendship VALUES ($"from_id", $"to_id", $"friendship_date") USING header="true", separator=",";}
     #+end_src
     + Pro tip: you can map using position or field name: ($1, $2...) instead of ($"id", $"first_name"...)
**** Run the loading job
     + =run loading job friends_data=
     + take note of job id
       + =show loading status friends.friends_data.file.m1.1624199308621=
     + default data storage path is =/tigergraph/data/gstore=
     + when in doubt use =gstatusgraph= to check the graph data storage path
**** GSQL - Delete stuff
     + =DROP=
       #+begin_src sql
         drop vertex <vertex_name>
         drop edge <edge_name>
         drop job <job_name>
         drop graph <graph_name>
         -- delete all vertices/edges, jobs, queries and data!
         drop all
       #+end_src
     +
**** GSQL - Query basics
     + interactive and saved queries
       #+begin_src sql
         select * FROM person-(friendship)->person WHERE from_id == 456

         CREATE QUERY close_friends(VERTEX <person>p) FOR GRAPH friends{Start = {p}; Result = SELECT tgt FROM Start:src -(friendship:e) -> person:tgt; PRINT Result;}
         INSTALL QUERY close_friends
         RUN QUERY close_friends(456)
       #+end_src
     + running saved queries using API
       #+begin_src bash
         curl -X GET http://localhost:9000/query/friends/close_friends?p=456
       #+end_src
     + Built-in queries - available using the API
       #+begin_src bash :session tigergraph-sh :results output replace
         # --List all information about a specific vertex
         # curl -X GET "http://server:port/graph/<graph_name>/vertices/<vertex_name>/<vertex_id>"
         # --List all vertices originating from a specific vertex, traversing a specific edge type
         # curl -X GET "http://server:port/graph/<graph_name>/edges/<source> <vertex_name>/<source> <vertex_id>/<edge_name>/"
         # --List TigerGraph version information
         # curl -X GET "http://server:port/version"

         curl -s -X GET "http://localhost:9000/graph/friends/vertices/person/123" | jq -r '.'
         curl -s -X GET "http://localhost:9000/graph/friends/vertices/person/496"|jq -r '.'
       #+end_src
     + GSQL: return specific vertex information
       #+begin_src sql
         SELECT * FROM person where email =="gmaslen3e@fastcompany.com"
       #+end_src
     + return multiple edges meeting conditions
       + list all friendship edge connected to Gabriella
         #+begin_src sql
            SELECT friendship_date FROM person-(friendship)->person WHERE from_id == 123
         #+end_src
**** GSQL - Advanced - Saved Queries
     + Named query syntax

       [[file:images/tigergraph-udemy-named-query-syntax.png]]

     + Query 1: Return all info for a specific vertex
       #+begin_src sql
         CREATE QUERY personal_info (STRING email) FOR GRAPH friends {  all_people = {person.*}; info = SELECT s FROM all_people:s WHERE s.email==email; PRINT info; }
         INSTALL QUERY personal_info
         RUN QUERY personal_info("apeekeb6@chronoengine.com")
         RUN QUERY personal_info("gmaslen3e@fastcompany.com")
       #+end_src
     + Query 2: list all women aged 20-30 who are friends with person X
       #+begin_src sql
         CREATE QUERY female_20s_friends (VERTEX <person> p) FOR GRAPH friends { start = {p}; result = SELECT tgt FROM start:src -(friendship:e)- person:tgt WHERE tgt.gender == "Female" AND tgt.age >= 20 AND tgt.age <= 30; PRINT result;}
         INSTALL QUERY female_20s_friends
         RUN QUERY female_20s_friends(123)
       #+end_src
**** Graph Studio intro
     + http://localhost:14240/
**** Graph Studio hands on
     + create a professional social network (more complex graph than previous)
     + Subtle differences in terminology
       | GraphStudio       | GSQL                        |
       |-------------------+-----------------------------|
       | Create Graph      | CREATE GRAPH                |
       | Design Schema     | CREATE VERTEX / CREATE EDGE |
       | Map Data to Graph | CREATE LOADING JOB          |
       | Load Data         | RUN LOADING JOB             |
     + Do all the following steps
       1. Start TigerGraph
       2. Open GraphStudio at http://localhost:14240/
       3. Create a Graph
         a. Click Global View, then Create a graph. Set the name to LinkedUp
       4. Create Vertices (Local)
          - Vertex type name: =Account=
            - Primary id: id
            - Primary id type: INT
            - Color: #FF6D00
            - Icon: person
            - Attributes (attribute type is STRING unless indicated otherwise)
              - FirstName
              - LastName
              - Email
              - Gender
              - JobTitle
              - Salary (DOUBLE)
              - Recruitable (BOOL)
          - Vertex type name: =Company=
            - Primary id: id
            - Primary id type: INT
            - Color: #C1D82F
            - Icon: company
            - Attributes (attribute type is STRING unless indicated otherwise)
              - name
          - Vertex type name: =City=
            - Primary id: id
            - Primary id type: INT
            - Color: #F8B717
            - Icon: upload and use city icon
            - Attributes (attribute type is STRING unless indicated otherwise)
              - Name
          - Vertex type name: =State=
            - Primary id: id
            - Primary id type: INT
            - Color: #FF3E02
            - Icon: upload and use state icon
            - Attributes (attribute type is STRING unless indicated otherwise)
              - name
          - Vertex type name: =Industry=
            - Primary id: id
            - Primary id type: INT
            - Color: #6871FF
            - Icon: upload and use industry icon
            - Attributes (attribute type is STRING unless indicated otherwise)
              - Name
       5. Create Edges (local)
          1) Edge type name: =connected_to=
             - From -> To: Account -> Account
             - Directed: No
             - Color: #FF6D00
          2) Edge type name: =works_in=
             - From -> To: Account -> Company
             - Directed: Yes
             - Color: #C1D82F
          3) Edge type name: =in_industry=
             - From -> To: Company -> Industry
             - Directed: Yes
             - Color: #6871FF
          4) Edge type name: =located_in=
             - From -> To: Company -> City
             - Directed: Yes
             - Color: #F8B717
          5) Edge type name: =is_in=
             - From -> To: City -> State
             - Directed: Yes
             - Color: #FF3E02
       6. Download and save CSV files from this video.
          - Into your =data= folder. You may create a subfolder named =LinkedUp= to keep your files organized.
       7. Map Data to Graph
          - In GraphStudio, select the =Map Data to Graph= link from the menu.
          - Click Add data file, browse to your LinkedUp folder and upload all 10 files.
          - Select the =account.csv= file from the =Files on Server= section, and check the
            =Has Header= checkbox, then click =Add=.
          - Click =map data file to vertex or edge=, then select the =account.csv= file
            icon, and then the =account= vertex. This will result in the mapping pane being displayed on the right.
            - Map the fields by clicking the field name in the source table (which is the
              CSV file), then selecting the field to map in the target (which is a vertex or edge).
          - Repeat the file mapping process for the remaining 9 vertices and edges
          - Click =publish data mapping=.
       8. Load Data
          - In GraphStudio, select =Load Data= from the menu.
          - Click the =Start/Resume Loading= button.
          - Click =Confirm= to start loading
          - Wait until all files have the =FINISHED= badge.
**** Graph Studio - explore graph
     transcript: [[file:graph/tigergraph/udemy/documents/GraphStudio-exploregraph.org]]
**** Graph Studio - add GSQL queries
     + THE QUERY
       #+begin_src sql
         // 1. Return all companies to which I am connected through my direct neighbours. Only include companies in city New York City and Industry Aerospace.
         // Vertex to test with: 291

         CREATE QUERY company_network(VERTEX <account> p) FOR GRAPH linkedup {
             //Define the Start point as the person in the parameter
             Start = {p};

             // Get all the contacts to said person
             contact_list= SELECT tgt_p
                         FROM Start:src - (connected_to) - account:tgt_p ;

             // Get all the companies in which contacts work
             contacts_companies= SELECT c
                               FROM contact_list:src - (works_in) -> company:c;

             // Get all the companies with location in New York
             companies_new_york= SELECT src
                                 FROM contacts_companies:src - (located_in) - city:c
                                 WHERE c.city_name=="New York";

             // Get all the companies in Aerospace Industry
             companies_new_york_industry = SELECT src
                                          FROM companies_new_york:src - (in_industry) - industry:i
                                          WHERE i.industry_name=="Aerospace";

             //Return all the companies
             PRINT companies_new_york_industry;
         }
       #+end_src

**** GraphStudio - visual query builder
     + Question: which companies are operating in the Aerospace industry, have an offfice
       based in New York and employ a contact of (person)?
     +

**** GraphStudio - RDBMS migration tool

**** Use case: Hub & Community detection
     + Identify the most influential member in a group and the community around them
***** Create new graph schema
****** Vertex =account=
       #+begin_src sql
         CREATE VERTEX account (
           PRIMARY_ID account_id INT,
           user_name STRING,
           first_name STRING,
           last_name STRING,
           email STRING,
           gender STRING,
           age INT)
           WITH PRIMARY_ID_AS_ATTRIBUTE = "true"
       #+end_src
****** Vertex =hobby=
       #+begin_src sql
         CREATE VERTEX hobby (
           PRIMARY_ID hobby_id INT,
           description STRING)
           WITH PRIMARY_ID_AS_ATTRIBUTE = "true"
       #+end_src
****** Edges
       #+begin_src sql
         CREATE DIRECTED EDGE follows (FROM account, TO account)
         CREATE DIRECTED EDGE referred_by (FROM account, TO account, referral_date DATETIME)
         CREATE DIRECTED EDGE interested_in (FROM account, TO hobby)
       #+end_src
****** Create graph =influencers=
       #+begin_src sql
         CREATE GRAPH influencers(account, hobby, follows, referred_by, interested_in)
       #+end_src
****** Create loading job
       #+begin_src sql
         USE GRAPH influencers

         BEGIN
         CREATE LOADING JOB load_influencers FOR GRAPH influencers {
           DEFINE FILENAME person_data="/home/tigergraph/mydata/insta-follow/users.csv";
           DEFINE FILENAME followers_data="/home/tigergraph/mydata/insta-follow/followers.csv";
           DEFINE FILENAME referred_data="/home/tigergraph/mydata/insta-follow/referrals.csv";
           DEFINE FILENAME interests="/home/tigergraph/mydata/insta-follow/interests.csv";
           DEFINE FILENAME people_interests="/home/tigergraph/mydata/insta-follow/people_interests.csv";

           LOAD person_data TO VERTEX account VALUES ($"id", $"user_name",
             $"first_name", $"last_name", $"email", $"gender",$"age")
             USING header="true", separator=",";
           LOAD interests TO VERTEX hobby VALUES ($"id", $"interest")
             USING header="true", separator=",";
           LOAD followers_data TO EDGE follows VALUES ($"from_id", $"to_id")
             USING header="true", separator=",";
           LOAD referred_data TO EDGE referred_by VALUES ($"from_id", $"to_id",
             $"referral_date")
             USING header="true", separator=",";
           LOAD people_interests TO EDGE interested_in VALUES ($"id", $"int_id")
             USING header="true", separator=",";
         }
         END
       #+end_src
****** Load the data
       #+begin_src sql
         USE GRAPH influencers
         RUN LOADING JOB load_influencers
       #+end_src
****** Try a bunch of queries
       + Sample queries for this module

         file:databases/tigergraph/udemy/documents/InstaFollow+Sample+Queries.sql

**** Use case: pattern matching
     + Search a graph for all occurences of a
***** Create vertices and edges
      #+begin_src sql
        CREATE VERTEX customer (
          PRIMARY_ID customer_id INT,
          first_name STRING,
          last_name STRING,
          user_name STRING,
          email STRING,
          gender STRING,
          date_of_birth DATETIME)
          WITH primary_id_as_attribute="true"

        CREATE VERTEX product (
          PRIMARY_ID product_id INT,
          product_name STRING,
          product_category STRING)
          WITH primary_id_as_attribute="true"

        CREATE DIRECTED EDGE has_purchased (FROM customer, TO product, purchase_date DATETIME)
      #+end_src
***** Create graph
      #+begin_src sql
        CREATE GRAPH ecommerce( customer, product, has_purchased)
      #+end_src
***** Create loading job
      #+begin_src sql
        USE GRAPH ecommerce

        BEGIN
        CREATE LOADING JOB load_ecommerce FOR GRAPH ecommerce {
          DEFINE FILENAME customer_data = "/home/tigergraph/mydata/ecommerce/customer.csv";
          DEFINE FILENAME product_data = "/home/tigergraph/mydata/ecommerce/product.csv";
          DEFINE FILENAME purchase_data = "/home/tigergraph/mydata/ecommerce/purchases/";

          LOAD customer_data TO VERTEX customer
          VALUES (
            $"customer_id",
            $"first_name",
            $"last_name",
            $"user_name",
            $"email",
            $"gender",
            $"date_of_birth")
          USING header="true", separator=",";

          LOAD product_data TO VERTEX product
          VALUES (
            $"product_id",
            $"product_name",
            $"product_category")
          USING header="true", separator=",";

          LOAD purchase_data TO EDGE has_purchased
          VALUES (
            $"customer_id",
            $"product_id",
            $"purchase_date")
          USING header="true", separator=",";
        }
        END
      #+end_src
***** Run loading job
      #+begin_src sql
        USE GRAPH ecommerce

        RUN LOADING JOB load_ecommerce
      #+end_src
***** Run some queries
      + GraphStudio was used in the course
      + Query 1: Return Customers Adhering to Given Pattern
        // Products: 1,2,3,4,5,6

        #+begin_src sql
          use graph ecommerce
          /* 1. Return Customers Adhering to Given Pattern */
          /* Products: 1,2,3,4,5,6 */
          CREATE QUERY find_customers_with_pattern(Vertex<product>p_1, Vertex<product>p_2, Vertex<product>p_3, Vertex<product> p_4, Vertex<product> p_5, Vertex<product>p_6) FOR GRAPH ecommerce SYNTAX v2 {

              /* Define the Start point as all customers in the Shop */
              Start = {customer.*};

              /* Get all the customers exhibiting the pattern */
              customers_with_pattern= SELECT src
                                      FROM Start:src - (has_purchased>:h) - product:p,
                                    Start:src - (has_purchased>:h2) - product:p2,
                                    Start:src - (has_purchased>:h3) - product:p3,
                                    Start:src - (has_purchased>:h4) - product:p4,
                                    Start:src - (has_purchased>:h5) - product:p5,
                                    Start:src - (has_purchased>:h6) - product:p6
                                      WHERE
                                          h.purchase_date< h2.purchase_date AND
                                          h2.purchase_date< h3.purchase_date AND
                                          h3.purchase_date< h4.purchase_date AND
                                          h4.purchase_date < h5.purchase_date AND
                                          h5.purchase_date< h6.purchase_date AND
                                          p==p_1 AND p2==p_2 AND p3==p_3 AND p4==p_4 AND p5==p_5 AND p6==p_6;

              /* Return all the customers */
              PRINT customers_with_pattern;
          }
        #+end_src
      + Query 2: Return customers adhering to *start* of given pattern
        #+begin_src sql
          use graph ecommerce
          /* 2. Return Customers Adhering to Start of Given Pattern */
          /* Products: 1,2,3 */
          /* End_Pattern: 4,5,6 */
          CREATE QUERY find_customers_with_first_3_pattern(Vertex<product>p_1, Vertex<product>p_2, Vertex<product>p_3, SET<int> end_pattern) FOR GRAPH ecommerce SYNTAX v2 {

              SetAccum<Vertex<customer>> @@customers_with_pattern_b;
              SetAccum<Vertex<customer>> @@customers_with_pattern_e;
              SetAccum<Vertex<customer>> @@final_list;


              /* Define the Start point as all customers in the Shop */
              Start = {customer.*};

              /* Get all the customers exhibiting the beginning pattern */
              customers_with_pattern_b= SELECT src
                                        FROM Start:src - (has_purchased>:h) - product:p,
                                      Start:src - (has_purchased>:h2) - product:p2,
                                      Start:src - (has_purchased>:h3) - product:p3
                                        WHERE
                                            h.purchase_date< h2.purchase_date AND
                                            h2.purchase_date< h3.purchase_date AND
                                            p==p_1 AND p2==p_2 AND p3==p_3
                                            ACCUM @@customers_with_pattern_b+=src
              ;


              /* Get all the customers exhibiting the end of the pattern */
              customers_with_pattern_e= SELECT src
                                        FROM Start:src - (has_purchased>:h) - product:p
                                        WHERE
                                            p.product_id in end_pattern
                                            ACCUM @@customers_with_pattern_e+=src
              ;

              /* Get all the customers with the beginning of the pattern but no product on the end of the pattern */
              @@final_list=@@customers_with_pattern_b MINUS @@customers_with_pattern_e;

              /* Return all the customers */
              PRINT @@final_list;
          }
        #+end_src
**** Use case: Deep link analysis
     + Traverse a graph via three or more hops and then analyze the data encountered in
       that traversal
***** Create vertices and edges
      #+begin_src sql
        CREATE VERTEX user_account (
          PRIMARY_ID account_id INT,
          user_name STRING,
          member_since DATETIME,
          last_login DATETIME)
          WITH primary_id_as_attribute="true"

        CREATE VERTEX movie (
          PRIMARY_ID movie_id INT,
          title_type STRING,
          primary_title STRING,
          original_title STRING,
          is_adult BOOL,
          release_year INT,
          runtime_minutes INT,
          genres SET<STRING>)
          WITH primary_id_as_attribute="true"

        CREATE VERTEX cluster (PRIMARY_ID cluster_id INT, description STRING) WITH primary_id_as_attribute="true"

        CREATE DIRECTED EDGE watched (FROM user_account, TO movie)
        CREATE DIRECTED EDGE recommended_to (FROM movie, TO user_account)
        CREATE DIRECTED EDGE belongs_to (FROM user_account, TO cluster)
      #+end_src
***** Create graph
      #+begin_src bash
        CREATE GRAPH streaming (user_account, movie, cluster, watched, recommended_to, belongs_to)
      #+end_src
***** Create loading job
      #+begin_src sql
        USE GRAPH streaming

        BEGIN
        CREATE LOADING JOB load_streaming FOR GRAPH streaming {
          DEFINE FILENAME user_data="/home/tigergraph/mydata/streaming/user.csv";
          DEFINE FILENAME movie_data="/home/tigergraph/mydata/streaming/movies.csv";
          DEFINE FILENAME clusters_data="/home/tigergraph/mydata/streaming/clusters.csv";
          DEFINE FILENAME watched="/home/tigergraph/mydata/streaming/movies_watched/";
          DEFINE FILENAME recommended_to="/home/tigergraph/mydata/streaming/movies_recommended/";
          DEFINE FILENAME belongs_to="/home/tigergraph/mydata/streaming/users_clusters/";

          LOAD user_data TO VERTEX user_account
          VALUES (
            $"id",
            $"user_name",
            $"member_since",
            $"last_login")
          USING header="true", separator=",";

          LOAD movie_data TO VERTEX movie
          VALUES (
            $"id",
            $"title_type",
            $"primary_title",
            $"original_title",
            $"is_adult",
            $"release_year",
            $"runtime_minutes",
            SPLIT($"genres","|"))
          USING header="true", separator=",";

          LOAD clusters_data TO VERTEX cluster
          VALUES (
            $"id",
            $"cluster")
          USING header="true", separator=",";

          LOAD watched TO EDGE watched
          VALUES (
            $"user_id",
            $"movie_id")
          USING header="true", separator=",";

          LOAD recommended_to TO EDGE recommended_to
          VALUES (
            $"movie_id",
            $"user_id")
          USING header="true",separator=",";

          LOAD belongs_to TO EDGE belongs_to
          VALUES (
            $"user_id",
            $"cluster_id")
          USING header="true", separator=",";
        }
        END
      #+end_src
***** Run loading job
      #+begin_src sql
        use graph streaming

        run loading job load_streaming
      #+end_src
**** TigerGraph Associate Certification
     +
**** Knowledge test
     + 95%


* Home automation
** Home Assistant
   + Ref: [[https://www.home-assistant.io/installation/linux][Linux - Home Assistant]]
   + Docker install
     #+begin_src bash
       docker run --init -d --restart=always \
              --name="homeassistant" \
              -e "TZ=America/Denver" \
              -v /volume1/homes/gregj/homeassistant/gregj-starter:/config \
              --network=host \
              homeassistant/home-assistant:stable

       # --device=/dev/ttyACM0 \
     #+end_src
