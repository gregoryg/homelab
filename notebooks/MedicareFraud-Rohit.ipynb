{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy as sc\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas_profiling as profile   # To check data distributions and correlations\n",
    "import warnings     # for supressing a warning when importing large files\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import tensorflow as tf\n",
    "from pylab import rcParams\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras import regularizers\n",
    "%matplotlib inline\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.5)\n",
    "rcParams['figure.figsize'] = 14, 8\n",
    "RANDOM_SEED = 42\n",
    "\n",
    "LABELS = [\"Normal\", \"Fraud\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Train Dataset\n",
    "datadir = \"/data/data-files/graph-data/medicare-fraud/\"\n",
    "\n",
    "Train=pd.read_csv(datadir + \"/Train-1542865627584.csv\")\n",
    "Train_Beneficiarydata=pd.read_csv(datadir + \"/Train_Beneficiarydata-1542865627584.csv\")\n",
    "Train_Inpatientdata=pd.read_csv(datadir + \"/Train_Inpatientdata-1542865627584.csv\")\n",
    "Train_Outpatientdata=pd.read_csv(datadir + \"/Train_Outpatientdata-1542865627584.csv\")\n",
    "\n",
    "# Load Test Dataset\n",
    "\n",
    "# Test=pd.read_csv(datadir + \"/data/data-files/graph-data/medicare-fraud/\n",
    "Test_Beneficiarydata=pd.read_csv(datadir + \"/Test_Beneficiarydata-1542969243754.csv\")\n",
    "Test_Inpatientdata=pd.read_csv(datadir + \"/Test_Inpatientdata-1542969243754.csv\")\n",
    "Test_Outpatientdata=pd.read_csv(datadir + \"/Test_Outpatientdata-1542969243754.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Train data : (5410, 2)\n",
      "Shape of Train_Beneficiarydata data : (138556, 25)\n",
      "Shape of Train_Inpatientdata data : (40474, 30)\n",
      "Shape of Train_Outpatientdata data : (517737, 27)\n",
      "Shape of Test_Beneficiarydata data : (63968, 25)\n",
      "Shape of Test_Inpatientdata data : (9551, 30)\n",
      "Shape of Test_Outpatientdata data : (125841, 27)\n"
     ]
    }
   ],
   "source": [
    "## Lets Check Shape of datasets \n",
    "\n",
    "print('Shape of Train data :',Train.shape)\n",
    "print('Shape of Train_Beneficiarydata data :',Train_Beneficiarydata.shape)\n",
    "print('Shape of Train_Inpatientdata data :',Train_Inpatientdata.shape)\n",
    "print('Shape of Train_Outpatientdata data :',Train_Outpatientdata.shape)\n",
    "\n",
    "# print('Shape of Test data :',Test.shape)\n",
    "print('Shape of Test_Beneficiarydata data :',Test_Beneficiarydata.shape)\n",
    "print('Shape of Test_Inpatientdata data :',Test_Inpatientdata.shape)\n",
    "print('Shape of Test_Outpatientdata data :',Test_Outpatientdata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Basic Data Understanding**\n",
    "\n",
    "Train and Test Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Shape: (5410, 2) \n",
      "\n",
      "Train Sample:\n",
      "    Provider PotentialFraud\n",
      "0  PRV51001             No\n",
      "1  PRV51003            Yes \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Lets check shape of Train and Test Data\n",
    "\n",
    "print('Train Shape:',Train.shape,'\\n')\n",
    "print('Train Sample:\\n',Train.head(2),'\\n')\n",
    "\n",
    "# print('\\n Test Shape:',Test.shape,'\\n')\n",
    "# print('Test Sample: \\n',Test.head(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRV51001    1\n",
      "PRV55516    1\n",
      "Name: Provider, dtype: int64\n",
      "\n",
      " Total missing values in Train : 0\n"
     ]
    }
   ],
   "source": [
    "## Lets check whether  providers details are unique or not in train data\n",
    "print(Train.Provider.value_counts(sort=True,ascending=False).head(2))  # number of unique providers in train data.Check for duplicates\n",
    "\n",
    "print('\\n Total missing values in Train :',Train.isna().sum().sum())\n",
    "\n",
    "# print('\\n Total missing values in Test :',Test.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Beneficiary Data Understanding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BeneID                                 0\n",
       "DOB                                    0\n",
       "DOD                                63394\n",
       "Gender                                 0\n",
       "Race                                   0\n",
       "RenalDiseaseIndicator                  0\n",
       "State                                  0\n",
       "County                                 0\n",
       "NoOfMonths_PartACov                    0\n",
       "NoOfMonths_PartBCov                    0\n",
       "ChronicCond_Alzheimer                  0\n",
       "ChronicCond_Heartfailure               0\n",
       "ChronicCond_KidneyDisease              0\n",
       "ChronicCond_Cancer                     0\n",
       "ChronicCond_ObstrPulmonary             0\n",
       "ChronicCond_Depression                 0\n",
       "ChronicCond_Diabetes                   0\n",
       "ChronicCond_IschemicHeart              0\n",
       "ChronicCond_Osteoporasis               0\n",
       "ChronicCond_rheumatoidarthritis        0\n",
       "ChronicCond_stroke                     0\n",
       "IPAnnualReimbursementAmt               0\n",
       "IPAnnualDeductibleAmt                  0\n",
       "OPAnnualReimbursementAmt               0\n",
       "OPAnnualDeductibleAmt                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets Check missing values in each column in beneficiary data :\n",
    "\n",
    "Train_Beneficiarydata.isna().sum()\n",
    "Test_Beneficiarydata.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BeneID                             object\n",
       "DOB                                object\n",
       "DOD                                object\n",
       "Gender                              int64\n",
       "Race                                int64\n",
       "RenalDiseaseIndicator              object\n",
       "State                               int64\n",
       "County                              int64\n",
       "NoOfMonths_PartACov                 int64\n",
       "NoOfMonths_PartBCov                 int64\n",
       "ChronicCond_Alzheimer               int64\n",
       "ChronicCond_Heartfailure            int64\n",
       "ChronicCond_KidneyDisease           int64\n",
       "ChronicCond_Cancer                  int64\n",
       "ChronicCond_ObstrPulmonary          int64\n",
       "ChronicCond_Depression              int64\n",
       "ChronicCond_Diabetes                int64\n",
       "ChronicCond_IschemicHeart           int64\n",
       "ChronicCond_Osteoporasis            int64\n",
       "ChronicCond_rheumatoidarthritis     int64\n",
       "ChronicCond_stroke                  int64\n",
       "IPAnnualReimbursementAmt            int64\n",
       "IPAnnualDeductibleAmt               int64\n",
       "OPAnnualReimbursementAmt            int64\n",
       "OPAnnualDeductibleAmt               int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets check data types of each column in beneficiary data\n",
    "\n",
    "Train_Beneficiarydata.dtypes\n",
    "Test_Beneficiarydata.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Replacing 2 with 0 for chronic conditions ,that means chroniv condition No is 0 and yes is 1\n",
    "\n",
    "Train_Beneficiarydata = Train_Beneficiarydata.replace({'ChronicCond_Alzheimer': 2, 'ChronicCond_Heartfailure': 2, 'ChronicCond_KidneyDisease': 2,\n",
    "                           'ChronicCond_Cancer': 2, 'ChronicCond_ObstrPulmonary': 2, 'ChronicCond_Depression': 2, \n",
    "                           'ChronicCond_Diabetes': 2, 'ChronicCond_IschemicHeart': 2, 'ChronicCond_Osteoporasis': 2, \n",
    "                           'ChronicCond_rheumatoidarthritis': 2, 'ChronicCond_stroke': 2 }, 0)\n",
    "\n",
    "Train_Beneficiarydata = Train_Beneficiarydata.replace({'RenalDiseaseIndicator': 'Y'}, 1)\n",
    "\n",
    "Test_Beneficiarydata = Test_Beneficiarydata.replace({'ChronicCond_Alzheimer': 2, 'ChronicCond_Heartfailure': 2, 'ChronicCond_KidneyDisease': 2,\n",
    "                           'ChronicCond_Cancer': 2, 'ChronicCond_ObstrPulmonary': 2, 'ChronicCond_Depression': 2, \n",
    "                           'ChronicCond_Diabetes': 2, 'ChronicCond_IschemicHeart': 2, 'ChronicCond_Osteoporasis': 2, \n",
    "                           'ChronicCond_rheumatoidarthritis': 2, 'ChronicCond_stroke': 2 }, 0)\n",
    "\n",
    "Test_Beneficiarydata = Test_Beneficiarydata.replace({'RenalDiseaseIndicator': 'Y'}, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BeneID</th>\n",
       "      <th>DOB</th>\n",
       "      <th>DOD</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "      <th>RenalDiseaseIndicator</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>NoOfMonths_PartACov</th>\n",
       "      <th>NoOfMonths_PartBCov</th>\n",
       "      <th>...</th>\n",
       "      <th>ChronicCond_Depression</th>\n",
       "      <th>ChronicCond_Diabetes</th>\n",
       "      <th>ChronicCond_IschemicHeart</th>\n",
       "      <th>ChronicCond_Osteoporasis</th>\n",
       "      <th>ChronicCond_rheumatoidarthritis</th>\n",
       "      <th>ChronicCond_stroke</th>\n",
       "      <th>IPAnnualReimbursementAmt</th>\n",
       "      <th>IPAnnualDeductibleAmt</th>\n",
       "      <th>OPAnnualReimbursementAmt</th>\n",
       "      <th>OPAnnualDeductibleAmt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BENE11001</td>\n",
       "      <td>1943-01-01</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>230</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36000</td>\n",
       "      <td>3204</td>\n",
       "      <td>60</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      BeneID         DOB  DOD  Gender  Race RenalDiseaseIndicator  State  \\\n",
       "0  BENE11001  1943-01-01  NaN       1     1                     0     39   \n",
       "\n",
       "   County  NoOfMonths_PartACov  NoOfMonths_PartBCov  ...  \\\n",
       "0     230                   12                   12  ...   \n",
       "\n",
       "   ChronicCond_Depression  ChronicCond_Diabetes  ChronicCond_IschemicHeart  \\\n",
       "0                       1                     1                          1   \n",
       "\n",
       "   ChronicCond_Osteoporasis  ChronicCond_rheumatoidarthritis  \\\n",
       "0                         0                                1   \n",
       "\n",
       "   ChronicCond_stroke  IPAnnualReimbursementAmt  IPAnnualDeductibleAmt  \\\n",
       "0                   1                     36000                   3204   \n",
       "\n",
       "   OPAnnualReimbursementAmt  OPAnnualDeductibleAmt  \n",
       "0                        60                     70  \n",
       "\n",
       "[1 rows x 25 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_Beneficiarydata.head(1)\n",
    "Test_Beneficiarydata.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add Age of Person based on his/her DOD(Date of death ) and DOB (Date of Birth)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets Create Age column to the dataset\n",
    "\n",
    "Train_Beneficiarydata['DOB'] = pd.to_datetime(Train_Beneficiarydata['DOB'] , format = '%Y-%m-%d')\n",
    "Train_Beneficiarydata['DOD'] = pd.to_datetime(Train_Beneficiarydata['DOD'],format = '%Y-%m-%d',errors='ignore')\n",
    "Train_Beneficiarydata['Age'] = round(((Train_Beneficiarydata['DOD'] - Train_Beneficiarydata['DOB']).dt.days)/365)\n",
    "\n",
    "\n",
    "Test_Beneficiarydata['DOB'] = pd.to_datetime(Test_Beneficiarydata['DOB'] , format = '%Y-%m-%d')\n",
    "Test_Beneficiarydata['DOD'] = pd.to_datetime(Test_Beneficiarydata['DOD'],format = '%Y-%m-%d',errors='ignore')\n",
    "Test_Beneficiarydata['Age'] = round(((Test_Beneficiarydata['DOD'] - Test_Beneficiarydata['DOB']).dt.days)/365)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## As we see that last DOD value is 2009-12-01 ,which means Beneficiary Details data is of year 2009.\n",
    "## so we will calculate age of other benficiaries for year 2009.\n",
    "\n",
    "Train_Beneficiarydata.Age.fillna(round(((pd.to_datetime('2009-12-01' , format = '%Y-%m-%d') - Train_Beneficiarydata['DOB']).dt.days)/365),\n",
    "                                 inplace=True)\n",
    "\n",
    "\n",
    "Test_Beneficiarydata.Age.fillna(round(((pd.to_datetime('2009-12-01' , format = '%Y-%m-%d') - Test_Beneficiarydata['DOB']).dt.days)/365),\n",
    "                                 inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BeneID</th>\n",
       "      <th>DOB</th>\n",
       "      <th>DOD</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Race</th>\n",
       "      <th>RenalDiseaseIndicator</th>\n",
       "      <th>State</th>\n",
       "      <th>County</th>\n",
       "      <th>NoOfMonths_PartACov</th>\n",
       "      <th>NoOfMonths_PartBCov</th>\n",
       "      <th>...</th>\n",
       "      <th>ChronicCond_Diabetes</th>\n",
       "      <th>ChronicCond_IschemicHeart</th>\n",
       "      <th>ChronicCond_Osteoporasis</th>\n",
       "      <th>ChronicCond_rheumatoidarthritis</th>\n",
       "      <th>ChronicCond_stroke</th>\n",
       "      <th>IPAnnualReimbursementAmt</th>\n",
       "      <th>IPAnnualDeductibleAmt</th>\n",
       "      <th>OPAnnualReimbursementAmt</th>\n",
       "      <th>OPAnnualDeductibleAmt</th>\n",
       "      <th>Age</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BENE11001</td>\n",
       "      <td>1943-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>230</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>36000</td>\n",
       "      <td>3204</td>\n",
       "      <td>60</td>\n",
       "      <td>70</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BENE11007</td>\n",
       "      <td>1940-09-01</td>\n",
       "      <td>2009-12-01</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>45</td>\n",
       "      <td>610</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1490</td>\n",
       "      <td>160</td>\n",
       "      <td>69.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      BeneID        DOB        DOD  Gender  Race RenalDiseaseIndicator  State  \\\n",
       "0  BENE11001 1943-01-01        NaT       1     1                     0     39   \n",
       "1  BENE11007 1940-09-01 2009-12-01       1     2                     0     45   \n",
       "\n",
       "   County  NoOfMonths_PartACov  NoOfMonths_PartBCov  ...  \\\n",
       "0     230                   12                   12  ...   \n",
       "1     610                   12                   12  ...   \n",
       "\n",
       "   ChronicCond_Diabetes  ChronicCond_IschemicHeart  ChronicCond_Osteoporasis  \\\n",
       "0                     1                          1                         0   \n",
       "1                     1                          0                         1   \n",
       "\n",
       "   ChronicCond_rheumatoidarthritis  ChronicCond_stroke  \\\n",
       "0                                1                   1   \n",
       "1                                1                   0   \n",
       "\n",
       "   IPAnnualReimbursementAmt  IPAnnualDeductibleAmt  OPAnnualReimbursementAmt  \\\n",
       "0                     36000                   3204                        60   \n",
       "1                         0                      0                      1490   \n",
       "\n",
       "   OPAnnualDeductibleAmt   Age  \n",
       "0                     70  67.0  \n",
       "1                    160  69.0  \n",
       "\n",
       "[2 rows x 26 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_Beneficiarydata.head(2)\n",
    "Test_Beneficiarydata.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add Flag column 'WhetherDead' using DOD values to tell whether beneficiary is dead on not**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0.0\n",
       "1    1.0\n",
       "2    0.0\n",
       "Name: WhetherDead, dtype: float64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets create a new variable 'WhetherDead' with flag 1 means Dead and 0 means not Dead\n",
    "\n",
    "Train_Beneficiarydata.loc[Train_Beneficiarydata.DOD.isna(),'WhetherDead']=0\n",
    "Train_Beneficiarydata.loc[Train_Beneficiarydata.DOD.notna(),'WhetherDead']=1\n",
    "Train_Beneficiarydata.loc[:,'WhetherDead'].head(7)\n",
    "\n",
    "\n",
    "Test_Beneficiarydata.loc[Test_Beneficiarydata.DOD.isna(),'WhetherDead']=0\n",
    "Test_Beneficiarydata.loc[Test_Beneficiarydata.DOD.notna(),'WhetherDead']=1\n",
    "Test_Beneficiarydata.loc[:,'WhetherDead'].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BeneID                       0\n",
       "ClaimID                      0\n",
       "ClaimStartDt                 0\n",
       "ClaimEndDt                   0\n",
       "Provider                     0\n",
       "InscClaimAmtReimbursed       0\n",
       "AttendingPhysician          31\n",
       "OperatingPhysician        3962\n",
       "OtherPhysician            8538\n",
       "AdmissionDt                  0\n",
       "ClmAdmitDiagnosisCode        0\n",
       "DeductibleAmtPaid          196\n",
       "DischargeDt                  0\n",
       "DiagnosisGroupCode           0\n",
       "ClmDiagnosisCode_1           0\n",
       "ClmDiagnosisCode_2          54\n",
       "ClmDiagnosisCode_3         169\n",
       "ClmDiagnosisCode_4         404\n",
       "ClmDiagnosisCode_5         719\n",
       "ClmDiagnosisCode_6        1197\n",
       "ClmDiagnosisCode_7        1736\n",
       "ClmDiagnosisCode_8        2360\n",
       "ClmDiagnosisCode_9        3238\n",
       "ClmDiagnosisCode_10       8664\n",
       "ClmProcedureCode_1        4118\n",
       "ClmProcedureCode_2        8297\n",
       "ClmProcedureCode_3        9328\n",
       "ClmProcedureCode_4        9522\n",
       "ClmProcedureCode_5        9549\n",
       "ClmProcedureCode_6        9551\n",
       "dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets check missing values in each column in inpatient data\n",
    "Train_Inpatientdata.isna().sum()\n",
    "\n",
    "Test_Inpatientdata.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create new column 'AdmitForDays' indicating number of days patient was admitted in hospita**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## As patient can be admitted for only for 1 day,we will add 1 to the difference of Discharge Date and Admission Date \n",
    "\n",
    "Train_Inpatientdata['AdmissionDt'] = pd.to_datetime(Train_Inpatientdata['AdmissionDt'] , format = '%Y-%m-%d')\n",
    "Train_Inpatientdata['DischargeDt'] = pd.to_datetime(Train_Inpatientdata['DischargeDt'],format = '%Y-%m-%d')\n",
    "Train_Inpatientdata['AdmitForDays'] = ((Train_Inpatientdata['DischargeDt'] - Train_Inpatientdata['AdmissionDt']).dt.days)+1\n",
    "\n",
    "\n",
    "Test_Inpatientdata['AdmissionDt'] = pd.to_datetime(Test_Inpatientdata['AdmissionDt'] , format = '%Y-%m-%d')\n",
    "Test_Inpatientdata['DischargeDt'] = pd.to_datetime(Test_Inpatientdata['DischargeDt'],format = '%Y-%m-%d')\n",
    "Test_Inpatientdata['AdmitForDays'] = ((Test_Inpatientdata['DischargeDt'] - Test_Inpatientdata['AdmissionDt']).dt.days)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets check Min and Max values of AdmitforDays column in Train and Test.\n",
    "print('Min AdmitForDays Train:- ',Train_Inpatientdata.AdmitForDays.min())\n",
    "print('Max AdmitForDays Train:- ',Train_Inpatientdata.AdmitForDays.max())\n",
    "Train_Inpatientdata.AdmitForDays.isnull().sum()   #Check Null values.\n",
    "\n",
    "print('Min AdmitForDays Test:- ',Test_Inpatientdata.AdmitForDays.min())\n",
    "print('Max AdmitForDays Test:- ',Test_Inpatientdata.AdmitForDays.max())\n",
    "Test_Inpatientdata.AdmitForDays.isnull().sum()   #Check Null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Outpatient Data understanding**\n",
    "\n",
    "Lets Check null values in each columnn of outpatient data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Outpatientdata.isna().sum()\n",
    "\n",
    "Test_Outpatientdata.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets Check Shape of datasets after adding new variables\n",
    "\n",
    "print('Shape of Train data :',Train.shape)\n",
    "print('Shape of Train_Beneficiarydata data :',Train_Beneficiarydata.shape)\n",
    "print('Shape of Train_Inpatientdata data :',Train_Inpatientdata.shape)\n",
    "print('Shape of Train_Outpatientdata data :',Train_Outpatientdata.shape)\n",
    "\n",
    "print('Shape of Test data :',Test.shape)\n",
    "print('Shape of Test_Beneficiarydata data :',Test_Beneficiarydata.shape)\n",
    "print('Shape of Test_Inpatientdata data :',Test_Inpatientdata.shape)\n",
    "print('Shape of Test_Outpatientdata data :',Test_Outpatientdata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Merging**\n",
    "## As we see columns in inpatient and outpatient data are similar, we will merge this data based on these similar keys using outer join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Key_Column_To_Merge_Outpatient=Train_Outpatientdata.columns\n",
    "print(Key_Column_To_Merge_Outpatient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge Inpatient and Outpatinet data and create dataset for all patients.\n",
    "**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets make union of Inpatienta and outpatient data .\n",
    "# We will use all keys in outpatient data as we want to make union and dont want duplicate columns from both tables.\n",
    "\n",
    "Train_Allpatientdata=pd.merge(Train_Outpatientdata,Train_Inpatientdata,\n",
    "                              left_on=['BeneID', 'ClaimID', 'ClaimStartDt', 'ClaimEndDt', 'Provider',\n",
    "       'InscClaimAmtReimbursed', 'AttendingPhysician', 'OperatingPhysician',\n",
    "       'OtherPhysician', 'ClmDiagnosisCode_1', 'ClmDiagnosisCode_2',\n",
    "       'ClmDiagnosisCode_3', 'ClmDiagnosisCode_4', 'ClmDiagnosisCode_5',\n",
    "       'ClmDiagnosisCode_6', 'ClmDiagnosisCode_7', 'ClmDiagnosisCode_8',\n",
    "       'ClmDiagnosisCode_9', 'ClmDiagnosisCode_10', 'ClmProcedureCode_1',\n",
    "       'ClmProcedureCode_2', 'ClmProcedureCode_3', 'ClmProcedureCode_4',\n",
    "       'ClmProcedureCode_5', 'ClmProcedureCode_6', 'DeductibleAmtPaid',\n",
    "       'ClmAdmitDiagnosisCode'],\n",
    "                              right_on=['BeneID', 'ClaimID', 'ClaimStartDt', 'ClaimEndDt', 'Provider',\n",
    "       'InscClaimAmtReimbursed', 'AttendingPhysician', 'OperatingPhysician',\n",
    "       'OtherPhysician', 'ClmDiagnosisCode_1', 'ClmDiagnosisCode_2',\n",
    "       'ClmDiagnosisCode_3', 'ClmDiagnosisCode_4', 'ClmDiagnosisCode_5',\n",
    "       'ClmDiagnosisCode_6', 'ClmDiagnosisCode_7', 'ClmDiagnosisCode_8',\n",
    "       'ClmDiagnosisCode_9', 'ClmDiagnosisCode_10', 'ClmProcedureCode_1',\n",
    "       'ClmProcedureCode_2', 'ClmProcedureCode_3', 'ClmProcedureCode_4',\n",
    "       'ClmProcedureCode_5', 'ClmProcedureCode_6', 'DeductibleAmtPaid',\n",
    "       'ClmAdmitDiagnosisCode']\n",
    "                              ,how='outer')\n",
    "\n",
    "\n",
    "Test_Allpatientdata=pd.merge(Test_Outpatientdata,Test_Inpatientdata,\n",
    "                              left_on=['BeneID', 'ClaimID', 'ClaimStartDt', 'ClaimEndDt', 'Provider',\n",
    "       'InscClaimAmtReimbursed', 'AttendingPhysician', 'OperatingPhysician',\n",
    "       'OtherPhysician', 'ClmDiagnosisCode_1', 'ClmDiagnosisCode_2',\n",
    "       'ClmDiagnosisCode_3', 'ClmDiagnosisCode_4', 'ClmDiagnosisCode_5',\n",
    "       'ClmDiagnosisCode_6', 'ClmDiagnosisCode_7', 'ClmDiagnosisCode_8',\n",
    "       'ClmDiagnosisCode_9', 'ClmDiagnosisCode_10', 'ClmProcedureCode_1',\n",
    "       'ClmProcedureCode_2', 'ClmProcedureCode_3', 'ClmProcedureCode_4',\n",
    "       'ClmProcedureCode_5', 'ClmProcedureCode_6', 'DeductibleAmtPaid',\n",
    "       'ClmAdmitDiagnosisCode'],\n",
    "                              right_on=['BeneID', 'ClaimID', 'ClaimStartDt', 'ClaimEndDt', 'Provider',\n",
    "       'InscClaimAmtReimbursed', 'AttendingPhysician', 'OperatingPhysician',\n",
    "       'OtherPhysician', 'ClmDiagnosisCode_1', 'ClmDiagnosisCode_2',\n",
    "       'ClmDiagnosisCode_3', 'ClmDiagnosisCode_4', 'ClmDiagnosisCode_5',\n",
    "       'ClmDiagnosisCode_6', 'ClmDiagnosisCode_7', 'ClmDiagnosisCode_8',\n",
    "       'ClmDiagnosisCode_9', 'ClmDiagnosisCode_10', 'ClmProcedureCode_1',\n",
    "       'ClmProcedureCode_2', 'ClmProcedureCode_3', 'ClmProcedureCode_4',\n",
    "       'ClmProcedureCode_5', 'ClmProcedureCode_6', 'DeductibleAmtPaid',\n",
    "       'ClmAdmitDiagnosisCode']\n",
    "                              ,how='outer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train_Allpatientdata',Train_Allpatientdata.shape)\n",
    "print('Test_Allpatientdata',Test_Allpatientdata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_Allpatientdata.head(4)\n",
    "Test_Allpatientdata.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge Beneficiary details to All Patients data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets merge All patient data with beneficiary details data based on 'BeneID' as joining key for inner join\n",
    "Train_AllPatientDetailsdata=pd.merge(Train_Allpatientdata,Train_Beneficiarydata,left_on='BeneID',right_on='BeneID',how='inner')\n",
    "\n",
    "Test_AllPatientDetailsdata=pd.merge(Test_Allpatientdata,Test_Beneficiarydata,left_on='BeneID',right_on='BeneID',how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets Print shape of data \n",
    "print('Shape of All Patient Details Train : ',Train_AllPatientDetailsdata.shape)\n",
    "Train_AllPatientDetailsdata.head()\n",
    "\n",
    "print('Shape of All Patient Details Test : ',Test_AllPatientDetailsdata.shape)\n",
    "Test_AllPatientDetailsdata.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Merge PotentialFraud details for each provider to create ProviderWithPatientDetails data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets merge patient data with fradulent providers details data with \"Provider\" as joining key for inner join\n",
    "\n",
    "Train_ProviderWithPatientDetailsdata=pd.merge(Train,Train_AllPatientDetailsdata,on='Provider')\n",
    "\n",
    "Test_ProviderWithPatientDetailsdata=pd.merge(Test,Test_AllPatientDetailsdata,on='Provider')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets Print the shape of dataset \n",
    "print(\"Shape of Train Provider with Patient Details data :-\",Train_ProviderWithPatientDetailsdata.shape)\n",
    "Train_ProviderWithPatientDetailsdata.head()\n",
    "\n",
    "print(\"Shape of Test Provider with Patient Details data :-\",Test_ProviderWithPatientDetailsdata.shape)\n",
    "Test_ProviderWithPatientDetailsdata.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lets check missing value percentage in full datasets\n",
    "\n",
    "#print('Percent missing values in Train Provider with patient details data :\\n')\n",
    "Train_ProviderWithPatientDetailsdata.isnull().sum()*100/len(Train_ProviderWithPatientDetailsdata)\n",
    "\n",
    "\n",
    "print('Percent missing values in Test Provider with patient details data :\\n')\n",
    "Test_ProviderWithPatientDetailsdata.isnull().sum()*100/len(Test_ProviderWithPatientDetailsdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets Check dtypes of both the datasets\n",
    "Train_ProviderWithPatientDetailsdata.dtypes\n",
    "Test_ProviderWithPatientDetailsdata.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exploratory Data Analysis**\n",
    "\n",
    "**Plot Potential fraud class proportion in both Train and Merged data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLotting the frequencies of fraud and non-fraud Merged transactions in the data\n",
    "\n",
    "sns.set_style('white',rc={'figure.figsize':(12,8)})\n",
    "count_classes = pd.value_counts(Train_ProviderWithPatientDetailsdata['PotentialFraud'], sort = True)\n",
    "print(\"Percent Distribution of Potential Fraud class:- \\n\",count_classes*100/len(Train_ProviderWithPatientDetailsdata))\n",
    "LABELS = [\"Non Fraud\", \"Fraud\"]\n",
    "#Drawing a barplot\n",
    "count_classes.plot(kind = 'bar', rot=0,figsize=(10,6))\n",
    "\n",
    "#Giving titles and labels to the plot\n",
    "plt.title(\"Potential Fraud distribution in Aggregated claim transactional data\")\n",
    "plt.xticks(range(2), LABELS)\n",
    "plt.xlabel(\"Potential Fraud Class \")\n",
    "plt.ylabel(\"Number of PotentialFraud per Class \")\n",
    "\n",
    "plt.savefig('PotentialFraudDistributionInMergedData')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLotting the frequencies of fraud and non-fraud transactions in the train data\n",
    "\n",
    "count_classes_provider = pd.value_counts(Train['PotentialFraud'], sort = True)\n",
    "print(\"Percent Distribution of Potential Fraud class:- \\n\",count_classes_provider*100/len(Train))\n",
    "LABELS = [\"Non Fraud\", \"Fraud\"]\n",
    "#Drawing a barplot\n",
    "count_classes_provider.plot(kind = 'bar', rot=0,figsize=(10,6))\n",
    "\n",
    "#Giving titles and labels to the plot\n",
    "plt.title(\"Potential Fraud distribution in individual Providers data\")\n",
    "plt.xticks(range(2), LABELS)\n",
    "plt.xlabel(\"Potential Fraud Class \")\n",
    "plt.ylabel(\"Number of PotentialFraud per Class \")\n",
    "\n",
    "\n",
    "plt.savefig('PotentialFraudDistributionImbalance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the above 2 graphs ,we can say that the proportion of fradulent claim transactions are more compared to non fraud providers.So we must get insights from number of claim transactions and amounts involved per - Beneficiary | Beneficiary + Physician |Physician | Diagnosis | Procedure etc...**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Providers NonFraud|Fraud Class Percent Distribution in Whole dataset :\\n',count_classes/len(Train_ProviderWithPatientDetailsdata)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Providers NonFraud|Fraud Class Percent Distribution :\\n',count_classes_provider/len(Train)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**State - wise Percent Beneficiary Distribution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLotting the frequencies of Statewise beneficiaries\n",
    "count_States = pd.value_counts(Train_Beneficiarydata['State'], sort = True)\n",
    "#print(\"Percent Distribution of Beneficieries per state:- \\n\",count_States*100/len(Train_Beneficiarydata))\n",
    "\n",
    "#Drawing a barplot\n",
    "(count_States*100/len(Train_Beneficiarydata)).plot(kind = 'bar', rot=0,figsize=(16,8),fontsize=12,legend=True)\n",
    "\n",
    "#Giving titles and labels to the plot\n",
    "\n",
    "plt.annotate('Maximum Beneficiaries are from this State', xy=(0.01,8), xytext=(8, 6.5),\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05))\n",
    "\n",
    "plt.yticks(np.arange(0,10,2), ('0 %','2 %', '4 %', '6 %', '8 %', '10%'))\n",
    "plt.title(\"State - wise Beneficiary Distribution\",fontsize=18)\n",
    "plt.xlabel(\"State Number\",fontsize=15)\n",
    "plt.ylabel(\"Percentage of Beneficiaries \"'%',fontsize=15)\n",
    "plt.show()\n",
    "\n",
    "plt.savefig('StateWiseBeneficiaryDistribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Above Plot shows top states with their beneficiary percentage distribution.States 5,10,45 are top states in terms of beneficiary percentage**\n",
    "\n",
    "****Race-wise Percent Distribution of Beneficiaries****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PLotting the frequencies of race-wise beneficiaries\n",
    "count_Race = pd.value_counts(Train_Beneficiarydata['Race'], sort = True)\n",
    "\n",
    "#Drawing a barplot\n",
    "(count_Race*100/len(Train_Beneficiarydata)).plot(kind = 'bar', rot=0,figsize=(10,6),fontsize=12)\n",
    "\n",
    "#Giving titles and labels to the plot\n",
    "plt.yticks(np.arange(0,100,20))#, ('0 %','20 %', '40 %', '60 %', '80 %', '100%'))\n",
    "plt.title(\"Race - wise Beneficiary Distribution\",fontsize=18)\n",
    "plt.xlabel(\"Race Code\",fontsize=15)\n",
    "plt.ylabel(\"Percentage of Beneficiaries \"'%',fontsize=15)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "plt.savefig('RacewiseBeneficiaryDistribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It seems that more than 80% beneficiaries are of same race which is Race 1 which means Maximum population in the dataset originated from same race.There is no race 4 in the dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Top-10 Procedures invloved in Healthcare Fraud****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets plot countplot for each fraud non fraud categories\n",
    "\n",
    "sns.set(rc={'figure.figsize':(12,8)},style='white')\n",
    "\n",
    "ax=sns.countplot(x='ClmProcedureCode_1',hue='PotentialFraud',data=Train_ProviderWithPatientDetailsdata\n",
    "              ,order=Train_ProviderWithPatientDetailsdata.ClmProcedureCode_1.value_counts().iloc[:10].index)\n",
    "\n",
    "plt.title('Top-10 Procedures invloved in Healthcare Fraud')\n",
    "    \n",
    "plt.show()\n",
    "\n",
    "plt.savefig('TopProceduresinvlovedinHealthcareFraud')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the above plot,we can say that Procedure 9904,8154,66 are top procedures (in terms of money involved).Distribution of fraud and non fraud count shows suspicious transactions involved in them.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Top-10 Diagnosis invloved in Healthcare Fraud****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## lets plot Top-10 Claim Diagnosis  invloved in Healthcare Fraud\n",
    "\n",
    "sns.set(rc={'figure.figsize':(12,8)},style='white')\n",
    "\n",
    "sns.countplot(x='ClmDiagnosisCode_1',hue='PotentialFraud',data=Train_ProviderWithPatientDetailsdata\n",
    "              ,order=Train_ProviderWithPatientDetailsdata.ClmDiagnosisCode_1.value_counts().iloc[:10].index)\n",
    "\n",
    "plt.title('Top-10 Diagnosis invloved in Healthcare Fraud')\n",
    "plt.show()\n",
    "\n",
    "plt.savefig('TopDiagnosisInnvlovedinHealthcareFraud')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the above plot,we can say that diagnosis 4019,4011,2724 are top diagnosis (in terms of money involved).Distribution of fraud and non fraud counts shows suspicious transactions involved in them.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Top-20 Attending Physicians invloved in Healthcare Fraud****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### lets plot Top-20 Attending Physicians invloved in Healthcare Fraud \n",
    "\n",
    "sns.set(rc={'figure.figsize':(12,8)},style='white')\n",
    "\n",
    "ax= sns.countplot(x='AttendingPhysician',hue='PotentialFraud',data=Train_ProviderWithPatientDetailsdata\n",
    "              ,order=Train_ProviderWithPatientDetailsdata.AttendingPhysician.value_counts().iloc[:20].index)\n",
    "\n",
    "    \n",
    "plt.title('Top-20 Attending physicians invloved in Healthcare Fraud')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n",
    "\n",
    "plt.savefig('TopAttendingphysiciansinvlovedinHealthcareFraud')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the above plot,we see count of involvement of attending physicians.And flags the nature of provider where they are working is whether fraud or non fraud.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IPAnnualDeductibleAmt Vs IPAnnualReimbursementAmt Fraud and non Fraud Categories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets Plot IPAnnualDeductibleAmt and IPAnnualReimbursementAmt in both fraud and non Fraud Categoories\n",
    "\n",
    "sns.set(rc={'figure.figsize':(12,8)},style='white')\n",
    "\n",
    "sns.lmplot(x='IPAnnualDeductibleAmt',y='IPAnnualReimbursementAmt',hue='PotentialFraud',\n",
    "           col='PotentialFraud',fit_reg=False,data=Train_ProviderWithPatientDetailsdata)\n",
    "\n",
    "\n",
    "#plt.title('IPAnnualDeductibleAmt and IPAnnualReimbursementAmt in both fraud and non Fraud Categoories')\n",
    "\n",
    "plt.savefig('IPAnnualDeductibleAmtandIPAnnualReimbursementAmtinbothfraudandnonFraud')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As we see in the above graph,there is no visible difference in IpAnnualDeductibleAmt and IPAnnualReimbursementAmt.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****DeductibleAmtPaid Vs InsClaimAmtReimbursed in Fraud and non Fraud Categories****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets Plot DeductibleAmtPaid and InsClaimAmtReimbursed in both fraud and non Fraud Categoories\n",
    "\n",
    "sns.set(rc={'figure.figsize':(12,8)},style='white')\n",
    "\n",
    "sns.lmplot(x='DeductibleAmtPaid',y='InscClaimAmtReimbursed',hue='PotentialFraud',\n",
    "           col='PotentialFraud',fit_reg=False,data=Train_ProviderWithPatientDetailsdata)\n",
    "\n",
    "\n",
    "plt.savefig('DeductibleAmtPaidandInsClaimAmtReimbursed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We can not differentiate between fraud and non fraud cases based only on DeductibleAmtPaid and InscClaimAmtReimbursed.This lets us derive more features from datasets.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insurance Claim Amount Reimbursed Vs Age in Fraud and Non Fraud**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Let's See Insurance Claim Amount Reimbursed Vs Age\n",
    "sns.set(rc={'figure.figsize':(12,8)},style='white')\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\n",
    "f.suptitle('Insurance Claim Amount Reimbursed Vs Age')\n",
    "\n",
    "ax1.scatter(Train_ProviderWithPatientDetailsdata[Train_ProviderWithPatientDetailsdata.PotentialFraud=='Yes'].Age, \n",
    "            Train_ProviderWithPatientDetailsdata[Train_ProviderWithPatientDetailsdata.PotentialFraud=='Yes'].InscClaimAmtReimbursed)\n",
    "ax1.set_title('Fraud')\n",
    "ax1.axhline(y=60000,c='r')\n",
    "ax1.set_ylabel('Insurance Claim Amout Reimbursed')\n",
    "\n",
    "ax2.scatter(Train_ProviderWithPatientDetailsdata[Train_ProviderWithPatientDetailsdata.PotentialFraud=='No'].Age, \n",
    "            Train_ProviderWithPatientDetailsdata[Train_ProviderWithPatientDetailsdata.PotentialFraud=='No'].InscClaimAmtReimbursed)\n",
    "ax2.set_title('Normal')\n",
    "ax2.axhline(y=60000,c='r')\n",
    "ax2.set_xlabel('Age (in Years)')\n",
    "ax2.set_ylabel('Insurance Claim Amout Reimbursed')\n",
    "\n",
    "plt.show()\n",
    "f.savefig('AgeVsClaimAmtReimbursed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**From the above graph,we see that occurance of fraud cases is more frequent in lower age groups(30-70 years) compared to higher age groups(70+ years).Age is one of the important feature for differentiating between fraud abd non fraud behaviour.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Engineering\n",
    "Append Train to Test to derive more accurate features.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check the last record of Test_ProviderWithPatientDetailsdata\n",
    "\n",
    "Test_ProviderWithPatientDetailsdata.iloc[[135391]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lets check the last record of Train_ProviderWithPatientDetailsdata\n",
    "\n",
    "Train_ProviderWithPatientDetailsdata.iloc[[558210]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Appending Train data to Test data will help you get good average scores of new features in Test data,as we see not all levels of variables are present in test data compared to train data.So our approach here will be-to append train data to test data ,derive new average features and take only test data to evaluate results.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets create a copy of test data first and merge test with train to get better feature averages\n",
    "\n",
    "Test_ProviderWithPatientDetailsdata_copy=Test_ProviderWithPatientDetailsdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets Check shape of copy\n",
    "\n",
    "print('Shape of Test Copy :-',Test_ProviderWithPatientDetailsdata_copy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### We will concat two datasets using columns of Test data only as we dont see target column in Test data.\n",
    "\n",
    "col_merge=Test_ProviderWithPatientDetailsdata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets add both test and train datasets\n",
    "\n",
    "Test_ProviderWithPatientDetailsdata=pd.concat([Test_ProviderWithPatientDetailsdata,\n",
    "                                               Train_ProviderWithPatientDetailsdata[col_merge]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lets verify shape after concatenating\n",
    "\n",
    "print(\"Shape of Test After Concatenation\",Test_ProviderWithPatientDetailsdata.shape)\n",
    "\n",
    "print(\"Expected rows after addition\",135392+558211)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lets check record number 135392 as this will be record from train datasets.\n",
    "Test_ProviderWithPatientDetailsdata.iloc[[135392]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**As we verified that our first record is appended to test data correctly,we are all set to derive Average features grouped according to columns of datasets.\n",
    "\n",
    "Other than basic explorations and visualizations, we can use certain methods to identify clues of fraud and abuse. One such simple method is 'Grouping based on Similarity'. In this method, we basically group all the records by the ProcedureCodes, DiagnosisCodes,Provider.\n",
    "\n",
    "For example, if we have a dataset with Procedure codes only for X Procedure, we will then group and check average Amounts involved at each levels of Procedure and analyse the behaviour**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Average Features based on grouping variables.\n",
    "Average features grouped by Provider**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_ProviderWithPatientDetailsdata[\"PerProviderAvg_InscClaimAmtReimbursed\"]=Train_ProviderWithPatientDetailsdata.groupby('Provider')['InscClaimAmtReimbursed'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerProviderAvg_DeductibleAmtPaid\"]=Train_ProviderWithPatientDetailsdata.groupby('Provider')['DeductibleAmtPaid'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerProviderAvg_IPAnnualReimbursementAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('Provider')['IPAnnualReimbursementAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerProviderAvg_IPAnnualDeductibleAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('Provider')['IPAnnualDeductibleAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerProviderAvg_OPAnnualReimbursementAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('Provider')['OPAnnualReimbursementAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerProviderAvg_OPAnnualDeductibleAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('Provider')['OPAnnualDeductibleAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerProviderAvg_Age\"]=Train_ProviderWithPatientDetailsdata.groupby('Provider')['Age'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerProviderAvg_NoOfMonths_PartACov\"]=Train_ProviderWithPatientDetailsdata.groupby('Provider')['NoOfMonths_PartACov'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerProviderAvg_NoOfMonths_PartBCov\"]=Train_ProviderWithPatientDetailsdata.groupby('Provider')['NoOfMonths_PartBCov'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerProviderAvg_AdmitForDays\"]=Train_ProviderWithPatientDetailsdata.groupby('Provider')['AdmitForDays'].transform('mean')\n",
    "\n",
    "\n",
    "Test_ProviderWithPatientDetailsdata[\"PerProviderAvg_InscClaimAmtReimbursed\"]=Test_ProviderWithPatientDetailsdata.groupby('Provider')['InscClaimAmtReimbursed'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerProviderAvg_DeductibleAmtPaid\"]=Test_ProviderWithPatientDetailsdata.groupby('Provider')['DeductibleAmtPaid'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerProviderAvg_IPAnnualReimbursementAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('Provider')['IPAnnualReimbursementAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerProviderAvg_IPAnnualDeductibleAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('Provider')['IPAnnualDeductibleAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerProviderAvg_OPAnnualReimbursementAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('Provider')['OPAnnualReimbursementAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerProviderAvg_OPAnnualDeductibleAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('Provider')['OPAnnualDeductibleAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerProviderAvg_Age\"]=Test_ProviderWithPatientDetailsdata.groupby('Provider')['Age'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerProviderAvg_NoOfMonths_PartACov\"]=Test_ProviderWithPatientDetailsdata.groupby('Provider')['NoOfMonths_PartACov'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerProviderAvg_NoOfMonths_PartBCov\"]=Test_ProviderWithPatientDetailsdata.groupby('Provider')['NoOfMonths_PartBCov'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerProviderAvg_AdmitForDays\"]=Test_ProviderWithPatientDetailsdata.groupby('Provider')['AdmitForDays'].transform('mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets check shape\n",
    "\n",
    "print('Train',Train_ProviderWithPatientDetailsdata.shape)\n",
    "Train_ProviderWithPatientDetailsdata.iloc[:,-10:].head(2)\n",
    "\n",
    "print(\"Test \",Test_ProviderWithPatientDetailsdata.shape)\n",
    "Test_ProviderWithPatientDetailsdata.iloc[:,-10:].head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Average features grouped by BeneID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Grouping based on BeneID explains amounts involved per beneficiary.Reason to derive this feature is that one beneficiary \n",
    "## can go to multiple providers and can be involved in fraud cases\n",
    "Train_ProviderWithPatientDetailsdata[\"PerBeneIDAvg_InscClaimAmtReimbursed\"]=Train_ProviderWithPatientDetailsdata.groupby('BeneID')['InscClaimAmtReimbursed'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerBeneIDAvg_DeductibleAmtPaid\"]=Train_ProviderWithPatientDetailsdata.groupby('BeneID')['DeductibleAmtPaid'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerBeneIDAvg_IPAnnualReimbursementAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('BeneID')['IPAnnualReimbursementAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerBeneIDAvg_IPAnnualDeductibleAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('BeneID')['IPAnnualDeductibleAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerBeneIDAvg_OPAnnualReimbursementAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('BeneID')['OPAnnualReimbursementAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerBeneIDAvg_OPAnnualDeductibleAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('BeneID')['OPAnnualDeductibleAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerBeneIDAvg_AdmitForDays\"]=Train_ProviderWithPatientDetailsdata.groupby('BeneID')['AdmitForDays'].transform('mean')\n",
    "\n",
    "\n",
    "Test_ProviderWithPatientDetailsdata[\"PerBeneIDAvg_InscClaimAmtReimbursed\"]=Test_ProviderWithPatientDetailsdata.groupby('BeneID')['InscClaimAmtReimbursed'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerBeneIDAvg_DeductibleAmtPaid\"]=Test_ProviderWithPatientDetailsdata.groupby('BeneID')['DeductibleAmtPaid'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerBeneIDAvg_IPAnnualReimbursementAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('BeneID')['IPAnnualReimbursementAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerBeneIDAvg_IPAnnualDeductibleAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('BeneID')['IPAnnualDeductibleAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerBeneIDAvg_OPAnnualReimbursementAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('BeneID')['OPAnnualReimbursementAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerBeneIDAvg_OPAnnualDeductibleAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('BeneID')['OPAnnualDeductibleAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerBeneIDAvg_AdmitForDays\"]=Test_ProviderWithPatientDetailsdata.groupby('BeneID')['AdmitForDays'].transform('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Average features grouped by OtherPhysician**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Average features grouped by OtherPhysician.\n",
    "\n",
    "Train_ProviderWithPatientDetailsdata[\"PerOtherPhysicianAvg_InscClaimAmtReimbursed\"]=Train_ProviderWithPatientDetailsdata.groupby('OtherPhysician')['InscClaimAmtReimbursed'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerOtherPhysicianAvg_DeductibleAmtPaid\"]=Train_ProviderWithPatientDetailsdata.groupby('OtherPhysician')['DeductibleAmtPaid'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerOtherPhysicianAvg_IPAnnualReimbursementAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('OtherPhysician')['IPAnnualReimbursementAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerOtherPhysicianAvg_IPAnnualDeductibleAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('OtherPhysician')['IPAnnualDeductibleAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerOtherPhysicianAvg_OPAnnualReimbursementAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('OtherPhysician')['OPAnnualReimbursementAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerOtherPhysicianAvg_OPAnnualDeductibleAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('OtherPhysician')['OPAnnualDeductibleAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerOtherPhysicianAvg_AdmitForDays\"]=Train_ProviderWithPatientDetailsdata.groupby('OtherPhysician')['AdmitForDays'].transform('mean')\n",
    "\n",
    "Test_ProviderWithPatientDetailsdata[\"PerOtherPhysicianAvg_InscClaimAmtReimbursed\"]=Test_ProviderWithPatientDetailsdata.groupby('OtherPhysician')['InscClaimAmtReimbursed'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerOtherPhysicianAvg_DeductibleAmtPaid\"]=Test_ProviderWithPatientDetailsdata.groupby('OtherPhysician')['DeductibleAmtPaid'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerOtherPhysicianAvg_IPAnnualReimbursementAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('OtherPhysician')['IPAnnualReimbursementAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerOtherPhysicianAvg_IPAnnualDeductibleAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('OtherPhysician')['IPAnnualDeductibleAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerOtherPhysicianAvg_OPAnnualReimbursementAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('OtherPhysician')['OPAnnualReimbursementAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerOtherPhysicianAvg_OPAnnualDeductibleAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('OtherPhysician')['OPAnnualDeductibleAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerOtherPhysicianAvg_AdmitForDays\"]=Test_ProviderWithPatientDetailsdata.groupby('OtherPhysician')['AdmitForDays'].transform('mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Average features grouped by OperatingPhysician**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Average features grouped by OperatingPhysician\n",
    "\n",
    "Train_ProviderWithPatientDetailsdata[\"PerOperatingPhysicianAvg_InscClaimAmtReimbursed\"]=Train_ProviderWithPatientDetailsdata.groupby('OperatingPhysician')['InscClaimAmtReimbursed'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerOperatingPhysicianAvg_DeductibleAmtPaid\"]=Train_ProviderWithPatientDetailsdata.groupby('OperatingPhysician')['DeductibleAmtPaid'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerOperatingPhysicianAvg_IPAnnualReimbursementAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('OperatingPhysician')['IPAnnualReimbursementAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerOperatingPhysicianAvg_IPAnnualDeductibleAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('OperatingPhysician')['IPAnnualDeductibleAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerOperatingPhysicianAvg_OPAnnualReimbursementAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('OperatingPhysician')['OPAnnualReimbursementAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerOperatingPhysicianAvg_OPAnnualDeductibleAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('OperatingPhysician')['OPAnnualDeductibleAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerOperatingPhysicianAvg_AdmitForDays\"]=Train_ProviderWithPatientDetailsdata.groupby('OperatingPhysician')['AdmitForDays'].transform('mean')\n",
    "\n",
    "Test_ProviderWithPatientDetailsdata[\"PerOperatingPhysicianAvg_InscClaimAmtReimbursed\"]=Test_ProviderWithPatientDetailsdata.groupby('OperatingPhysician')['InscClaimAmtReimbursed'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerOperatingPhysicianAvg_DeductibleAmtPaid\"]=Test_ProviderWithPatientDetailsdata.groupby('OperatingPhysician')['DeductibleAmtPaid'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerOperatingPhysicianAvg_IPAnnualReimbursementAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('OperatingPhysician')['IPAnnualReimbursementAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerOperatingPhysicianAvg_IPAnnualDeductibleAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('OperatingPhysician')['IPAnnualDeductibleAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerOperatingPhysicianAvg_OPAnnualReimbursementAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('OperatingPhysician')['OPAnnualReimbursementAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerOperatingPhysicianAvg_OPAnnualDeductibleAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('OperatingPhysician')['OPAnnualDeductibleAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerOperatingPhysicianAvg_AdmitForDays\"]=Test_ProviderWithPatientDetailsdata.groupby('OperatingPhysician')['AdmitForDays'].transform('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Average features grouped by AttendingPhysician**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Average features grouped by AttendingPhysician   \n",
    "\n",
    "Train_ProviderWithPatientDetailsdata[\"PerAttendingPhysicianAvg_InscClaimAmtReimbursed\"]=Train_ProviderWithPatientDetailsdata.groupby('AttendingPhysician')['InscClaimAmtReimbursed'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerAttendingPhysicianAvg_DeductibleAmtPaid\"]=Train_ProviderWithPatientDetailsdata.groupby('AttendingPhysician')['DeductibleAmtPaid'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerAttendingPhysicianAvg_IPAnnualReimbursementAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('AttendingPhysician')['IPAnnualReimbursementAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerAttendingPhysicianAvg_IPAnnualDeductibleAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('AttendingPhysician')['IPAnnualDeductibleAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerAttendingPhysicianAvg_OPAnnualReimbursementAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('AttendingPhysician')['OPAnnualReimbursementAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerAttendingPhysicianAvg_OPAnnualDeductibleAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('AttendingPhysician')['OPAnnualDeductibleAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerAttendingPhysicianAvg_AdmitForDays\"]=Train_ProviderWithPatientDetailsdata.groupby('AttendingPhysician')['AdmitForDays'].transform('mean')\n",
    "\n",
    "\n",
    "Test_ProviderWithPatientDetailsdata[\"PerAttendingPhysicianAvg_InscClaimAmtReimbursed\"]=Test_ProviderWithPatientDetailsdata.groupby('AttendingPhysician')['InscClaimAmtReimbursed'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerAttendingPhysicianAvg_DeductibleAmtPaid\"]=Test_ProviderWithPatientDetailsdata.groupby('AttendingPhysician')['DeductibleAmtPaid'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerAttendingPhysicianAvg_IPAnnualReimbursementAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('AttendingPhysician')['IPAnnualReimbursementAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerAttendingPhysicianAvg_IPAnnualDeductibleAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('AttendingPhysician')['IPAnnualDeductibleAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerAttendingPhysicianAvg_OPAnnualReimbursementAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('AttendingPhysician')['OPAnnualReimbursementAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerAttendingPhysicianAvg_OPAnnualDeductibleAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('AttendingPhysician')['OPAnnualDeductibleAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerAttendingPhysicianAvg_AdmitForDays\"]=Test_ProviderWithPatientDetailsdata.groupby('AttendingPhysician')['AdmitForDays'].transform('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Average features grouped by DiagnosisGroupCode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Average features grouped by DiagnosisGroupCode  \n",
    "\n",
    "Train_ProviderWithPatientDetailsdata[\"PerDiagnosisGroupCodeAvg_InscClaimAmtReimbursed\"]=Train_ProviderWithPatientDetailsdata.groupby('DiagnosisGroupCode')['InscClaimAmtReimbursed'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerDiagnosisGroupCodeAvg_DeductibleAmtPaid\"]=Train_ProviderWithPatientDetailsdata.groupby('DiagnosisGroupCode')['DeductibleAmtPaid'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerDiagnosisGroupCodeAvg_IPAnnualReimbursementAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('DiagnosisGroupCode')['IPAnnualReimbursementAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerDiagnosisGroupCodeAvg_IPAnnualDeductibleAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('DiagnosisGroupCode')['IPAnnualDeductibleAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerDiagnosisGroupCodeAvg_OPAnnualReimbursementAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('DiagnosisGroupCode')['OPAnnualReimbursementAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerDiagnosisGroupCodeAvg_OPAnnualDeductibleAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('DiagnosisGroupCode')['OPAnnualDeductibleAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerDiagnosisGroupCodeAvg_AdmitForDays\"]=Train_ProviderWithPatientDetailsdata.groupby('DiagnosisGroupCode')['AdmitForDays'].transform('mean')\n",
    "\n",
    "Test_ProviderWithPatientDetailsdata[\"PerDiagnosisGroupCodeAvg_InscClaimAmtReimbursed\"]=Test_ProviderWithPatientDetailsdata.groupby('DiagnosisGroupCode')['InscClaimAmtReimbursed'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerDiagnosisGroupCodeAvg_DeductibleAmtPaid\"]=Test_ProviderWithPatientDetailsdata.groupby('DiagnosisGroupCode')['DeductibleAmtPaid'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerDiagnosisGroupCodeAvg_IPAnnualReimbursementAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('DiagnosisGroupCode')['IPAnnualReimbursementAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerDiagnosisGroupCodeAvg_IPAnnualDeductibleAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('DiagnosisGroupCode')['IPAnnualDeductibleAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerDiagnosisGroupCodeAvg_OPAnnualReimbursementAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('DiagnosisGroupCode')['OPAnnualReimbursementAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerDiagnosisGroupCodeAvg_OPAnnualDeductibleAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('DiagnosisGroupCode')['OPAnnualDeductibleAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerDiagnosisGroupCodeAvg_AdmitForDays\"]=Test_ProviderWithPatientDetailsdata.groupby('DiagnosisGroupCode')['AdmitForDays'].transform('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Average features grouped by ClmAdmitDiagnosisCode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Average features grouped by ClmAdmitDiagnosisCode \n",
    "\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmAdmitDiagnosisCodeAvg_InscClaimAmtReimbursed\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmAdmitDiagnosisCode')['InscClaimAmtReimbursed'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmAdmitDiagnosisCodeAvg_DeductibleAmtPaid\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmAdmitDiagnosisCode')['DeductibleAmtPaid'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmAdmitDiagnosisCodeAvg_IPAnnualReimbursementAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmAdmitDiagnosisCode')['IPAnnualReimbursementAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmAdmitDiagnosisCodeAvg_IPAnnualDeductibleAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmAdmitDiagnosisCode')['IPAnnualDeductibleAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmAdmitDiagnosisCodeAvg_OPAnnualReimbursementAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmAdmitDiagnosisCode')['OPAnnualReimbursementAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmAdmitDiagnosisCodeAvg_OPAnnualDeductibleAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmAdmitDiagnosisCode')['OPAnnualDeductibleAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmAdmitDiagnosisCodeAvg_AdmitForDays\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmAdmitDiagnosisCode')['AdmitForDays'].transform('mean')\n",
    "\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmAdmitDiagnosisCodeAvg_InscClaimAmtReimbursed\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmAdmitDiagnosisCode')['InscClaimAmtReimbursed'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmAdmitDiagnosisCodeAvg_DeductibleAmtPaid\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmAdmitDiagnosisCode')['DeductibleAmtPaid'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmAdmitDiagnosisCodeAvg_IPAnnualReimbursementAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmAdmitDiagnosisCode')['IPAnnualReimbursementAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmAdmitDiagnosisCodeAvg_IPAnnualDeductibleAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmAdmitDiagnosisCode')['IPAnnualDeductibleAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmAdmitDiagnosisCodeAvg_OPAnnualReimbursementAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmAdmitDiagnosisCode')['OPAnnualReimbursementAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmAdmitDiagnosisCodeAvg_OPAnnualDeductibleAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmAdmitDiagnosisCode')['OPAnnualDeductibleAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmAdmitDiagnosisCodeAvg_AdmitForDays\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmAdmitDiagnosisCode')['AdmitForDays'].transform('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Average features grouped by ClmProcedureCode_1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Average features grouped by ClmProcedureCode_1 \n",
    "\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_1Avg_InscClaimAmtReimbursed\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_1')['InscClaimAmtReimbursed'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_1Avg_DeductibleAmtPaid\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_1')['DeductibleAmtPaid'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_1Avg_IPAnnualReimbursementAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_1')['IPAnnualReimbursementAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_1Avg_IPAnnualDeductibleAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_1')['IPAnnualDeductibleAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_1Avg_OPAnnualReimbursementAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_1')['OPAnnualReimbursementAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_1Avg_OPAnnualDeductibleAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_1')['OPAnnualDeductibleAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_1Avg_AdmitForDays\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_1')['AdmitForDays'].transform('mean')\n",
    "\n",
    "\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_1Avg_InscClaimAmtReimbursed\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_1')['InscClaimAmtReimbursed'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_1Avg_DeductibleAmtPaid\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_1')['DeductibleAmtPaid'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_1Avg_IPAnnualReimbursementAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_1')['IPAnnualReimbursementAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_1Avg_IPAnnualDeductibleAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_1')['IPAnnualDeductibleAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_1Avg_OPAnnualReimbursementAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_1')['OPAnnualReimbursementAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_1Avg_OPAnnualDeductibleAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_1')['OPAnnualDeductibleAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_1Avg_AdmitForDays\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_1')['AdmitForDays'].transform('mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Average features grouped by ClmProcedureCode_2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Average features grouped by ClmProcedureCode_2\n",
    "\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_2Avg_InscClaimAmtReimbursed\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_2')['InscClaimAmtReimbursed'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_2Avg_DeductibleAmtPaid\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_2')['DeductibleAmtPaid'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_2Avg_IPAnnualReimbursementAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_2')['IPAnnualReimbursementAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_2Avg_IPAnnualDeductibleAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_2')['IPAnnualDeductibleAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_2Avg_OPAnnualReimbursementAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_2')['OPAnnualReimbursementAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_2Avg_OPAnnualDeductibleAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_2')['OPAnnualDeductibleAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_2Avg_AdmitForDays\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_2')['AdmitForDays'].transform('mean')\n",
    "\n",
    "\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_2Avg_InscClaimAmtReimbursed\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_2')['InscClaimAmtReimbursed'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_2Avg_DeductibleAmtPaid\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_2')['DeductibleAmtPaid'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_2Avg_IPAnnualReimbursementAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_2')['IPAnnualReimbursementAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_2Avg_IPAnnualDeductibleAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_2')['IPAnnualDeductibleAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_2Avg_OPAnnualReimbursementAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_2')['OPAnnualReimbursementAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_2Avg_OPAnnualDeductibleAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_2')['OPAnnualDeductibleAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_2Avg_AdmitForDays\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_2')['AdmitForDays'].transform('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Average features grouped by ClmProcedureCode_3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Average features grouped by ClmProcedureCode_3\n",
    "\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_3Avg_InscClaimAmtReimbursed\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_3')['InscClaimAmtReimbursed'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_3Avg_DeductibleAmtPaid\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_3')['DeductibleAmtPaid'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_3Avg_IPAnnualReimbursementAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_3')['IPAnnualReimbursementAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_3Avg_IPAnnualDeductibleAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_3')['IPAnnualDeductibleAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_3Avg_OPAnnualReimbursementAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_3')['OPAnnualReimbursementAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_3Avg_OPAnnualDeductibleAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_3')['OPAnnualDeductibleAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_3Avg_AdmitForDays\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_3')['AdmitForDays'].transform('mean')\n",
    "\n",
    "\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_3Avg_InscClaimAmtReimbursed\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_3')['InscClaimAmtReimbursed'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_3Avg_DeductibleAmtPaid\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_3')['DeductibleAmtPaid'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_3Avg_IPAnnualReimbursementAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_3')['IPAnnualReimbursementAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_3Avg_IPAnnualDeductibleAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_3')['IPAnnualDeductibleAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_3Avg_OPAnnualReimbursementAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_3')['OPAnnualReimbursementAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_3Avg_OPAnnualDeductibleAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_3')['OPAnnualDeductibleAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmProcedureCode_3Avg_AdmitForDays\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmProcedureCode_3')['AdmitForDays'].transform('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Average features grouped by ClmDiagnosisCode_1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Average features grouped by ClmDiagnosisCode_1 \n",
    "\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_1Avg_InscClaimAmtReimbursed\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_1')['InscClaimAmtReimbursed'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_1Avg_DeductibleAmtPaid\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_1')['DeductibleAmtPaid'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_1Avg_IPAnnualReimbursementAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_1')['IPAnnualReimbursementAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_1Avg_IPAnnualDeductibleAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_1')['IPAnnualDeductibleAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_1Avg_OPAnnualReimbursementAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_1')['OPAnnualReimbursementAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_1Avg_OPAnnualDeductibleAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_1')['OPAnnualDeductibleAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_1Avg_AdmitForDays\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_1')['AdmitForDays'].transform('mean')\n",
    "\n",
    "\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_1Avg_InscClaimAmtReimbursed\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_1')['InscClaimAmtReimbursed'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_1Avg_DeductibleAmtPaid\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_1')['DeductibleAmtPaid'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_1Avg_IPAnnualReimbursementAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_1')['IPAnnualReimbursementAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_1Avg_IPAnnualDeductibleAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_1')['IPAnnualDeductibleAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_1Avg_OPAnnualReimbursementAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_1')['OPAnnualReimbursementAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_1Avg_OPAnnualDeductibleAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_1')['OPAnnualDeductibleAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_1Avg_AdmitForDays\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_1')['AdmitForDays'].transform('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Average features grouped by ClmDiagnosisCode_2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Average features grouped by ClmDiagnosisCode_2\n",
    "\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_2Avg_InscClaimAmtReimbursed\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_2')['InscClaimAmtReimbursed'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_2Avg_DeductibleAmtPaid\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_2')['DeductibleAmtPaid'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_2Avg_IPAnnualReimbursementAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_2')['IPAnnualReimbursementAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_2Avg_IPAnnualDeductibleAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_2')['IPAnnualDeductibleAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_2Avg_OPAnnualReimbursementAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_2')['OPAnnualReimbursementAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_2Avg_OPAnnualDeductibleAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_2')['OPAnnualDeductibleAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_2Avg_AdmitForDays\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_2')['AdmitForDays'].transform('mean')\n",
    "\n",
    "\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_2Avg_InscClaimAmtReimbursed\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_2')['InscClaimAmtReimbursed'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_2Avg_DeductibleAmtPaid\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_2')['DeductibleAmtPaid'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_2Avg_IPAnnualReimbursementAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_2')['IPAnnualReimbursementAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_2Avg_IPAnnualDeductibleAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_2')['IPAnnualDeductibleAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_2Avg_OPAnnualReimbursementAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_2')['OPAnnualReimbursementAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_2Avg_OPAnnualDeductibleAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_2')['OPAnnualDeductibleAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_2Avg_AdmitForDays\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_2')['AdmitForDays'].transform('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Average features grouped by ClmDiagnosisCode_3\n",
    "**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Average features grouped by ClmDiagnosisCode_3\n",
    "\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_3Avg_InscClaimAmtReimbursed\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_3')['InscClaimAmtReimbursed'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_3Avg_DeductibleAmtPaid\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_3')['DeductibleAmtPaid'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_3Avg_IPAnnualReimbursementAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_3')['IPAnnualReimbursementAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_3Avg_IPAnnualDeductibleAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_3')['IPAnnualDeductibleAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_3Avg_OPAnnualReimbursementAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_3')['OPAnnualReimbursementAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_3Avg_OPAnnualDeductibleAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_3')['OPAnnualDeductibleAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_3Avg_AdmitForDays\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_3')['AdmitForDays'].transform('mean')\n",
    "\n",
    "\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_3Avg_InscClaimAmtReimbursed\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_3')['InscClaimAmtReimbursed'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_3Avg_DeductibleAmtPaid\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_3')['DeductibleAmtPaid'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_3Avg_IPAnnualReimbursementAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_3')['IPAnnualReimbursementAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_3Avg_IPAnnualDeductibleAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_3')['IPAnnualDeductibleAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_3Avg_OPAnnualReimbursementAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_3')['OPAnnualReimbursementAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_3Avg_OPAnnualDeductibleAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_3')['OPAnnualDeductibleAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_3Avg_AdmitForDays\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_3')['AdmitForDays'].transform('mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Average features grouped by ClmDiagnosisCode_4**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###  Average features grouped by ClmDiagnosisCode_4\n",
    "\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_4Avg_InscClaimAmtReimbursed\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_4')['InscClaimAmtReimbursed'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_4Avg_DeductibleAmtPaid\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_4')['DeductibleAmtPaid'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_4Avg_IPAnnualReimbursementAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_4')['IPAnnualReimbursementAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_4Avg_IPAnnualDeductibleAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_4')['IPAnnualDeductibleAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_4Avg_OPAnnualReimbursementAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_4')['OPAnnualReimbursementAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_4Avg_OPAnnualDeductibleAmt\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_4')['OPAnnualDeductibleAmt'].transform('mean')\n",
    "Train_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_4Avg_AdmitForDays\"]=Train_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_4')['AdmitForDays'].transform('mean')\n",
    "\n",
    "\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_4Avg_InscClaimAmtReimbursed\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_4')['InscClaimAmtReimbursed'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_4Avg_DeductibleAmtPaid\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_4')['DeductibleAmtPaid'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_4Avg_IPAnnualReimbursementAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_4')['IPAnnualReimbursementAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_4Avg_IPAnnualDeductibleAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_4')['IPAnnualDeductibleAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_4Avg_OPAnnualReimbursementAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_4')['OPAnnualReimbursementAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_4Avg_OPAnnualDeductibleAmt\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_4')['OPAnnualDeductibleAmt'].transform('mean')\n",
    "Test_ProviderWithPatientDetailsdata[\"PerClmDiagnosisCode_4Avg_AdmitForDays\"]=Test_ProviderWithPatientDetailsdata.groupby('ClmDiagnosisCode_4')['AdmitForDays'].transform('mean')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Claims are filed by Provider,so fraud can be organized crime.So we will check ClmCounts filed by Providers and when pairs like Provider +BeneID, Provider+Attending Physician, Provider+ClmAdmitDiagnosisCode, Provider+ClmProcedureCode_1,Provider+ClmDiagnosisCode_1 are together.**\n",
    "\n",
    "\n",
    "**Average Feature based on grouping based on combinations of different variables.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Average Feature based on grouping based on combinations of different variables\n",
    "\n",
    "Train_ProviderWithPatientDetailsdata[\"ClmCount_Provider\"]=Train_ProviderWithPatientDetailsdata.groupby(['Provider'])['ClaimID'].transform('count')\n",
    "Train_ProviderWithPatientDetailsdata[\"ClmCount_Provider_BeneID\"]=Train_ProviderWithPatientDetailsdata.groupby(['Provider','BeneID'])['ClaimID'].transform('count')\n",
    "Train_ProviderWithPatientDetailsdata[\"ClmCount_Provider_AttendingPhysician\"]=Train_ProviderWithPatientDetailsdata.groupby(['Provider','AttendingPhysician'])['ClaimID'].transform('count')\n",
    "Train_ProviderWithPatientDetailsdata[\"ClmCount_Provider_OtherPhysician\"]=Train_ProviderWithPatientDetailsdata.groupby(['Provider','OtherPhysician'])['ClaimID'].transform('count')\n",
    "Train_ProviderWithPatientDetailsdata[\"ClmCount_Provider_OperatingPhysician\"]=Train_ProviderWithPatientDetailsdata.groupby(['Provider','OperatingPhysician'])['ClaimID'].transform('count')\n",
    "Train_ProviderWithPatientDetailsdata[\"ClmCount_Provider_ClmAdmitDiagnosisCode\"]=Train_ProviderWithPatientDetailsdata.groupby(['Provider','ClmAdmitDiagnosisCode'])['ClaimID'].transform('count')\n",
    "Train_ProviderWithPatientDetailsdata[\"ClmCount_Provider_ClmProcedureCode_1\"]=Train_ProviderWithPatientDetailsdata.groupby(['Provider','ClmProcedureCode_1'])['ClaimID'].transform('count')\n",
    "Train_ProviderWithPatientDetailsdata[\"ClmCount_Provider_ClmProcedureCode_2\"]=Train_ProviderWithPatientDetailsdata.groupby(['Provider','ClmProcedureCode_2'])['ClaimID'].transform('count')\n",
    "Train_ProviderWithPatientDetailsdata[\"ClmCount_Provider_ClmProcedureCode_3\"]=Train_ProviderWithPatientDetailsdata.groupby(['Provider','ClmProcedureCode_3'])['ClaimID'].transform('count')\n",
    "Train_ProviderWithPatientDetailsdata[\"ClmCount_Provider_ClmProcedureCode_4\"]=Train_ProviderWithPatientDetailsdata.groupby(['Provider','ClmProcedureCode_4'])['ClaimID'].transform('count')\n",
    "Train_ProviderWithPatientDetailsdata[\"ClmCount_Provider_ClmProcedureCode_5\"]=Train_ProviderWithPatientDetailsdata.groupby(['Provider','ClmProcedureCode_5'])['ClaimID'].transform('count')\n",
    "Train_ProviderWithPatientDetailsdata[\"ClmCount_Provider_ClmDiagnosisCode_1\"]=Train_ProviderWithPatientDetailsdata.groupby(['Provider','ClmDiagnosisCode_1'])['ClaimID'].transform('count')\n",
    "Train_ProviderWithPatientDetailsdata[\"ClmCount_Provider_ClmDiagnosisCode_2\"]=Train_ProviderWithPatientDetailsdata.groupby(['Provider','ClmDiagnosisCode_2'])['ClaimID'].transform('count')\n",
    "Train_ProviderWithPatientDetailsdata[\"ClmCount_Provider_ClmDiagnosisCode_3\"]=Train_ProviderWithPatientDetailsdata.groupby(['Provider','ClmDiagnosisCode_3'])['ClaimID'].transform('count')\n",
    "Train_ProviderWithPatientDetailsdata[\"ClmCount_Provider_ClmDiagnosisCode_4\"]=Train_ProviderWithPatientDetailsdata.groupby(['Provider','ClmDiagnosisCode_4'])['ClaimID'].transform('count')\n",
    "Train_ProviderWithPatientDetailsdata[\"ClmCount_Provider_ClmDiagnosisCode_5\"]=Train_ProviderWithPatientDetailsdata.groupby(['Provider','ClmDiagnosisCode_5'])['ClaimID'].transform('count')\n",
    "Train_ProviderWithPatientDetailsdata[\"ClmCount_Provider_ClmDiagnosisCode_6\"]=Train_ProviderWithPatientDetailsdata.groupby(['Provider','ClmDiagnosisCode_6'])['ClaimID'].transform('count')\n",
    "Train_ProviderWithPatientDetailsdata[\"ClmCount_Provider_ClmDiagnosisCode_7\"]=Train_ProviderWithPatientDetailsdata.groupby(['Provider','ClmDiagnosisCode_7'])['ClaimID'].transform('count')\n",
    "Train_ProviderWithPatientDetailsdata[\"ClmCount_Provider_ClmDiagnosisCode_8\"]=Train_ProviderWithPatientDetailsdata.groupby(['Provider','ClmDiagnosisCode_8'])['ClaimID'].transform('count')\n",
    "Train_ProviderWithPatientDetailsdata[\"ClmCount_Provider_ClmDiagnosisCode_9\"]=Train_ProviderWithPatientDetailsdata.groupby(['Provider','ClmDiagnosisCode_9'])['ClaimID'].transform('count')\n",
    "Train_ProviderWithPatientDetailsdata[\"ClmCount_Provider_DiagnosisGroupCode\"]=Train_ProviderWithPatientDetailsdata.groupby(['Provider','DiagnosisGroupCode'])['ClaimID'].transform('count')\n",
    "\n",
    "Train_ProviderWithPatientDetailsdata[\"ClmCount_Provider_BeneID_AttendingPhysician\"]=Train_ProviderWithPatientDetailsdata.groupby(['Provider','BeneID','AttendingPhysician'])['ClaimID'].transform('count')\n",
    "Train_ProviderWithPatientDetailsdata[\"ClmCount_Provider_BeneID_OtherPhysician\"]=Train_ProviderWithPatientDetailsdata.groupby(['Provider','BeneID','OtherPhysician'])['ClaimID'].transform('count')\n",
    "Train_ProviderWithPatientDetailsdata[\"ClmCount_Provider_BeneID_AttendingPhysician_ClmProcedureCode_1\"]=Train_ProviderWithPatientDetailsdata.groupby(['Provider','BeneID','AttendingPhysician','ClmProcedureCode_1'])['ClaimID'].transform('count')\n",
    "Train_ProviderWithPatientDetailsdata[\"ClmCount_Provider_BeneID_AttendingPhysician_ClmDiagnosisCode_1\"]=Train_ProviderWithPatientDetailsdata.groupby(['Provider','BeneID','AttendingPhysician','ClmDiagnosisCode_1'])['ClaimID'].transform('count')\n",
    "Train_ProviderWithPatientDetailsdata[\"ClmCount_Provider_BeneID_OperatingPhysician\"]=Train_ProviderWithPatientDetailsdata.groupby(['Provider','BeneID','OperatingPhysician'])['ClaimID'].transform('count')\n",
    "Train_ProviderWithPatientDetailsdata[\"ClmCount_Provider_BeneID_ClmProcedureCode_1\"]=Train_ProviderWithPatientDetailsdata.groupby(['Provider','BeneID','ClmProcedureCode_1'])['ClaimID'].transform('count')\n",
    "Train_ProviderWithPatientDetailsdata[\"ClmCount_Provider_BeneID_ClmDiagnosisCode_1\"]=Train_ProviderWithPatientDetailsdata.groupby(['Provider','BeneID','ClmDiagnosisCode_1'])['ClaimID'].transform('count')\n",
    "Train_ProviderWithPatientDetailsdata[\"ClmCount_Provider_BeneID_ClmDiagnosisCode_1_ClmProcedureCode_1\"]=Train_ProviderWithPatientDetailsdata.groupby(['Provider','BeneID','ClmDiagnosisCode_1','ClmProcedureCode_1'])['ClaimID'].transform('count')\n",
    "\n",
    "\n",
    "Test_ProviderWithPatientDetailsdata[\"ClmCount_Provider\"]=Test_ProviderWithPatientDetailsdata.groupby(['Provider'])['ClaimID'].transform('count')\n",
    "Test_ProviderWithPatientDetailsdata[\"ClmCount_Provider_BeneID\"]=Test_ProviderWithPatientDetailsdata.groupby(['Provider','BeneID'])['ClaimID'].transform('count')\n",
    "Test_ProviderWithPatientDetailsdata[\"ClmCount_Provider_AttendingPhysician\"]=Test_ProviderWithPatientDetailsdata.groupby(['Provider','AttendingPhysician'])['ClaimID'].transform('count')\n",
    "Test_ProviderWithPatientDetailsdata[\"ClmCount_Provider_OtherPhysician\"]=Test_ProviderWithPatientDetailsdata.groupby(['Provider','OtherPhysician'])['ClaimID'].transform('count')\n",
    "Test_ProviderWithPatientDetailsdata[\"ClmCount_Provider_OperatingPhysician\"]=Test_ProviderWithPatientDetailsdata.groupby(['Provider','OperatingPhysician'])['ClaimID'].transform('count')\n",
    "Test_ProviderWithPatientDetailsdata[\"ClmCount_Provider_ClmAdmitDiagnosisCode\"]=Test_ProviderWithPatientDetailsdata.groupby(['Provider','ClmAdmitDiagnosisCode'])['ClaimID'].transform('count')\n",
    "Test_ProviderWithPatientDetailsdata[\"ClmCount_Provider_ClmProcedureCode_1\"]=Test_ProviderWithPatientDetailsdata.groupby(['Provider','ClmProcedureCode_1'])['ClaimID'].transform('count')\n",
    "Test_ProviderWithPatientDetailsdata[\"ClmCount_Provider_ClmProcedureCode_2\"]=Test_ProviderWithPatientDetailsdata.groupby(['Provider','ClmProcedureCode_2'])['ClaimID'].transform('count')\n",
    "Test_ProviderWithPatientDetailsdata[\"ClmCount_Provider_ClmProcedureCode_3\"]=Test_ProviderWithPatientDetailsdata.groupby(['Provider','ClmProcedureCode_3'])['ClaimID'].transform('count')\n",
    "Test_ProviderWithPatientDetailsdata[\"ClmCount_Provider_ClmProcedureCode_4\"]=Test_ProviderWithPatientDetailsdata.groupby(['Provider','ClmProcedureCode_4'])['ClaimID'].transform('count')\n",
    "Test_ProviderWithPatientDetailsdata[\"ClmCount_Provider_ClmProcedureCode_5\"]=Test_ProviderWithPatientDetailsdata.groupby(['Provider','ClmProcedureCode_5'])['ClaimID'].transform('count')\n",
    "Test_ProviderWithPatientDetailsdata[\"ClmCount_Provider_ClmDiagnosisCode_1\"]=Test_ProviderWithPatientDetailsdata.groupby(['Provider','ClmDiagnosisCode_1'])['ClaimID'].transform('count')\n",
    "Test_ProviderWithPatientDetailsdata[\"ClmCount_Provider_ClmDiagnosisCode_2\"]=Test_ProviderWithPatientDetailsdata.groupby(['Provider','ClmDiagnosisCode_2'])['ClaimID'].transform('count')\n",
    "Test_ProviderWithPatientDetailsdata[\"ClmCount_Provider_ClmDiagnosisCode_3\"]=Test_ProviderWithPatientDetailsdata.groupby(['Provider','ClmDiagnosisCode_3'])['ClaimID'].transform('count')\n",
    "Test_ProviderWithPatientDetailsdata[\"ClmCount_Provider_ClmDiagnosisCode_4\"]=Test_ProviderWithPatientDetailsdata.groupby(['Provider','ClmDiagnosisCode_4'])['ClaimID'].transform('count')\n",
    "Test_ProviderWithPatientDetailsdata[\"ClmCount_Provider_ClmDiagnosisCode_5\"]=Test_ProviderWithPatientDetailsdata.groupby(['Provider','ClmDiagnosisCode_5'])['ClaimID'].transform('count')\n",
    "Test_ProviderWithPatientDetailsdata[\"ClmCount_Provider_ClmDiagnosisCode_6\"]=Test_ProviderWithPatientDetailsdata.groupby(['Provider','ClmDiagnosisCode_6'])['ClaimID'].transform('count')\n",
    "Test_ProviderWithPatientDetailsdata[\"ClmCount_Provider_ClmDiagnosisCode_7\"]=Test_ProviderWithPatientDetailsdata.groupby(['Provider','ClmDiagnosisCode_7'])['ClaimID'].transform('count')\n",
    "Test_ProviderWithPatientDetailsdata[\"ClmCount_Provider_ClmDiagnosisCode_8\"]=Test_ProviderWithPatientDetailsdata.groupby(['Provider','ClmDiagnosisCode_8'])['ClaimID'].transform('count')\n",
    "Test_ProviderWithPatientDetailsdata[\"ClmCount_Provider_ClmDiagnosisCode_9\"]=Test_ProviderWithPatientDetailsdata.groupby(['Provider','ClmDiagnosisCode_9'])['ClaimID'].transform('count')\n",
    "Test_ProviderWithPatientDetailsdata[\"ClmCount_Provider_DiagnosisGroupCode\"]=Test_ProviderWithPatientDetailsdata.groupby(['Provider','DiagnosisGroupCode'])['ClaimID'].transform('count')\n",
    "\n",
    "Test_ProviderWithPatientDetailsdata[\"ClmCount_Provider_BeneID_AttendingPhysician\"]=Test_ProviderWithPatientDetailsdata.groupby(['Provider','BeneID','AttendingPhysician'])['ClaimID'].transform('count')\n",
    "Test_ProviderWithPatientDetailsdata[\"ClmCount_Provider_BeneID_OtherPhysician\"]=Test_ProviderWithPatientDetailsdata.groupby(['Provider','BeneID','OtherPhysician'])['ClaimID'].transform('count')\n",
    "Test_ProviderWithPatientDetailsdata[\"ClmCount_Provider_BeneID_AttendingPhysician_ClmProcedureCode_1\"]=Test_ProviderWithPatientDetailsdata.groupby(['Provider','BeneID','AttendingPhysician','ClmProcedureCode_1'])['ClaimID'].transform('count')\n",
    "Test_ProviderWithPatientDetailsdata[\"ClmCount_Provider_BeneID_AttendingPhysician_ClmDiagnosisCode_1\"]=Test_ProviderWithPatientDetailsdata.groupby(['Provider','BeneID','AttendingPhysician','ClmDiagnosisCode_1'])['ClaimID'].transform('count')\n",
    "Test_ProviderWithPatientDetailsdata[\"ClmCount_Provider_BeneID_OperatingPhysician\"]=Test_ProviderWithPatientDetailsdata.groupby(['Provider','BeneID','OperatingPhysician'])['ClaimID'].transform('count')\n",
    "Test_ProviderWithPatientDetailsdata[\"ClmCount_Provider_BeneID_ClmProcedureCode_1\"]=Test_ProviderWithPatientDetailsdata.groupby(['Provider','BeneID','ClmProcedureCode_1'])['ClaimID'].transform('count')\n",
    "Test_ProviderWithPatientDetailsdata[\"ClmCount_Provider_BeneID_ClmDiagnosisCode_1\"]=Test_ProviderWithPatientDetailsdata.groupby(['Provider','BeneID','ClmDiagnosisCode_1'])['ClaimID'].transform('count')\n",
    "Test_ProviderWithPatientDetailsdata[\"ClmCount_Provider_BeneID_ClmDiagnosisCode_1_ClmProcedureCode_1\"]=Test_ProviderWithPatientDetailsdata.groupby(['Provider','BeneID','ClmDiagnosisCode_1','ClmProcedureCode_1'])['ClaimID'].transform('count')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets Check Shape after creating new features.\n",
    "\n",
    "print('Train_ProviderWithPatientDetailsdata shape-',Train_ProviderWithPatientDetailsdata.shape)\n",
    "print('Test_ProviderWithPatientDetailsdata shape-',Test_ProviderWithPatientDetailsdata.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ## Lets Check unique values of ICD Diagnosis Codes\n",
    "\n",
    "diagnosiscode_2chars=Train_ProviderWithPatientDetailsdata['ClmDiagnosisCode_1'].astype(str).str[0:2]\n",
    "\n",
    "diagnosiscode_2chars.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**x=diagnosiscode_2chars.sort_values(ascending=True)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.unique()\n",
    "#x.value_counts()[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Above Data Shows that if we take only first 2 characters of diagnosis code for the purpose of grouping them ,we might end up creating large sparse matrix ,as each 'code' column will generate 120+ dummy columns.This will increase computational time and loose explicability.**\n",
    "\n",
    "**Data Preprocessing**\n",
    "\n",
    "**Impute Numeric columns with 0's.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Lets impute numeric columns with 0\n",
    "\n",
    "cols1 = Train_ProviderWithPatientDetailsdata.select_dtypes([np.number]).columns\n",
    "cols2 = Train_ProviderWithPatientDetailsdata.select_dtypes(exclude = [np.number]).columns\n",
    "\n",
    "Train_ProviderWithPatientDetailsdata[cols1] = Train_ProviderWithPatientDetailsdata[cols1].fillna(value=0)\n",
    "Test_ProviderWithPatientDetailsdata[cols1]=Test_ProviderWithPatientDetailsdata[cols1].fillna(value=0)\n",
    "print('Test_ProviderWithPatientDetailsdata shape:',Test_ProviderWithPatientDetailsdata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Feature Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets remove unnecessary columns ,as we grouped based on these columns and derived maximum infromation from them.\n",
    "\n",
    "cols=Train_ProviderWithPatientDetailsdata.columns\n",
    "cols[:58]\n",
    "\n",
    "remove_these_columns=['BeneID', 'ClaimID', 'ClaimStartDt','ClaimEndDt','AttendingPhysician',\n",
    "       'OperatingPhysician', 'OtherPhysician', 'ClmDiagnosisCode_1',\n",
    "       'ClmDiagnosisCode_2', 'ClmDiagnosisCode_3', 'ClmDiagnosisCode_4',\n",
    "       'ClmDiagnosisCode_5', 'ClmDiagnosisCode_6', 'ClmDiagnosisCode_7',\n",
    "       'ClmDiagnosisCode_8', 'ClmDiagnosisCode_9', 'ClmDiagnosisCode_10',\n",
    "       'ClmProcedureCode_1', 'ClmProcedureCode_2', 'ClmProcedureCode_3',\n",
    "       'ClmProcedureCode_4', 'ClmProcedureCode_5', 'ClmProcedureCode_6',\n",
    "       'ClmAdmitDiagnosisCode', 'AdmissionDt',\n",
    "       'DischargeDt', 'DiagnosisGroupCode','DOB', 'DOD',\n",
    "        'State', 'County']\n",
    "\n",
    "Train_category_removed=Train_ProviderWithPatientDetailsdata.drop(axis=1,columns=remove_these_columns)\n",
    "Test_category_removed=Test_ProviderWithPatientDetailsdata.drop(axis=1,columns=remove_these_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets Check shape and missing values\n",
    "\n",
    "print('Train Shape :',Train_category_removed.shape)\n",
    "print('Test Shape : ',Test_category_removed.shape)\n",
    "print(\"Train Missing Values\",Train_category_removed.isnull().sum().sum())\n",
    "print(\"Test Missing Values\",Test_category_removed.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Type Conversion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets Convert types of gender and race to categorical.\n",
    "\n",
    "Train_category_removed.Gender=Train_category_removed.Gender.astype('category')\n",
    "Test_category_removed.Gender=Test_category_removed.Gender.astype('category')\n",
    "\n",
    "Train_category_removed.Race=Train_category_removed.Race.astype('category')\n",
    "Test_category_removed.Race=Test_category_removed.Race.astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dummification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets create dummies for categorrical columns.\n",
    "\n",
    "Train_category_removed=pd.get_dummies(Train_category_removed,columns=['Gender','Race'],drop_first=True)\n",
    "Test_category_removed=pd.get_dummies(Test_category_removed,columns=['Gender','Race'],drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Train_category_removed.head().T\n",
    "Test_category_removed.iloc[135391:135393]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert Target values to 1 and 0,wher '1' means Yes and '0' means No**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    Train_category_removed.PotentialFraud.replace(['Yes','No'],['1','0'],inplace=True)\n",
    "Train_category_removed.head()\n",
    "Train_category_removed.PotentialFraud=Train_category_removed.PotentialFraud.astype('int64')\n",
    "Train_category_removed.PotentialFraud.dtypes\n",
    "Train_category_removed.PotentialFraud.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_category_removed.PotentialFraud.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Select only Test related data from merged data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Shape before removing',Test_category_removed.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_category_removed=Test_category_removed.iloc[:135392]   ##Remove train data from appended test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_category_removed.tail()   # Check last 5 records.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Test Shape before removing',Test_category_removed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Aggregation to the Providers level**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Lets aggregate claims data to unique providers.\n",
    "\n",
    "Train_category_removed_groupedbyProv_PF=Train_category_removed.groupby(['Provider','PotentialFraud'],as_index=False).agg('sum')\n",
    "Test_category_removed_groupedbyProv_PF=Test_category_removed.groupby(['Provider'],as_index=False).agg('sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Providers in Train:' ,Train_category_removed_groupedbyProv_PF.shape)\n",
    "print('Providers in Test :',Test_category_removed_groupedbyProv_PF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_category_removed_groupedbyProv_PF.head(3)\n",
    "Test_category_removed_groupedbyProv_PF.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train Validation split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train shape:',Train_category_removed_groupedbyProv_PF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_category_removed_groupedbyProv_PF.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets Seperate out Target and providers from independent variables.Create Target column y.\n",
    "\n",
    "X=Train_category_removed_groupedbyProv_PF.drop(axis=1,columns=['Provider','PotentialFraud'])\n",
    "y=Train_category_removed_groupedbyProv_PF['PotentialFraud']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Standardization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets apply StandardScaler and transform values to its z form,where 99.7% values range between -3 to 3.\n",
    "sc=StandardScaler()   # MinMaxScaler\n",
    "sc.fit(X)\n",
    "X_std=sc.transform(X)\n",
    "\n",
    "X_teststd=sc.transform(Test_category_removed_groupedbyProv_PF.iloc[:,1:])   #Apply Standard Scaler to unseen data as well.\n",
    "\n",
    "#print(X_std[0:3,:])\n",
    "#print(X_teststd[0:3,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X Shape:',X_std.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lets Split data in train and validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 'stratify=y' will make sure equal distribution of yes:no in both train and validation\n",
    "\n",
    "X_train,X_val,y_train,y_val = train_test_split(X_std,y,test_size=0.3,random_state=101,stratify=y,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train :',X_train.shape)\n",
    "print('y_train :',y_train.shape)\n",
    "\n",
    "print('X_val :',X_val.shape)\n",
    "print('y_val :',y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('X_teststd',X_teststd.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Building**\n",
    "\n",
    "\n",
    "**Logistic regression****"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "log = LogisticRegressionCV(cv=10,class_weight='balanced',random_state=123)    \n",
    "\n",
    "# The \"balanced\" mode uses the values of y to automatically adjust weights inversely proportional to class frequencies\n",
    "#in the input data as ``n_samples / (n_classes * np.bincount(y))``.\n",
    "\n",
    "log.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets predict probability of 1 and 0 for X_train and X_val\n",
    "\n",
    "log_train_pred_probability=log.predict_proba(X_train)\n",
    "log_val_pred_probability=log.predict_proba(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression Model Prediction for Train and Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets compare model prediction performance on train and Validation both.\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "\n",
    "sns.distplot(log.predict_proba(X_train)[:,1],color='darkblue')\n",
    "sns.distplot(log.predict_proba(X_val)[:,1],color='firebrick')\n",
    "plt.title('Predictions of Train and Validation ')\n",
    "plt.xlim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic regression : ROC Curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc,precision_recall_curve\n",
    "fpr, tpr, thresholds = roc_curve(y_val,log.predict_proba(X_val)[:,1])         #log_val_pred_probability[:,1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "\n",
    "for label in range(1,10,1):\n",
    "    plt.text((10-label)/10,(10-label)/10,thresholds[label*15],fontdict={'size': 14})\n",
    "\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fpr, tpr, thresholds =roc_curve(y_val, log.predict_proba(X_val)[:,1],pos_label=1)     #log_val_pred_probability[:,1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(\"Area under the ROC curve : %f\" % roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression : Precision Vs Recall Curve¶**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets Check precision and recall \n",
    "precision, recall, _ = precision_recall_curve(y_val, log.predict_proba(X_val)[:,1])\n",
    "\n",
    "plt.plot(precision,recall)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision Vs Recall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression : TPR Vs FPR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets Check Tpr vs fpr distribution.\n",
    "\n",
    "fig = plt.figure(figsize=(12,8))\n",
    "\n",
    "sns.distplot(tpr,color='firebrick')\n",
    "\n",
    "sns.distplot(fpr,color='darkblue')\n",
    "plt.title('TPR Vs FPR ')\n",
    "plt.xlim([-.25, 1.2])\n",
    "\n",
    "plt.text(0.1,4,'Negatives',color='darkblue')\n",
    "plt.text(0.7,4,'Positives',color='firebrick')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Distribution')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Probability Thresholding**\n",
    "\n",
    "\n",
    "**Thresholding Probabilities: \n",
    "\n",
    "If the algorithm returns probabilities (or some other score), thresholding can be applied after a model has been built. Essentially you change the classification threshold from 50-50 to an appropriate trade-off level. This typically can be optimized by generated a curve of the evaluation metric (e.g. F-measure). The limitation here is that you are making absolute trade-offs. Any modification in the cutoff will in turn decrease the accuracy of predicting the other class.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets Set probability Threshold to 0.60\n",
    "\n",
    "log_train_pred_60=(log.predict_proba(X_train)[:,1]>0.60).astype(bool)\n",
    "log_val_pred_60=(log.predict_proba(X_val)[:,1]>0.60).astype(bool)   # set threshold as 0.60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression : Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix, Accuracy, sensitivity and specificity\n",
    "\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,cohen_kappa_score,roc_auc_score,f1_score,auc\n",
    "\n",
    "cm0 = confusion_matrix(y_train, log_train_pred_60,labels=[1,0])\n",
    "print('Confusion Matrix Train : \\n', cm0)\n",
    "\n",
    "cm1 = confusion_matrix(y_val, log_val_pred_60,labels=[1,0])\n",
    "print('Confusion Matrix Val: \\n', cm1)\n",
    "\n",
    "total0=sum(sum(cm0))\n",
    "total1=sum(sum(cm1))\n",
    "#####from confusion matrix calculate accuracy\n",
    "accuracy0=(cm0[0,0]+cm0[1,1])/total0\n",
    "print ('Accuracy Train: ', accuracy0)\n",
    "\n",
    "accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
    "print ('Accuracy Val: ', accuracy1)\n",
    "\n",
    "sensitivity0 = cm0[0,0]/(cm0[0,0]+cm0[0,1])\n",
    "print('Sensitivity Train : ', sensitivity0 )\n",
    "\n",
    "sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "print('Sensitivity Val: ', sensitivity1 )\n",
    "\n",
    "\n",
    "specificity0 = cm0[1,1]/(cm0[1,0]+cm0[1,1])\n",
    "print('Specificity Train: ', specificity0)\n",
    "\n",
    "specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "print('Specificity Val: ', specificity1)\n",
    "\n",
    "KappaValue=cohen_kappa_score(y_val, log_val_pred_60)\n",
    "print(\"Kappa Value :\",KappaValue)\n",
    "AUC=roc_auc_score(y_val, log_val_pred_60)\n",
    "\n",
    "print(\"AUC         :\",AUC)\n",
    "\n",
    "print(\"F1-Score Train  : \",f1_score(y_train, log_train_pred_60))\n",
    "\n",
    "print(\"F1-Score Val  : \",f1_score(y_val, log_val_pred_60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "****Confusion Matrix** Confusion Matrix Train : [[ 270 84] [ 210 3223]] Confusion Matrix Val: [[ 103 49] [ 93 1378]] Accuracy Train: 0.922365988909427 Accuracy Val: 0.9125077017868145 Sensitivity Train : 0.7627118644067796 Sensitivity Val: 0.6776315789473685 Specificity Train: 0.9388290125254879 Specificity Val: 0.9367777022433719 Kappa Value : 0.5438304105142315 AUC : 0.8072046405953702 F1-Score Train : 0.6474820143884892 F1-Score Val : 0.5919540229885056**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets predict on Test data\n",
    "\n",
    "log_test_pred_60 = (log.predict_proba(X_teststd)[:,1]>0.60).astype(bool)\n",
    "log_test_pred=pd.DataFrame(log_test_pred_60)\n",
    "log_test_pred.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Lets Replace 1 with Yes and 0 With No\n",
    "Replacement = {1:'Yes',0:'No'}\n",
    "\n",
    "Labels=log_test_pred[0].apply(lambda x : Replacement[x])\n",
    "Labels.value_counts()    #Check count of Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Submission file\n",
    "\n",
    "submission_log=pd.DataFrame({\"Provider\":Test_category_removed_groupedbyProv_PF.Provider})\n",
    "submission_log['PotentialFraud']=Labels\n",
    "submission_log.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('Submission Shape:',submission_log.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write Submission file\n",
    "\n",
    "submission_log.to_csv(\"Submission_Logistic_Regression_F1_60_Threshold_60Prcnt.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_log.PotentialFraud.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets Apply Random Forest \n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=500,class_weight='balanced',random_state=123,max_depth=4)   # We will set max_depth =4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.fit(X_train,y_train)  #fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest : ROC Curve**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, thresholds = roc_curve(y_val, rfc.predict_proba(X_val)[:,1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "\n",
    "for label in range(1,10,1):\n",
    "    plt.text((10-label)/10,(10-label)/10,thresholds[label*15],fontdict={'size': 14})\n",
    "    \n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\n",
    "Random Forest : TPR Vs FPR**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12,8))\n",
    "\n",
    "sns.distplot(tpr,color='firebrick')\n",
    "\n",
    "sns.distplot(fpr,color='darkblue')\n",
    "plt.title('TPR Vs FPR ')\n",
    "plt.xlim([-.25, 1.2])\n",
    "\n",
    "plt.text(0.1,4,'Negatives',color='darkblue')\n",
    "plt.text(0.7,4,'Positives',color='firebrick')\n",
    "plt.xlabel('Probability')\n",
    "plt.ylabel('Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rfc_train_pred = (rfc.predict_proba(X_train)[:,1]>0.5).astype(bool)   # Set threshold to 0.5\n",
    "rfc_val_pred = (rfc.predict_proba(X_val)[:,1]>0.5).astype(bool)# Set threshold to 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest : Model Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Confusion matrix, Accuracy, sensitivity and specificity\n",
    "from sklearn.metrics import confusion_matrix,accuracy_score,cohen_kappa_score,roc_auc_score,f1_score,roc_curve\n",
    "\n",
    "cm0 = confusion_matrix(y_train, rfc_train_pred,labels=[1,0])\n",
    "print('Confusion Matrix Train : \\n', cm0)\n",
    "\n",
    "cm1 = confusion_matrix(y_val, rfc_val_pred,labels=[1,0])\n",
    "print('Confusion Matrix Test: \\n', cm1)\n",
    "\n",
    "total0=sum(sum(cm0))\n",
    "total1=sum(sum(cm1))\n",
    "#####from confusion matrix calculate accuracy\n",
    "accuracy0=(cm0[0,0]+cm0[1,1])/total0\n",
    "print ('Accuracy Train : ', accuracy0)\n",
    "\n",
    "accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
    "print ('Accuracy Test : ', accuracy1)\n",
    "\n",
    "sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "print('Sensitivity : ', sensitivity1 )\n",
    "\n",
    "specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "print('Specificity : ', specificity1)\n",
    "\n",
    "KappaValue=cohen_kappa_score(y_val, rfc_val_pred)\n",
    "print(\"Kappa Value :\",KappaValue)\n",
    "AUC=roc_auc_score(y_val, rfc_val_pred)\n",
    "print(\"AUC         :\",AUC)\n",
    "\n",
    "\n",
    "print(\"F1-Score Train\",f1_score(y_train,rfc_train_pred))\n",
    "print(\"F1-Score Validation : \",f1_score(y_val, rfc_val_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion Matrix Train : [[ 275 79] [ 239 3194]] Confusion Matrix Test: [[ 107 45] [ 105 1366]] Accuracy Train : 0.916028518616319 Accuracy Test : 0.9075785582255084 Sensitivity : 0.7039473684210527 Specificity : 0.9286199864038069 Kappa Value : 0.5374522157454344 AUC : 0.8162836774124298 F1-Score Train 0.6336405529953918 F1-Score Validation : 0.587912087912088**\n",
    "\n",
    "\n",
    "\n",
    "**Random Forest : Feature Importance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = list(Test_category_removed_groupedbyProv_PF.columns)\n",
    "# Get numerical feature importances\n",
    "importances = list(rfc.feature_importances_)\n",
    "# List of tuples with variable and importance\n",
    "feature_importances = [(feature, round(importance, 2)) for feature, importance in zip(feature_list[1:], importances)]\n",
    "# Sort the feature importances by most important first\n",
    "feature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n",
    "# Print out the feature and importances \n",
    "print('Top -20 features impacting Random forest model and their importance score :- \\n',)\n",
    "[print('Variable: {:20} Importance: {}'.format(*pair)) for pair in feature_importances[:15]];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest : Prediction on Unseen Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets predict on Test data\n",
    "\n",
    "rfc_test_pred = rfc.predict(X_teststd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rfc_test_pred=pd.DataFrame(rfc_test_pred)\n",
    "rfc_test_pred.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Lets Replace 1 with Yes and 0 With No\n",
    "\n",
    "Replacement = {1:'Yes',0:'No'}\n",
    "\n",
    "Labels=rfc_test_pred[0].apply(lambda x : Replacement[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "submission_rfc=pd.DataFrame({\"Provider\":Test_category_removed_groupedbyProv_PF.Provider})\n",
    "submission_rfc['PotentialFraud']=Labels\n",
    "submission_rfc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Shape Of Submission',submission_rfc.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #write Submission file\n",
    "\n",
    "submission_rfc.to_csv(\"Submission_Random_Forest_Estimator_F1_60_estimators_3000.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Other Models and their Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets import packages \n",
    "from collections import Counter\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clfs = {\n",
    "    #'mnb': MultinomialNB(),\n",
    "    'svm1': SVC(C=0.01,kernel='linear',probability=1),\n",
    "    'svm2': SVC(C=0.01,kernel='rbf',probability=1),\n",
    "    'svm3': SVC(C=.01,kernel='poly',degree=2,probability=1),\n",
    "    'ada': AdaBoostClassifier(),\n",
    "    'dtc': DecisionTreeClassifier(class_weight='balanced'),\n",
    "    'gbc': GradientBoostingClassifier(),\n",
    "    'lr': LogisticRegression(class_weight='balanced'),\n",
    "    'xgb': XGBClassifier(booster='gbtree')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets Fit These models and check their performance \n",
    "\n",
    "f1_scores = dict()\n",
    "for clf_name in clfs:\n",
    "    print(clf_name)\n",
    "    clf = clfs[clf_name]\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred =((clf.predict_proba(X_val)[:,1]>0.5).astype(bool))\n",
    "    f1_scores[clf_name] = f1_score(y_pred, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint.pprint(f1_scores) #Printing F1 Scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Above figures shows that Logistic regression model is performing better compared to other models.**\n",
    "\n",
    "\n",
    "\n",
    "**Principal Component Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_category_removed_groupedbyProv_PF.head(2)\n",
    "Test_category_removed_groupedbyProv_PF.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Standard Scaler to scale data\n",
    "sc_pca=StandardScaler()\n",
    "sc_pca=sc.fit(Train_category_removed_groupedbyProv_PF.iloc[:,2:])\n",
    "Train_category_removed_groupedbyProv_PF_scaled=sc_pca.transform(Train_category_removed_groupedbyProv_PF.iloc[:,2:])\n",
    "Train_category_removed_groupedbyProv_PF_scaled=pd.DataFrame(Train_category_removed_groupedbyProv_PF_scaled)\n",
    "\n",
    "Test_category_removed_groupedbyProv_PF_scaled=sc_pca.transform(Test_category_removed_groupedbyProv_PF.iloc[:,1:])\n",
    "Test_category_removed_groupedbyProv_PF_scaled=pd.DataFrame(Test_category_removed_groupedbyProv_PF_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_category_removed_groupedbyProv_PF_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Test_category_removed_groupedbyProv_PF_scaled.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PCA : Use maximum Variance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    from sklearn.decomposition import PCA  #import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets Take 29 PCA components\n",
    "pca = PCA(n_components=29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(Train_category_removed_groupedbyProv_PF_scaled.iloc[:,0:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('PCA Explained Variance :\\n',np.round(pca.explained_variance_ratio_,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Transform test and train based on train components\n",
    "train_pca=pca.transform(Train_category_removed_groupedbyProv_PF_scaled.iloc[:,0:])\n",
    "\n",
    "test_pca=pca.transform(Test_category_removed_groupedbyProv_PF_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Shape:',train_pca.shape)\n",
    "print('Test Shape:',test_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Convert to Pandas dataframe\n",
    "\n",
    "train_pca=pd.DataFrame((train_pca))\n",
    "test_pca=pd.DataFrame(test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train Shape:',train_pca.shape)\n",
    "print('Test Shape:',test_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Add target to train Data\n",
    "train_pca['PotentialFraud']=Train_category_removed_groupedbyProv_PF.PotentialFraud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pca.isnull().sum().sum()\n",
    "print('Train PCA',train_pca.shape)\n",
    "#train_pca.head()\n",
    "print('Test PCA',test_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pca.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Auto - Encoder**\n",
    "\n",
    "**An Autoencoder is a type of artificial neural network used to learn efficient data codings in an unsupervised manner. The aim of an autoencoder is to learn a representation (encoding) for a set of data, typically for dimensionality reduction. Along with the reduction side, a reconstructing side is learnt, where the autoencoder tries to generate from the reduced encoding a representation as close as possible to its original input, hence its name. Recently, the autoencoder concept has become more widely used for learning generative models of data.\n",
    "\n",
    "** We will use the same technique to learn patterns in non fraud data and we will train the model accordingly.We will use reconstruction error threshold ,to predict the Target class of data.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#Converting data to array\n",
    "train_pca = np.array(train_pca)\n",
    "test_pca =np.array(test_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into train and test and observing their dimensions\n",
    "X_train_pca, X_test_pca = train_test_split(train_pca, test_size=0.2, random_state=123)\n",
    "\n",
    "print(X_train_pca.shape)\n",
    "print(X_test_pca.shape)\n",
    "print(test_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Obtaining the fraud and non-fraud records in train\n",
    "print(np.unique(X_train_pca[:,29],return_counts=True))\n",
    "print(np.unique(X_test_pca[:,29],return_counts=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We will consider Non fraud data to train the autoencoder and check the threshold for reconstruction error on fraud data.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Now consider only the non-fraud records for training\n",
    "X_train_NF = X_train_pca[X_train_pca[:,-1] == 0]\n",
    "X_train_NF = X_train_NF[:,:-1]\n",
    "print(X_train_NF.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating out the fraud records from the train \n",
    "X_train_F = X_train_pca[X_train_pca[:,-1] == 1]\n",
    "print(X_train_F.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding/concatenating the fraud records from train data to the test\n",
    "X_test_pca=np.concatenate((X_test_pca,X_train_F),axis=0)\n",
    "print(X_test_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test,X_eval = train_test_split(X_test_pca, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape)\n",
    "print(X_eval.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Separating the independent and the class variable\n",
    "y_test = X_test[:,-1]\n",
    "X_test = X_test[:,:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Expanding the dimensions of y for later concatenation\n",
    "y_test = np.expand_dims(y_test, axis=1)\n",
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Autoencoder Network Building**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = X_train_NF.shape[1]   # Set input dimension to number of inputs\n",
    "encoding_dim = 15               # set encoding dimension to size 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "\n",
    "autoencoder = Sequential()   # Add sequential network\n",
    "\n",
    "autoencoder.add(Dropout(0.2, input_shape=(input_dim,)))   # Add dropout to add noise to data.\n",
    "autoencoder.add(Dense(encoding_dim, activation='relu'))   #Add Dense layer to encode patterns. \n",
    "autoencoder.add(Dense(input_dim, activation='linear'))    #Add Dense layer of size of input shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 100    #number of epochs\n",
    "batch_size = 32   #batchsize or sample size\n",
    "\n",
    "autoencoder.compile(optimizer='adam', \n",
    "                    loss='mean_squared_error', \n",
    "                    metrics=['mse'])                      ## We will use mse as a metric as each neuron can be a linear regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fit the model and save history\n",
    "hist = []\n",
    "for _ in range(100):\n",
    "    hist.append(autoencoder.fit(X_train_NF, X_train_NF,\n",
    "                    epochs=1,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                     validation_split=0.3,\n",
    "                    verbose=0).history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist[0:5]   #First 5 records of history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Making predictions on the train data\n",
    "predictions=autoencoder.predict(X_train_NF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[0:2]   #See the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##We want to separate out fraud records and non-fraud records for later use\n",
    "f = np.hstack((X_test,y_test))\n",
    "print(f.shape)\n",
    "\n",
    "test_nf=f[f[:,29]==0]\n",
    "print('Test_nf',test_nf.shape)\n",
    "\n",
    "test_f=f[f[:,29]==1]\n",
    "print('Test_f',test_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the errors from the non fraud data separately \n",
    "autoencoder.evaluate(test_nf[:,:29],test_nf[:,:29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Getting the errors from the fraud data separately\n",
    "autoencoder.evaluate(test_f[:,:29],test_f[:,:29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining predictions for non fraud records\n",
    "predictions_nf=autoencoder.predict(test_nf[:,:29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Obtaining predictions for fraud records\n",
    "predictions_f=autoencoder.predict(test_f[:,:29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Identifying the error computation method by autoencoder(Mean Squared Error). The computation is as follows \n",
    "np.mean(np.square(np.abs(test_nf[:,:29]-predictions_nf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing errors on the non-fraud data\n",
    "errors_nf = np.mean(np.square(np.abs(test_nf[:,:29]-predictions_nf)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_nf[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing errors on the fraud data\n",
    "errors_f = np.mean(np.square(np.abs(test_f[:,:29]-predictions_f)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_f[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing the distribution of errors in both non-fraud and fraud data\n",
    "print(np.min(errors_nf))\n",
    "print(np.max(errors_nf))\n",
    "print(np.median(errors_nf))\n",
    "\n",
    "print(np.min(errors_f))\n",
    "print(np.max(errors_f))\n",
    "print(np.median(errors_f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.24613738200955018\n",
    "#PLotting the error box plots \n",
    "\n",
    "plt.subplot(1, 2,1)\n",
    "plt.boxplot(errors_f)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.boxplot(errors_nf)\n",
    "\n",
    "fig.suptitle('Error boxplot for Fraud and nonfraud')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experimentation to fix a threshold for classification of a transaction into fraud or non-fraud\n",
    "print(sum(errors_nf>np.median(errors_f)))\n",
    "print(sum(errors_f<np.median(errors_f)))\n",
    "print(sum(errors_f<np.median(errors_nf)))\n",
    "print(sum(errors_nf>np.median(errors_nf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(errors_nf.shape)\n",
    "print(errors_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions_nf.shape)\n",
    "print(predictions_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = autoencoder.predict(X_test[:,:29])\n",
    "test_recon  = (((test_pred-X_test)**2).mean(-1))\n",
    "\n",
    "train_pred = autoencoder.predict(X_train_NF[:,:29])\n",
    "mean_recon = (((train_pred - X_train_NF)**2).mean(-1).mean())\n",
    "\n",
    "from sklearn.metrics import precision_score,recall_score,f1_score,confusion_matrix\n",
    "\n",
    "scores_f1 = []\n",
    "thres = []\n",
    "\n",
    "th = 0\n",
    "for i in range(100):\n",
    "    th+=0.1\n",
    "    fraud = (test_recon>mean_recon+th)\n",
    "    scores_f1.append(f1_score(y_test,fraud))\n",
    "    thres.append(th+mean_recon)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(thres, scores_f1)\n",
    "\n",
    "print(thres[np.array(scores_f1).argmax()])\n",
    "\n",
    "fraud = (test_recon>thres[np.array(scores_f1).argmax()])\n",
    "\n",
    "confusion_matrix(y_test, fraud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Predicting on Valdation \n",
    "\n",
    "predictions_eval=autoencoder.predict(X_eval[:,:29])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors_eval=np.square(np.subtract(predictions_eval,X_eval[:,:29]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold=0.1718219230420997\n",
    "\n",
    "fraud_eval=(((errors_eval-X_eval[:,:29])**2).mean(-1))>threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_true=X_eval[:,29],y_pred=fraud_eval,labels=[1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining errors on individual attributes on non-fraud data\n",
    "ind_errors_nf=np.abs(np.subtract(predictions_nf,test_nf[:,:29]))\n",
    "print(ind_errors_nf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Obtaining errors on individual attributes on fraud data\n",
    "ind_errors_f=np.abs(np.subtract(predictions_f,test_f[:,:29]))\n",
    "print(ind_errors_f.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combining all fraud and non-fraud data errors\n",
    "X=np.concatenate((ind_errors_f,ind_errors_nf),axis=0)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a target column\n",
    "x=np.array([1,0])\n",
    "Y=np.repeat(x,[788,407])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Expanding the shape of the target for merging with all other attributes\n",
    "print(Y.shape)\n",
    "Y = np.expand_dims(Y, axis=1)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenating all the attribute errors with target class\n",
    "X=np.concatenate((X,Y),axis=1)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map(str,range(X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a data frame and naming the columns\n",
    "X=pd.DataFrame(X)\n",
    "X.columns = map(str, range(X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Melting the data for box plot visualization\n",
    "M = pd.melt(X, \"29\", var_name=\"var\", value_name=\"value\")\n",
    "M['29'].astype('category')\n",
    "M.tail()\n",
    "#M.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "size=(20,8)\n",
    "fig,ax = plt.subplots(figsize=size)\n",
    "# the size of A4 paper\n",
    "g=sns.factorplot(x=\"var\", y='value', hue=\"29\", data=M, kind='box',ax=ax)\n",
    "ax.set_yscale(value=\"linear\")\n",
    "ax.set_ybound(0,3)\n",
    "ax.legend(fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_nf=ind_errors_nf[:,[0,1,11,17,18,4,8,14]]\n",
    "np.mean(pred_nf,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_f=ind_errors_f[:,[0,1,11,17,18,4,8,14]]\n",
    "np.mean(pred_f,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Percentile distribution of absolute errors\n",
    "percentile_nf=np.percentile(ind_errors_nf[:,[0,1,11,17,18,4,8,14]],[10,20,30,40,50,60,70,80,90,100],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_nf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_nf[:,7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_f=np.percentile(ind_errors_f[:,[0,1,11,17,18,4,8,14]],[10,20,30,],axis=0)\n",
    "np.percentile(ind_errors_f[:,[0,1,11,17,18,4,8,14]],[10,20,30,40,50,60,70,80,90,100],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentile_f[:,7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set Threshold**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thr= 0.10003411   #0.50795386       #0.32079228\n",
    "\n",
    "test_pred = autoencoder.predict(X_test)\n",
    "test_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_error= np.abs(np.subtract(test_pred,X_test[:,:29]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_error.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score,recall_score,accuracy_score,confusion_matrix\n",
    "fraud = (recon_error[:,1]>thr)\n",
    "print(\"Recall=\",recall_score(y_test,fraud))\n",
    "print(\"Precision=\",precision_score(y_test,fraud))\n",
    "print(\"Accuracy=\",accuracy_score(y_test,fraud))\n",
    "print(\"F1-Score\",f1_score(y_test,fraud))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_true=y_test,y_pred=fraud,labels=[1,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluaton**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_eval=autoencoder.predict(X_eval[:,:29])\n",
    "predictions_eval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_errors_eval=np.abs(np.subtract(predictions_eval,X_eval[:,:29]))\n",
    "ind_errors_eval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_eval=ind_errors_eval[:,1]>thr\n",
    "print(\"Recall\",recall_score(X_eval[:,29],fraud_eval))\n",
    "print(\"Precision\",precision_score(X_eval[:,29],fraud_eval))\n",
    "print(\"Accuracy\",accuracy_score(X_eval[:,29],fraud_eval))\n",
    "print(\"F1-Score\",f1_score(X_eval[:,29],fraud_eval))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_true=X_eval[:,29],y_pred=fraud_eval,labels=[1,0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_eval[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's predict on unseen data\n",
    "**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_unseen=autoencoder.predict(test_pca[:,:29])\n",
    "predictions_unseen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_errors_unseen=np.abs(np.subtract(predictions_unseen,test_pca[:,:29]))\n",
    "ind_errors_unseen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_unseen=ind_errors_unseen[:,1]>thr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_unseen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_unseen[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluate results with the best performing model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Recall\",recall_score((log.predict_proba(X_teststd)[:,1]>0.60).astype(bool),fraud_unseen))\n",
    "print(\"Precision\",precision_score((log.predict_proba(X_teststd)[:,1]>0.60).astype(bool),fraud_unseen))\n",
    "print(\"Accuracy\",accuracy_score((log.predict_proba(X_teststd)[:,1]>0.60).astype(bool),fraud_unseen))\n",
    "print(\"F1-Score\",f1_score((log.predict_proba(X_teststd)[:,1]>0.60).astype(bool),fraud_unseen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_unseen=pd.DataFrame(fraud_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraud_unseen.shape\n",
    "fraud_unseen.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "Replacement = {1:'Yes',0:'No'}\n",
    "\n",
    "AE_Labels=fraud_unseen[0].apply(lambda x : Replacement[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AE_Labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "submission_AutoEncoder=pd.DataFrame({\"Provider\":Test_category_removed_groupedbyProv_PF.Provider})\n",
    "submission_AutoEncoder['PotentialFraud']=AE_Labels\n",
    "submission_AutoEncoder.head(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_AutoEncoder.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Autoencoder with 2 hidden layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(train_pca)  #Again use train \n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df.isnull().values.any()   #check null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check distruibution of classes\n",
    "count_classes = pd.value_counts(df.iloc[:,29], sort = True)\n",
    "count_classes.plot(kind = 'bar', rot=0)\n",
    "plt.title(\"Transaction class distribution\")\n",
    "plt.xticks(range(2), LABELS)\n",
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "frauds = df[df.iloc[:,29] == 1]\n",
    "normal = df[df.iloc[:,29] == 0]\n",
    "print('Fraud data shape:',frauds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Normal data shape:',normal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_AE, X_test_AE = train_test_split(df, test_size=0.2, random_state=RANDOM_SEED)\n",
    "X_train_AE = X_train_AE[X_train_AE.iloc[:,29] == 0]\n",
    "X_train_AE = X_train_AE.drop([29], axis=1)\n",
    "y_test_AE = X_test_AE[29]\n",
    "X_test_AE = X_test_AE.drop([29], axis=1)\n",
    "X_train_AE = X_train_AE.values\n",
    "X_test_AE = X_test_AE.values\n",
    "X_train_AE.shape\n",
    "\n",
    "###df[ df.columns[[1,3]] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Add 2 hidden layers to autoencoder\n",
    "**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AutoEncoder with 2 Hidden Layers\n",
    "\n",
    "input_dim = X_train_AE.shape[1]\n",
    "encoding_dim = 14\n",
    "\n",
    "input_layer = Input(shape=(input_dim, ))\n",
    "encoder = Dense(encoding_dim, activation=\"tanh\", \n",
    "                activity_regularizer=regularizers.l1(10e-5))(input_layer)\n",
    "encoder = Dense(int(encoding_dim / 2), activation=\"relu\")(encoder)\n",
    "decoder = Dense(int(encoding_dim / 2), activation='tanh')(encoder)\n",
    "decoder = Dense(input_dim, activation='relu')(decoder)\n",
    "autoencoder = Model(inputs=input_layer, outputs=decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_epoch = 100\n",
    "batch_size = 32\n",
    "autoencoder.compile(optimizer='adam', \n",
    "                    loss='mean_squared_error', \n",
    "                    metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath=\"model.h5\",\n",
    "                               verbose=0,\n",
    "                               save_best_only=True)\n",
    "tensorboard = TensorBoard(log_dir='./logs',     #Tensorboard \n",
    "                          histogram_freq=0,\n",
    "                          write_graph=True,\n",
    "                          write_images=True)\n",
    "history = autoencoder.fit(X_train_AE, X_train_AE,\n",
    "                    epochs=nb_epoch,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=True,\n",
    "                    validation_data=(X_test_AE, X_test_AE),\n",
    "                    verbose=1,\n",
    "                    callbacks=[checkpointer, tensorboard]).history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets Load Model\n",
    "autoencoder = load_model('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the loss and Val loss\n",
    "plt.plot(history['loss'])\n",
    "plt.plot(history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper right');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lets predict on test and evaluate\n",
    "\n",
    "predictions = autoencoder.predict(X_test_AE)\n",
    "mse = np.mean(np.power(X_test_AE - predictions, 2), axis=1)\n",
    "error_df = pd.DataFrame({'reconstruction_error': mse,\n",
    "                        'true_class': y_test_AE})\n",
    "error_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reconstruction error for Normal data\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "normal_error_df = error_df[(error_df['true_class']== 0) & (error_df['reconstruction_error'] < 10)]\n",
    "_ = ax.hist(normal_error_df.reconstruction_error.values, bins=10)\n",
    "\n",
    "ax.set_ylim(0,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Reconstruction Error with Fraud data\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "fraud_error_df = error_df[error_df['true_class'] == 1]\n",
    "_ = ax.hist(fraud_error_df.reconstruction_error.values, bins=10)\n",
    "ax.set_ylim(0,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n",
    "                             roc_curve, recall_score, classification_report, f1_score,\n",
    "                             precision_recall_fscore_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n",
    "                             roc_curve, recall_score, classification_report, f1_score,\n",
    "                             precision_recall_fscore_support)\n",
    "###ROC for errors and true class\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(error_df.true_class, error_df.reconstruction_error)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\n",
    "plt.legend(loc='lower right')\n",
    "plt.plot([0,1],[0,1],'r--')\n",
    "plt.xlim([-0.001, 1])\n",
    "plt.ylim([0, 1.001])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Recall Vs Precision\n",
    "\n",
    "precision, recall, th = precision_recall_curve(error_df.true_class, error_df.reconstruction_error)\n",
    "plt.plot(recall, precision, 'b', label='Precision-Recall curve')\n",
    "plt.title('Recall vs Precision')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Threshold Vs Precision \n",
    "plt.plot(th, precision[1:], 'b', label='Threshold-Precision curve')\n",
    "plt.title('Precision for different threshold values')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Precision')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Recall vs Thresold\n",
    "plt.plot(th, recall[1:], 'b', label='Threshold-Recall curve')\n",
    "plt.title('Recall for different threshold values')\n",
    "plt.xlabel('Reconstruction error')\n",
    "plt.ylabel('Recall')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold =0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = error_df.groupby('true_class')\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for name, group in groups:\n",
    "    ax.plot(group.index, group.reconstruction_error, marker='o', ms=3.5, linestyle='',\n",
    "            label= \"Fraud\" if name == 1 else \"Normal\")\n",
    "ax.hlines(threshold, ax.get_xlim()[0], ax.get_xlim()[1], colors=\"r\", zorder=100, label='Threshold')\n",
    "ax.legend()\n",
    "plt.title(\"Reconstruction error for different classes\")\n",
    "plt.ylabel(\"Reconstruction error\")\n",
    "plt.xlabel(\"Data point index\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict class based on error threshold\n",
    "\n",
    "y_pred = [1 if e > threshold else 0 for e in error_df.reconstruction_error.values]\n",
    "conf_matrix = confusion_matrix(error_df.true_class, y_pred,labels=[0,1])\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\",cmap='Blues');\n",
    "plt.title(\"Confusion matrix\")\n",
    "plt.ylabel('True class')\n",
    "plt.xlabel('Predicted class')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('F1_Score:',f1_score(error_df.true_class, y_pred))\n",
    "print('F1_Score:',f1_score(error_df.true_class, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm1 = confusion_matrix(error_df.true_class, y_pred,labels=[1,0])\n",
    "print('Confusion Matrix Val: \\n', cm1)\n",
    "\n",
    "total1=sum(sum(cm1))\n",
    "#####from confusion matrix calculate accuracy\n",
    "\n",
    "accuracy1=(cm1[0,0]+cm1[1,1])/total1\n",
    "print ('Accuracy Val: ', accuracy1)\n",
    "\n",
    "\n",
    "sensitivity1 = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "print('Sensitivity Val: ', sensitivity1 )\n",
    "\n",
    "\n",
    "specificity1 = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "print('Specificity Val: ', specificity1)\n",
    "\n",
    "KappaValue=cohen_kappa_score(error_df.true_class, y_pred)\n",
    "print(\"Kappa Value :\",KappaValue)\n",
    "AUC=roc_auc_score(error_df.true_class, y_pred)\n",
    "\n",
    "print(\"AUC         :\",AUC)\n",
    "\n",
    "print(\"F1-Score Val  : \",f1_score(error_df.true_class, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**With just 2 layers and 100 epochs we achieved F1 score 0.57.**\n",
    "\n",
    "**Our model seems to catch a lot of the fraudulent cases. The number of normal transactions classified as frauds is really high. Based on business decision ,one can set threshold to create a tradeoff between Fraud and Non Fraud class predictions. Adding More data time to time and training will improve the performance of detection of new fraud patterns and help us to understand Providers fradulent behaviour.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary of Project**\n",
    "\n",
    "**In this Project,I have used Supervised and Unsupervised machine learning algorithms to classify Fradulent behaviour of Healthcare providers.For the purpose of classifying providers in Fraud and Non Fraud category I used following methods:-\n",
    "\n",
    "1) Feature Engineering\n",
    "\n",
    "Medicare fraud is categorised as organized crime which involves peers working together to create fraud transactions of claims.Adding features from grouping them helped in improving accuracy of prediction and fraud pattern recognition. Grouping and aggregating numeric features to provider level helped in detecting behaviour of their transactions overall.\n",
    "\n",
    "2) Logistic Regression Classifier\n",
    "\n",
    "Features derived from above step are trained using logistic regression and evaluated.My decision of choosing LR is to check linear behaviour between dependent and independent variables.Also Logistic model adds explicability to the predictions. Performance of the LR model showcase the linearity between variables.\n",
    "\n",
    "3) Random Forest Classifier\n",
    "\n",
    "One of benefits of Random forest which excites most is, the power of handle large data set with higher dimensionality. It can handle thousands of input variables and identify most significant variables. Further, the model outputs Importance of variable, which can be a very handy feature.It also checks for non linearity between variables.\n",
    "\n",
    "4) Autoencoders\n",
    "\n",
    "Autoencoders are neural networks that aims to copy their inputs to their outputs. They work by compressing the input into a latent-space representation, and then reconstructing the output from this representation. My aim for the project is to train non fraud data using autoeencoder and reconstructing it back.While reconstructing Faud data it will create an error,called as reconstruction error.Based on the threshold setting of reconstruction errors, we can easily predict Fraudulent behaviour of healthcare provider.\n",
    "\n",
    "Conclusion\n",
    "Important Features\n",
    "In this Project ,I studied behaviour of Providers and found following important features impactful in predicting Fraud/NonFraud are folowing:\n",
    "\n",
    "1) PerProviderAvg_InscClaimAmtReimbursed( Importance: 8%)\n",
    "\n",
    "2) InscClaimAmtReimbursed (Importance: 7%)\n",
    "\n",
    "3) PerAttendingPhysicianAvg_InscClaimAmtReimbursed (Importance: 7%)\n",
    "\n",
    "4) PerOperatingPhysicianAvg_InscClaimAmtReimbursed (Importance: 6%)\n",
    "\n",
    "5) PerClmAdmitDiagnosisCodeAvg_InscClaimAmtReimbursed (Importance: 4%)\n",
    "\n",
    "Model Performance\n",
    "Based on business requirement,Threshold can be set on prediction probabilities.This threshold can be varied for different performance of these models.Recall and Precision tradeoff is entirely based on business decision.\n",
    "\n",
    "Our models consistentently performed with ~0.90 Accuracy, ~0.80 AUROC score and ~0.55 Kappa Score.\n",
    "\n",
    "**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Improvement**\n",
    "**Based on the above model performance ,there is a scope in improving model performance by :\n",
    "\n",
    "1) Adding more fraud data to the training dataset help in predicting unseen fraudulent behaviour time to time.\n",
    "\n",
    "2) Ensembling methods with parameter tuning can improve performance of the models.\n",
    "\n",
    "3) Vectorizing Medical codes(ICD 9 codes) with Count Vectoriser may add performance imporvement.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Business Recommendation and Improvement**\n",
    "\n",
    "\n",
    "\n",
    "**1) Above model will help in predicting Provider fraud ,which will be helpful for insurance companies to scrutinize claims thoroughly.\n",
    "\n",
    "2) Further improvement in the project will help Government to take decision against fradulent health providers and will help in ammending rules and regulations in this domain.\n",
    "\n",
    "3) Improvement in the model will help in detecting networks of fraud Physicians,Providers and Beneficiaries.\n",
    "\n",
    "4) This type of project will help in improving health of economy by reducing inflation caused by fraud peers and lowering down insurance premiums which will certainly not cause health to become costly affair.\n",
    "\n",
    "The End**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
