{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1YwMUyt9LHG1"
   },
   "source": [
    "# Structured Dreaming - Styledream notebook\n",
    "The Styledream notebook is a notebook for finetuning Stylegan2 models with CLIP.\n",
    "\n",
    "Disclaimer: The underlying repository StructuredDreaming https://github.com/ekgren/StructuredDreaming will continually undergo changes that might break copies of this notebook.  \n",
    "\n",
    "Author: Ariel Ekgren  \n",
    "https://github.com/ekgren  \n",
    "https://twitter.com/ArYoMo  \n",
    "\n",
    "Resources:  \n",
    "CLIP https://github.com/openai/CLIP  \n",
    "Stylegan2 ADA https://github.com/NVlabs/stylegan2-ada-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qZ3rNuAWAewx",
    "outputId": "6cc57a89-47a8-4e36-8c23-ed54ce3cbfe8"
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-_UVMZCIAq_r",
    "outputId": "156c56f0-7461-46e0-ccfb-1303e164ed9b"
   },
   "outputs": [],
   "source": [
    "!pip install ftfy regex tqdm pyspng ninja imageio-ffmpeg==0.4.3\n",
    "!git clone https://github.com/ekgren/StructuredDreaming.git\n",
    "!pip install -e ./StructuredDreaming\n",
    "\n",
    "!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git\n",
    "import sys\n",
    "# insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, 'stylegan2-ada-pytorch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q1j56lCZJ4lV",
    "outputId": "a9c661d1-c538-4fc8-eb92-e613e86e3f12"
   },
   "outputs": [],
   "source": [
    "!git -C ./StructuredDreaming/ pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JmbrcrhpBPC6"
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import PIL\n",
    "from matplotlib import pyplot as pl\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# StructuredDreaming imports\n",
    "from StructuredDreaming import structure\n",
    "from StructuredDreaming.structure import clip\n",
    "from StructuredDreaming.structure import sample\n",
    "from StructuredDreaming.structure import optim\n",
    "\n",
    "# Stylegan imports\n",
    "import dnnlib\n",
    "import legacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VnQjGugaDZPJ"
   },
   "outputs": [],
   "source": [
    "# Load models\n",
    "perceptor, normalize_image = structure.clip.load('ViT-B/16', jit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 57
    },
    "id": "iu44wy8u-O-x",
    "outputId": "6d03647f-566e-4e80-c46b-cad29e38cc6b"
   },
   "outputs": [],
   "source": [
    "# Utils\n",
    "def display_img(input: torch.Tensor, size: float = 1.):\n",
    "    \"\"\" Assumes tensor values in the range [0, 1] \"\"\"\n",
    "    with torch.no_grad():\n",
    "        batch_size, num_channels, height, width = input.shape\n",
    "        img = torch.nn.functional.interpolate(input, (int(size*height), int(size*width)), mode='area')\n",
    "        img_show = img.cpu()[0].transpose(0, 1).transpose(1, 2)\n",
    "        img_out = (img_show * 255).clamp(0, 255).to(torch.uint8)\n",
    "        display(PIL.Image.fromarray(img_out.cpu().numpy(), 'RGB'))\n",
    "        pl.show()\n",
    "\n",
    "def stylegan_to_rgb(input: torch.Tensor) -> torch.Tensor:\n",
    "    return (input * 127.5 + 128) / 255\n",
    "\n",
    "display_img(torch.rand(1, 3, 10, 10, requires_grad=False), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XYNwvEy3HH49"
   },
   "outputs": [],
   "source": [
    "#@title # Prompt and training parameters{ run: \"auto\" }\n",
    "#@markdown Write your image prompt in the txt field below.\n",
    "\n",
    "#@markdown Prompt suggestions:\n",
    "#@markdown * \"portrait painting of android from dystopic future by James Gurney\"\n",
    "#@markdown * \"portrait of anime character in the style of studio ghibli | cute anime character\"\n",
    "\n",
    "txt = \"eternal alien #film #eternity | trending on artstation | art\" #@param {type:\"string\"}\n",
    "\n",
    "# Training parameters\n",
    "iterations = 400\n",
    "grad_acc_steps = 1\n",
    "batch_size = 1\n",
    "lr = 2e-4\n",
    "loss_scale = 100.\n",
    "steps_show = 8\n",
    "truncation_psi = 0.6\n",
    "clamp_val = 1e-30\n",
    "drop = 0.8\n",
    "\n",
    "# Sampler\n",
    "sample_size = 224\n",
    "kernel_min = 1\n",
    "kernel_max = 16\n",
    "grid_size_min = 224\n",
    "grid_size_max = 3*224\n",
    "noise = 1.\n",
    "noise_std = 0.3\n",
    "cutout = 1.\n",
    "cutout_size = 0.25\n",
    "\n",
    "network_pkl = 'https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 546
    },
    "id": "xXjFDyvt0Rn4",
    "outputId": "ea8434ad-6f2a-4e2d-ebc0-a265518e0a2c"
   },
   "outputs": [],
   "source": [
    "#@title Train loop {vertical-output: true}\n",
    "#@markdown Loading and fine-tuning the model.\n",
    "#@markdown The image shown during training is displayed at half size.\n",
    "\n",
    "device = torch.device('cuda')\n",
    "with dnnlib.util.open_url(network_pkl) as f:\n",
    "    G = legacy.load_network_pkl(f)['G_ema'].to(device) # type: ignore\n",
    "for p in G.parameters():\n",
    "    p.requires_grad = True\n",
    "c = None\n",
    "\n",
    "# Training\n",
    "txt_tok = structure.clip.tokenize(txt)\n",
    "text_latent = perceptor.encode_text(txt_tok.to(device)).detach()\n",
    "sampler = torch.jit.script(\n",
    "              structure.sample.ImgSampleStylegan(kernel_min=kernel_min,\n",
    "                                                 kernel_max=kernel_max,\n",
    "                                                 grid_size_min=grid_size_min,\n",
    "                                                 grid_size_max=grid_size_max,\n",
    "                                                 noise=noise,\n",
    "                                                 noise_std=noise_std,\n",
    "                                                 cutout=cutout,\n",
    "                                                 cutout_size=cutout_size).to(device)\n",
    "          )\n",
    "optimizer = structure.optim.ClampSGD(G.parameters(),\n",
    "                                     lr=lr, \n",
    "                                     clamp=clamp_val,\n",
    "                                     drop=drop)\n",
    "\n",
    "print('Generating image.')\n",
    "for i in range(iterations):\n",
    "\n",
    "    if (i + 1) % steps_show == 0:\n",
    "        with torch.no_grad():\n",
    "            clear_output(True)\n",
    "            z = torch.randn([1, G.z_dim], device=device)          \n",
    "            img = G(z, c, truncation_psi)\n",
    "            img = stylegan_to_rgb(img)\n",
    "            display_img(img, 0.5)\n",
    "            print(i, \n",
    "                  loss.item(),\n",
    "                  img.min().item(), \n",
    "                  img.max().item(),) \n",
    "    \n",
    "    for j in range(grad_acc_steps):\n",
    "        optimizer.zero_grad()\n",
    "        z = torch.randn([1, G.z_dim], device=device)\n",
    "        img = G(z, c, truncation_psi)\n",
    "        img = stylegan_to_rgb(img)\n",
    "        img = sampler(img, size=sample_size, bs=batch_size)\n",
    "        img = normalize_image(img)\n",
    "        img_latents = perceptor.encode_image(img)\n",
    "        loss = torch.cosine_similarity(text_latent, img_latents, dim=-1).mean().neg() * loss_scale\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "uPNzZ9yf7U3T",
    "outputId": "176e7893-a3c4-4674-ac61-63412e915681"
   },
   "outputs": [],
   "source": [
    "#@title Generate images from the fine-tuned model\n",
    "\n",
    "with torch.no_grad():\n",
    "    clear_output(True)\n",
    "    z = torch.randn([1, G.z_dim], device=device)\n",
    "    img = G(z, c, truncation_psi)\n",
    "    img = stylegan_to_rgb(img)\n",
    "    display_img(img, 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T31ZUpdDm1lO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Structured Dreaming - Styledreams.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
