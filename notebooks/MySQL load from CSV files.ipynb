{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a83ad631-4e39-45e9-a995-48f997c64f04",
   "metadata": {},
   "source": [
    "# Load CSV to MySQL/MariaDB or Spark SQL using PySpark\n",
    "\n",
    "It is very useful in the data exploration or descriptive analytics phase of a project to be able to query your CSV files more or less directly using the power of SQL.\n",
    "\n",
    "Spark makes this very simple by creating tables in Hive that reference the CSVs. In addition to SQL, this gives us the additional capabilities of the Spark and Pandas data frames.\n",
    "\n",
    "Spark then allows us to create permanent tables in Hive using the very efficient Parquet file format. \n",
    "\n",
    "If we then want to store to a database outside of the Spark environment, we can save those dataframes to MySQL/MariaDB or other JDBC compliant databases.\n",
    "\n",
    "## Prep the Spark server with the required JDBC driver `.jar` file\n",
    "\n",
    "On Databricks, this can be done in the UI: Compute -> cluster -> Libraries\n",
    "\n",
    "## Prep the target MySQL/MariaDB server\n",
    "Create the database you want to use beforehand.  This code will create tables, but not the database.\n",
    "\n",
    "On local Spark, the `.jar` can be placed in `$SPARK_HOME/jars/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d35d24e-0751-4fff-adac-f45a5e800b61",
   "metadata": {},
   "source": [
    "## Import libraries and configure secrets\n",
    "\n",
    "In this example, we are using the `configparser` library to read a simple `.ini` style file named `CREDENTIALS.config`.  If you choose to use this method, create a section in the file like this:\n",
    "```\n",
    "[csvload]\n",
    "sqluser = <myuser>\n",
    "sqlpassword = <mypass>\n",
    "sqlhost = <host or ip>\n",
    "sqlport = <3306 or custom port>\n",
    "sqldb = <database name>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be89b892-bf1e-4400-86a7-5e1bac624c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "import pandas as pd\n",
    "from configparser import ConfigParser\n",
    "\n",
    "import os\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /usr/share/java/mariadb-java-client.jar pyspark-shell'\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4c74ad-74fa-44be-8ef6-37cf44714edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !env|grep SPARK\n",
    "# !cp -v /usr/share/java/mariadb-java-client.jar $SPARK_HOME/jars/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f83b592-5657-42eb-8972-3661ec160d4a",
   "metadata": {},
   "source": [
    "# Establish our Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4ca7f8-acef-4f0c-bcb1-1b0a3ae8ea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder.config(\"spark.jars\", \"/usr/share/java/mariadb-java-client.jar\").appName(\"TeslafiLoad\").getOrCreate()\n",
    "spark = SparkSession.builder.appName(\"CSVLoad\").getOrCreate()\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9347aa-c6ce-44a3-a955-26f7ee937c5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !echo $VIRTUAL_ENV\n",
    "# !env|grep SPARK\n",
    "!echo $SPARK_HOME\n",
    "# !cp -v  /usr/share/java/mariadb-java-client.jar $SPARK_HOME/jars/\n",
    "# !ls $SPARK_HOME/jars\n",
    "# # !pip install pandas findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e40462-5937-4cfa-903f-91204cf096a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.listdir(path='/data/data-files/teslafi/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace7116c-9f6a-4104-a876-7cc035d8c9ec",
   "metadata": {},
   "source": [
    "# Optional - create a Spark/Hive temporary table from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b841b0fe-f98b-484a-957f-40bfddc264c1",
   "metadata": {},
   "source": [
    "# Write CSVs to MySQL/MariaDB\n",
    "\n",
    "\n",
    "## Set all the parameters\n",
    "For privacy, you may prefer to load `csvdir` from `CREDENTIALS.config`.  \n",
    "\n",
    "The same goes for all the parameters.  Default practice is load everything from the credentials file, \n",
    "then override one or two variables as long as the values would not cause problems if checked in to a public repo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a84814-6ebe-4183-a32b-1bf1350a2d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File location and type\n",
    "\n",
    "file_type = \"csv\"\n",
    "# CSV options\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# MySQL/MariaDB info\n",
    "# Get credentials from file\n",
    "config_section = 'erebor-mariadb'\n",
    "parser = ConfigParser()\n",
    "_ = parser.read('CREDENTIALS.config')\n",
    "\n",
    "# csvdir = parser.get(config_section, 'csvdir')\n",
    "csvdir = '/data/data-files/teslafi/'\n",
    "user = parser.get(config_section, 'sqluser')\n",
    "password = parser.get(config_section, 'sqlpassword')\n",
    "host = parser.get(config_section, 'sqlhost')\n",
    "port = parser.get(config_section, 'sqlport')\n",
    "# db = parser.get(config_section, 'sqldb')\n",
    "db = 'teslafi'\n",
    "sqlurl = 'jdbc:mysql://' + host + ':' + port + '/' + db\n",
    "\n",
    "# csvdir = '/data/graph-data/AntiFraud/data/'\n",
    "# csvdir = '/data/data-files/teslafi/'\n",
    "# csvdir = '/data/graph-data/ldbc-workshop/data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cd9adc-5fc1-4089-aaa9-ae89d231f7fd",
   "metadata": {},
   "source": [
    "## Write the CSVs to the database\n",
    "Database must be created beforehand, but not the tables\n",
    "\n",
    "Modify the rules paying special attention to the `timestamp` columns in the dataframe.  `TIMESTAMP` column type in MySQL cannot represent dates prior to 1980.  If needed, transfer those dataframe columes to `date` prior to creating the MySQL table with `df.write()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d029085-38f8-4efd-b7e6-0bb7c484b2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for path,name in [(f.path,f.name) for f in dbutils.fs.ls(csvdir) if f.path.endswith('.csv')  ]:\n",
    "for fname in os.listdir(path=csvdir):\n",
    "    if fname.endswith('.csv') and fname.startswith('teslafi-202') :\n",
    "        print(fname)\n",
    "        # if not fname in ['ff_party.csv', 'ff_company.csv']:\n",
    "        #     print('skipping ' + fname)\n",
    "        #     continue\n",
    "        df = spark.read.format('csv') \\\n",
    "        .option(\"inferSchema\", infer_schema) \\\n",
    "        .option(\"header\", first_row_is_header) \\\n",
    "        .option(\"sep\", delimiter) \\\n",
    "        .option(\"na_values\", [\"None\", None]) \\\n",
    "        .load(csvdir + '/' + fname)\n",
    "        # timestamp type in MySQL represents dates only after 1980\n",
    "        # if timestamp is desired, comment or modify the following code\n",
    "        # this code changes *all* timestamp columns to Date type\n",
    "        for a,b in df.dtypes:\n",
    "            if b == \"timestamp\" and a.find(\"timestamp\") < 0: \n",
    "                newdf = df.withColumn(a, df[a].cast(DateType()))\n",
    "                print('   CHANGED column ' + a + ' to Date type')\n",
    "                df = newdf\n",
    "            if '.' in a:\n",
    "                df.withColumnRenamed(a, a.replace(',', '_'))\n",
    "        # Modify filename-to-tablename as needed\n",
    "        df.write.format('jdbc').options(\n",
    "            url=sqlurl,\n",
    "            driver='org.mariadb.jdbc.Driver',\n",
    "            # dbtable=fname.replace('.csv','').replace('-', '_').replace('c360_','').replace('ce_',''),\n",
    "            dbtable='teslafi',\n",
    "            user=user,\n",
    "            password=password).mode('append').save()\n",
    "            # password=password).mode('overwrite').save()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e064959-32ea-4c45-a370-c70521dc8a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4430aa3c-337b-425c-ae62-a76654aa2f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# p = df.toPandas()\n",
    "p['is_auto_conditioning_on'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9193cef6-449d-46ff-88a7-367fe54538ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "type([\"hi\", \"there\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bf3d6a-5246-4a45-aca3-2f1adc9a6896",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
