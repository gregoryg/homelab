{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a83ad631-4e39-45e9-a995-48f997c64f04",
   "metadata": {},
   "source": [
    "# Load teslafi to MySQL/MariaDB or Spark SQL using PySpark\n",
    "\n",
    "It is very useful in the data exploration or descriptive analytics phase of a project to be able to query your CSV files more or less directly using the power of SQL.\n",
    "\n",
    "Spark makes this very simple by creating tables in Hive that reference the CSVs. In addition to SQL, this gives us the additional capabilities of the Spark and Pandas data frames.\n",
    "\n",
    "Spark then allows us to create permanent tables in Hive using the very efficient Parquet file format. \n",
    "\n",
    "If we then want to store to a database outside of the Spark environment, we can save those dataframes to MySQL/MariaDB or other JDBC compliant databases.\n",
    "\n",
    "## Prep the Spark server with the required JDBC driver `.jar` file\n",
    "\n",
    "On Databricks, this can be done in the UI: Compute -> cluster -> Libraries\n",
    "\n",
    "## Prep the target MySQL/MariaDB server\n",
    "Create the database you want to use beforehand.  This code will create tables, but not the database.\n",
    "\n",
    "On local Spark, the `.jar` can be placed in `$SPARK_HOME/jars/`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d35d24e-0751-4fff-adac-f45a5e800b61",
   "metadata": {},
   "source": [
    "## Import libraries and configure secrets\n",
    "\n",
    "In this example, we are using the `configparser` library to read a simple `.ini` style file named `CREDENTIALS.config`.  If you choose to use this method, create a section in the file like this:\n",
    "```\n",
    "[csvload]\n",
    "sqluser = <myuser>\n",
    "sqlpassword = <mypass>\n",
    "sqlhost = <host or ip>\n",
    "sqlport = <3306 or custom port>\n",
    "sqldb = <database name>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be89b892-bf1e-4400-86a7-5e1bac624c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark.sql.types import DateType\n",
    "\n",
    "import pandas as pd\n",
    "from configparser import ConfigParser\n",
    "\n",
    "import os\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--jars /usr/share/java/mariadb-java-client.jar pyspark-shell'\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d4c74ad-74fa-44be-8ef6-37cf44714edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !env|grep SPARK\n",
    "# !cp -v /usr/share/java/mariadb-java-client.jar $SPARK_HOME/jars/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f83b592-5657-42eb-8972-3661ec160d4a",
   "metadata": {},
   "source": [
    "# Establish our Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4ca7f8-acef-4f0c-bcb1-1b0a3ae8ea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark = SparkSession.builder.config(\"spark.jars\", \"/usr/share/java/mariadb-java-client.jar\").appName(\"TeslafiLoad\").getOrCreate()\n",
    "spark = SparkSession.builder.appName(\"CSVLoad\").getOrCreate()\n",
    "sc = spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9347aa-c6ce-44a3-a955-26f7ee937c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !echo $VIRTUAL_ENV\n",
    "# !env|grep SPARK\n",
    "!echo $SPARK_HOME\n",
    "# !cp -v  /usr/share/java/mariadb-java-client.jar $SPARK_HOME/jars/\n",
    "!ls $SPARK_HOME/jars\n",
    "# # !pip install pandas findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e40462-5937-4cfa-903f-91204cf096a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.listdir(path='/data/data-files/teslafi/')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace7116c-9f6a-4104-a876-7cc035d8c9ec",
   "metadata": {},
   "source": [
    "# Optional - create a Spark/Hive temporary table from the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b841b0fe-f98b-484a-957f-40bfddc264c1",
   "metadata": {},
   "source": [
    "# Write CSVs to MySQL/MariaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181e7779-414e-4a46-8ab1-2421e05b3cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File location and type\n",
    "csvdir = '/data/graph-data/AntiFraud/data/'\n",
    "# csvdir = '/data/data-files/teslafi/'\n",
    "\n",
    "file_type = \"csv\"\n",
    "# CSV options\n",
    "infer_schema = \"true\"\n",
    "first_row_is_header = \"true\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# MySQL/MariaDB info\n",
    "# Get credentials from file\n",
    "config_section = 'csvload'\n",
    "parser = ConfigParser()\n",
    "_ = parser.read('CREDENTIALS.config')\n",
    "\n",
    "\n",
    "user = parser.get(config_section, 'sqluser')\n",
    "password = parser.get(config_section, 'sqlpassword')\n",
    "host = parser.get(config_section, 'sqlhost')\n",
    "port = parser.get(config_section, 'sqlport')\n",
    "db = parser.get(config_section, 'sqldb')\n",
    "sqlurl = 'jdbc:mysql://' + host + ':' + port + '/' + db\n",
    "\n",
    "# # for path,name in [(f.path,f.name) for f in dbutils.fs.ls(csvdir) if f.path.endswith('.csv')  ]:\n",
    "for fname in os.listdir(path=csvdir):\n",
    "    if fname.endswith('.csv'):\n",
    "        print(fname)\n",
    "        # if not fname in ['ff_party.csv', 'ff_company.csv']:\n",
    "        #     print('skipping ' + fname)\n",
    "        #     continue\n",
    "        df = spark.read.format('csv') \\\n",
    "        .option(\"inferSchema\", infer_schema) \\\n",
    "        .option(\"header\", first_row_is_header) \\\n",
    "        .option(\"sep\", delimiter) \\\n",
    "        .load(csvdir + '/' + fname)\n",
    "        # for a,b in df.dtypes:\n",
    "            # if b == 'timestamp': \n",
    "            #     newdf = df.withColumn(a, df[a].cast(DateType()))\n",
    "            #     print('   CHANGED column ' + a + ' to Date type')\n",
    "            #     df = newdf\n",
    "\n",
    "        df.write.format('jdbc').options(\n",
    "            url=sqlurl,\n",
    "            driver='org.mariadb.jdbc.Driver',\n",
    "            dbtable=fname.replace('.csv',''),\n",
    "            user=user,\n",
    "            password=password).mode('append').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e064959-32ea-4c45-a370-c70521dc8a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4430aa3c-337b-425c-ae62-a76654aa2f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2282452f-7b44-4fcb-bd76-249c99797c85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/22 07:40:08 WARN HeartbeatReceiver: Removing executor driver with no recent heartbeats: 32872527 ms exceeds timeout 120000 ms\n",
      "22/07/22 07:40:08 WARN SparkContext: Killing executors is not supported by current scheduler.\n"
     ]
    }
   ],
   "source": [
    "p['polling'].unique()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
