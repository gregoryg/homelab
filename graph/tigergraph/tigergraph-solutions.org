#+options: ':nil *:t -:t ::t <:t H:3 \n:nil ^:nil arch:headline author:t broken-links:nil
#+options: c:nil creator:nil d:(not "LOGBOOK") date:t e:t email:nil f:t inline:t num:t
#+options: p:nil pri:nil prop:nil stat:t tags:t tasks:t tex:t timestamp:t title:t toc:t
#+options: todo:t |:t
#+title: TigerGraph Solutions Generator
#+date: <2021-11-30 Tue>
#+author: Gregory Grubbs
#+email: gregory.grubbs@gmail.com
#+language: en
#+select_tags: export
#+exclude_tags: noexport
#+creator: Emacs 28.0.60 (Org mode 9.5)
#+cite_export:
#+setupfile: ~/projects/emacs/org-html-themes/org/theme-bigblow-local.setup
* Overview
  This file generates all the scripts and GSQL code in the =solutions= directory.  Edits
  to those generated files will be lost whenever this Org Mode file is used for tangling
  (generating) those files.

* Principles to follow using this framework
  + Idempotent: graph will install into an existing TG cluster without wiping schema or data
  + (extra credit): generate way to translate solution export file into scripted
       nondestructive import
  + Every solution needs a README.md with a script - every query needs some examples of
    successful parameters for the data
  + Make GSQL scripts and loading jobs easily human-readable
    + e.g. replace column positions with column names
    + make import file names explicit
  + Literate programming to create all scripts - orgmode
  + Make nothing at the Global level
  + Deploy for data loading to =mydata= directory, best located directly under =DataRoot=
    directory on node =m1=.  The reason is that when a Docker or Kubernetes container is
    restarted, data anywhere else may be lost.
    + Command to find =DataRoot=
      #+begin_src bash
        gadmin config get System.DataRoot
        # Alternative using full dump of config
        gadmin config dump | jq -r '.System.DataRoot'
      #+end_src
  + Use TRAMP from this file to transfer solutions scripts and possibly grab data.

* Solutions
** Customer 360 New and Improved?
*** Resources
    + Data: /data/data-files/graph-data/cust360-capgemini/Customer360.zip or s3://gjgeksbackup/Customer360.zip
*** Create graph
    #+begin_src sql :tangle solutions/customer360/01-create-graph.gsql :mkdirp yes
      ###############################################################################
      # CUSTOMER EXPERIENCE
      ###############################################################################
      USE GRAPH Customer360
      DROP JOB customer360_load
      DROP JOB customer360_schema

      USE GLOBAL
      DROP GRAPH Customer360
      CREATE GRAPH Customer360()
      USE GRAPH Customer360

      CREATE SCHEMA_CHANGE JOB customer360_schema FOR GRAPH Customer360 {

      ######################################
      #### VERTEX - Lookups
      ######################################
        ADD VERTEX Profile    (PRIMARY_ID profile_id INT, first_name STRING, last_name STRING, user_name STRING, gender STRING, ssn STRING, birthdate DATETIME, job_desc STRING, creditcard STRING, ethnicity STRING, married STRING, income STRING, education STRING, prodCommunity INT, movieCommunity INT, bookCommunity INT, otherCommunity INT)  WITH primary_id_as_attribute="false", STATS="outdegree_by_edgetype";
        ADD VERTEX Orders     (PRIMARY_ID order_id STRING, order_source STRING, order_date DATETIME, order_type STRING, order_qty INT, order_amt DOUBLE, order_profit DOUBLE)  WITH primary_id_as_attribute="false", STATS="outdegree_by_edgetype";
        ADD VERTEX Products   (PRIMARY_ID item_id STRING, item_desc STRING, department STRING, category STRING, productClass STRING, productBrand STRING, item_price DOUBLE, item_cost DOUBLE) WITH primary_id_as_attribute="false", STATS="outdegree_by_edgetype";
        ADD VERTEX Sessions   (PRIMARY_ID session_id STRING, session_timestamp DATETIME, hostname STRING, referer STRING, session_time INT, ip_address STRING, event_cnt INT, view_cnt INT, click_cnt INT) WITH primary_id_as_attribute="false", STATS="outdegree_by_edgetype";
        ADD VERTEX Events     (PRIMARY_ID event_id STRING, event_timestamp DATETIME, event_type STRING, event_time INT, event_order INT, page_url STRING, prev_page_url STRING) WITH primary_id_as_attribute="false", STATS="outdegree_by_edgetype";
        ADD VERTEX Movies     (PRIMARY_ID movie_id STRING, movieTitle STRING, movieYear INT)  WITH primary_id_as_attribute="false", STATS="outdegree_by_edgetype";
        ADD VERTEX Genre      (PRIMARY_ID genre STRING)  WITH primary_id_as_attribute="true", STATS="outdegree_by_edgetype";
        ADD VERTEX Books      (PRIMARY_ID book_id STRING, bookTitle STRING, bookYear INT, bookImage STRING, bookISBN STRING)  WITH primary_id_as_attribute="false", STATS="outdegree_by_edgetype";
        ADD VERTEX Publisher  (PRIMARY_ID publisher STRING)  WITH primary_id_as_attribute="true", STATS="outdegree_by_edgetype";
        ADD VERTEX Author     (PRIMARY_ID author STRING)  WITH primary_id_as_attribute="true", STATS="outdegree_by_edgetype";
        ADD VERTEX Campaign   (PRIMARY_ID campaign_id INT, campaign_year INT, campaign_name STRING, campaign_type STRING, start_date DATETIME, end_date DATETIME)  WITH primary_id_as_attribute="false", STATS="outdegree_by_edgetype";
        ADD VERTEX Promotions (PRIMARY_ID promotion_id STRING, promotion_name STRING, promotion_channel STRING)  WITH primary_id_as_attribute="false", STATS="outdegree_by_edgetype";
        ADD VERTEX Ads        (PRIMARY_ID ad_id STRING, ad_name STRING, ad_channel STRING) WITH primary_id_as_attribute="false", STATS="outdegree_by_edgetype";
        ADD VERTEX Location   (PRIMARY_ID zipcode STRING, locationCity STRING, locationState STRING, Market STRING, Region STRING, population INT, latitude DOUBLE, longitude DOUBLE)  WITH primary_id_as_attribute="false", STATS="outdegree_by_edgetype";
        ADD VERTEX Address    (PRIMARY_ID address_id INT, address_type STRING, address_line STRING, city STRING, state STRING, postalcode STRING, country STRING)  WITH primary_id_as_attribute="false", STATS="outdegree_by_edgetype";
        ADD VERTEX IpAddress  (PRIMARY_ID ip_address STRING, ip_type STRING)  WITH primary_id_as_attribute="true", STATS="outdegree_by_edgetype";
        ADD VERTEX Device     (PRIMARY_ID device STRING, device_type STRING)  WITH primary_id_as_attribute="true", STATS="outdegree_by_edgetype";
        ADD VERTEX Email      (PRIMARY_ID email_address STRING, email_type STRING, email_perm STRING, email_freq STRING)  WITH primary_id_as_attribute="true", STATS="outdegree_by_edgetype";
        ADD VERTEX Phone      (PRIMARY_ID phone_number STRING, phone_perm STRING, phone_freq INT)  WITH primary_id_as_attribute="true", STATS="outdegree_by_edgetype";
        ADD VERTEX urlPages   (PRIMARY_ID page_url STRING, page_location STRING, page_menu STRING, page_menuitem STRING)  WITH primary_id_as_attribute="true", STATS="outdegree_by_edgetype";
        ADD VERTEX UserAgent  (PRIMARY_ID ua_id INT, user_agent STRING)  WITH primary_id_as_attribute="false", STATS="outdegree_by_edgetype";
        ADD VERTEX Browser    (PRIMARY_ID browser STRING)  WITH primary_id_as_attribute="true", STATS="outdegree_by_edgetype";
        ADD VERTEX DataSource (PRIMARY_ID source_id INT, source_name STRING, source_type STRING)  WITH primary_id_as_attribute="false", STATS="outdegree_by_edgetype";
        ADD VERTEX Store      (PRIMARY_ID store_id INT, store_name STRING, store_type STRING, shiptostore STRING, autocenter STRING, salon STRING, store_zipcode STRING, store_city STRING, store_state STRING, store_market STRING, store_region STRING, store_lat DOUBLE, store_lng DOUBLE)  WITH primary_id_as_attribute="false", STATS="outdegree_by_edgetype";

        ADD UNDIRECTED EDGE campaign_ads       (FROM Campaign, TO Ads);
        ADD UNDIRECTED EDGE campaign_promotion (FROM Campaign, TO Promotions);
        ADD UNDIRECTED EDGE address_location   (FROM Address,  TO Location);
        ADD UNDIRECTED EDGE session_browser    (FROM Sessions, TO Browser);
        ADD UNDIRECTED EDGE session_useragent  (FROM Sessions, TO UserAgent);
        ADD UNDIRECTED EDGE session_ipaddr     (FROM Sessions, TO IpAddress);
        ADD UNDIRECTED EDGE session_device     (FROM Sessions, TO Device);
        ADD UNDIRECTED EDGE profile_sessions   (FROM Profile,  TO Sessions);
        ADD UNDIRECTED EDGE session_events     (FROM Sessions, TO Events);
        ADD UNDIRECTED EDGE event_ads          (FROM Events,   TO Ads);
        ADD UNDIRECTED EDGE event_url          (FROM Events,   TO urlPages);
        ADD UNDIRECTED EDGE profile_datasource (FROM Profile,  TO DataSource);
        ADD UNDIRECTED EDGE profile_address    (FROM Profile,  TO Address, addressType STRING);
        ADD UNDIRECTED EDGE profile_ipaddress  (FROM Profile,  TO IpAddress);
        ADD UNDIRECTED EDGE profile_device     (FROM Profile,  TO Device);
        ADD UNDIRECTED EDGE profile_phone      (FROM Profile,  TO Phone);
        ADD UNDIRECTED EDGE profile_email      (FROM Profile,  TO Email, emailType STRING, emailDate DATETIME);
        ADD UNDIRECTED EDGE profile_friends    (FROM Profile,  TO Profile, friend_type STRING, friend_date DATETIME);

      ###############################################################################
      # VERTICIES for the OMNI CHANNEL Group
      ###############################################################################
        ADD UNDIRECTED EDGE profile_campaign (FROM Profile,  TO Campaign, campaign_date DATETIME);
        ADD UNDIRECTED EDGE profile_response (FROM Campaign, TO Profile, response_date DATETIME, response_chnl STRING);

      ###############################################################################
      # EDGES for the PRODUCT Group
      ###############################################################################
        ADD UNDIRECTED EDGE profile_purchase (FROM Profile, TO Orders);
        ADD UNDIRECTED EDGE order_store      (FROM Orders,  TO Store);
        ADD UNDIRECTED EDGE order_items      (FROM Orders,  TO Products, itemDate DATETIME, itemQty INT, itemPrice DOUBLE, itemCost DOUBLE, itemAmt DOUBLE, itemProfit DOUBLE);
        ADD UNDIRECTED EDGE store_location   (FROM Store,   TO Location);

      ###############################################################################
      # EDGES for the MOVIE Book Group
      ###############################################################################
        ADD UNDIRECTED EDGE movie_ratings  (FROM Profile, TO Movies, movie_date DATETIME, movie_rating INT);
        ADD UNDIRECTED EDGE movie_genre    (FROM Movies,  TO Genre);
        ADD UNDIRECTED EDGE book_ratings   (FROM Profile, TO Books, book_date DATETIME, book_rating INT);
        ADD UNDIRECTED EDGE book_publisher (FROM Books,   TO Publisher);
        ADD UNDIRECTED EDGE book_author    (FROM Books,   TO Author);
        ADD UNDIRECTED EDGE same_profile   (FROM Profile, TO Profile, score FLOAT);
        ADD UNDIRECTED EDGE community_edge (FROM Profile, TO Profile, community_id FLOAT);
      }


      RUN SCHEMA_CHANGE JOB customer360_schema
      DROP JOB customer360_schema
    #+end_src
*** Load data
    #+begin_src sql :tangle solutions/customer360/02-load-data.gsql :mkdirp yes
      USE GRAPH Customer360
      DROP JOB customer360_load

      CREATE LOADING JOB customer360_load FOR GRAPH Customer360 {

        ###################################################################
        # define customer group
        ###################################################################
        DEFINE FILENAME f_solicts      = "m1:/home/tigergraph/mydata/customer360/data/c360_solicits.csv";
        DEFINE FILENAME f_respons      = "m1:/home/tigergraph/mydata/customer360/data/c360_response.csv";
        DEFINE FILENAME f_profile      = "m1:/home/tigergraph/mydata/customer360/data/c360_profile.csv";
        DEFINE FILENAME f_address      = "m1:/home/tigergraph/mydata/customer360/data/c360_address.csv";
        DEFINE FILENAME f_ipaddr       = "m1:/home/tigergraph/mydata/customer360/data/c360_ipaddress.csv";
        DEFINE FILENAME f_email        = "m1:/home/tigergraph/mydata/customer360/data/c360_emails.csv";
        DEFINE FILENAME f_device       = "m1:/home/tigergraph/mydata/customer360/data/c360_devices.csv";
        DEFINE FILENAME f_phone        = "m1:/home/tigergraph/mydata/customer360/data/c360_phones.csv";
        DEFINE FILENAME e_address      = "m1:/home/tigergraph/mydata/customer360/data/c360_profile_address.csv";
        DEFINE FILENAME e_ipaddr       = "m1:/home/tigergraph/mydata/customer360/data/c360_profile_ipaddr.csv";
        DEFINE FILENAME e_email        = "m1:/home/tigergraph/mydata/customer360/data/c360_profile_email.csv";
        DEFINE FILENAME e_device       = "m1:/home/tigergraph/mydata/customer360/data/c360_profile_device.csv";
        DEFINE FILENAME e_phone        = "m1:/home/tigergraph/mydata/customer360/data/c360_profile_phone.csv";
        DEFINE FILENAME e_links        = "m1:/home/tigergraph/mydata/customer360/data/c360_profile_links.csv";
        DEFINE FILENAME f_orders       = "m1:/home/tigergraph/mydata/customer360/data/c360_orders.csv";
        DEFINE FILENAME f_movieratings = "m1:/home/tigergraph/mydata/customer360/data/c360_movie_ratings.csv";
        DEFINE FILENAME f_bookratings  = "m1:/home/tigergraph/mydata/customer360/data/c360_book_ratings.csv";
        DEFINE FILENAME f_sessions     = "m1:/home/tigergraph/mydata/customer360/data/c360_sessions.csv";
        DEFINE FILENAME f_events       = "m1:/home/tigergraph/mydata/customer360/data/c360_events.csv";

        DEFINE FILENAME f_camp_ads     = "m1:/home/tigergraph/mydata/customer360/data/ce_camp-ads.csv";
        DEFINE FILENAME f_camp_promo   = "m1:/home/tigergraph/mydata/customer360/data/ce_camp-promos.csv";
        DEFINE FILENAME f_source       = "m1:/home/tigergraph/mydata/customer360/data/ce_datasource.csv";
        DEFINE FILENAME f_geog         = "m1:/home/tigergraph/mydata/customer360/data/ce_geog.csv";
        DEFINE FILENAME f_urls         = "m1:/home/tigergraph/mydata/customer360/data/ce_urls.csv";
        DEFINE FILENAME f_useragent    = "m1:/home/tigergraph/mydata/customer360/data/ce_useragent.csv";
        DEFINE FILENAME f_browser      = "m1:/home/tigergraph/mydata/customer360/data/ce_browser.csv";
        DEFINE FILENAME f_items        = "m1:/home/tigergraph/mydata/customer360/data/ce_items.csv";
        DEFINE FILENAME f_store        = "m1:/home/tigergraph/mydata/customer360/data/ce_store.csv";
        DEFINE FILENAME f_movies       = "m1:/home/tigergraph/mydata/customer360/data/ce_movies.csv";
        DEFINE FILENAME f_books        = "m1:/home/tigergraph/mydata/customer360/data/ce_books.csv";

        DEFINE HEADER hdr_camp_ads   = "campaign_id", "campaign_name", "start_date", "end_date", "campaign_type", "ad_id", "ad_name", "ad_channel";
        DEFINE HEADER hdr_camp_promo = "campaign_id", "campaign_name", "campaign_year", "start_date", "end_date", "campaign_type", "promotion_id", "promotion_name", "promotion_channel";
        DEFINE HEADER hdr_solicits   = "user_id", "campaign_id", "solicit_timestamp";
        DEFINE HEADER hdr_response   = "user_id", "campaign_id", "resp_timestamp", "resp_chnl";
        DEFINE HEADER hdr_profile    = "user_id", "first_name", "last_name", "user_name", "gender", "ssn", "birthdate", "job_desc", "creditcard", "ethnicity", "married", "income", "education", "source_id";
        DEFINE HEADER hdr_ipaddr     = "ipaddress", "ip_type";
        DEFINE HEADER hdr_device     = "device_id", "device_type";
        DEFINE HEADER hdr_email      = "email", "email_type", "emal_perm", "email_freq";
        DEFINE HEADER hdr_phone      = "phone_number", "phone_perm", "phone_freq";
        DEFINE HEADER hdr_address    = "address_id", "address_type", "address_line", "address_nbr", "city", "postalcode", "state", "country";
        DEFINE HEADER hdr_source     = "source_id", "source_name", "source_type";
        DEFINE HEADER hdr_geog       = "address_id","zipcode","city","state_abbr","state","county_id","county_name","msa_id","msa_name","lat","lng","population","market_id","market_name","region_id","region_name","store_id","other";
        DEFINE HEADER hdr_urls       = "page_id", "page_location", "page_menu", "page_item", "item_id", "page_weight", "page_url", "next_page_id";
        DEFINE HEADER hdr_useragent  = "ua_id", "user_agent", "browser", "browser_ver", "device", "device_os", "device_osver", "device_branch", "device_model";
        DEFINE HEADER hdr_browser    = "browser";
        DEFINE HEADER hdr_events     = "user_id", "session_id", "event_id", "event_timestamp", "event_type", "event_time", "event_order", "ua_id", "page_url", "prev_page_url", "ad_id";
        DEFINE HEADER hdr_sessions   = "user_id", "session_id", "session_timestamp", "ua_id", "browser_id", "hostname", "referer", "promotion_id", "session_time", "device_id", "ip_address", "event_cnt", "view_cnt", "click_cnt", "cart_flg", "purch_flg";
        DEFINE HEADER hdr_item       = "item_id", "item_desc", "dept_id", "dept_desc", "catg_id", "catg_desc", "prodClass", "prodBrand", "item_price", "item_cost", "next_dept_id", "next_catg_id", "item_weight";
        DEFINE HEADER hdr_store      = "store_id", "store_name", "store_type", "shiptostore", "autocenter", "salon", "zipcode", "city", "state", "county_name", "msa_name", "lat", "lng", "market", "region";
        DEFINE HEADER hdr_orders     = "user_id", "order_nbr", "order_source", "order_timestamp", "order_type", "store_id", "promo_id", "order_qty", "order_amt", "order_profit", "item_id", "item_qty", "item_price", "item_cost", "item_amt", "item_profit";
        DEFINE HEADER hdr_movieratings = "user_id","movie_id","movie_date","movie_rating";
        DEFINE HEADER hdr_bookratings  = "user_id","book_id","book_date","book_rating";
        DEFINE HEADER hdr_movies       = "movie_id", "movie_title", "movie_year", "movie_genre";
        DEFINE HEADER hdr_books        = "book_id","ISBN", "BookTitle","BookAuthor","BookYear","Publisher","BookImage";

        ###################################################################
        # Entity Resolution
        ###################################################################
        LOAD f_profile TO VERTEX Profile        VALUES ($"user_id", $"first_name", $"last_name", $"user_name", $"gender", $"ssn", $"birthdate", $"job_desc", $"creditcard", $"ethnicity", $"married", $"income", $"education",_,_,_,_) USING header="true", separator=",", USER_DEFINED_HEADER="hdr_profile";
        LOAD f_address TO VERTEX Address        VALUES ($"address_id", $"address_type", $"address_line", $"city", $"postalcode", $"state", $"country") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_address";
        LOAD f_address TO EDGE address_location VALUES ($"address_id", $"postalcode") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_address";
        LOAD f_ipaddr  TO VERTEX IpAddress      VALUES ($"ipaddress", $"ip_type") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_ipaddr";
        LOAD f_email   TO VERTEX Email          VALUES ($"email", $"email_type", $"emal_perm", $"email_freq") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_email";
        LOAD f_device  TO VERTEX Device         VALUES ($"device_id", $"device_type") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_device";
        LOAD f_phone   TO VERTEX Phone          VALUES ($"phone_number", $"phone_perm", $"phone_freq") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_phone";

        LOAD e_address TO EDGE profile_address VALUES ($0, $1, $2) USING header="true", separator=",";
        LOAD e_ipaddr  TO EDGE profile_ipaddress VALUES ($0, $1) USING header="true", separator=",";
        LOAD e_email   TO EDGE profile_email VALUES ($0, $1, $2, $3) USING header="true", separator=",";
        LOAD e_device  TO EDGE profile_device VALUES ($0, $1) USING header="true", separator=",";
        LOAD e_phone   TO EDGE profile_phone VALUES ($0, $1) USING header="true", separator=",";
        LOAD e_links   TO EDGE profile_friends  VALUES ($0, $1, $2, $3) USING header="true", separator=",";

        LOAD f_source  TO VERTEX DataSource VALUES ($"source_id", $"source_name", $"source_type") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_source";
        LOAD f_geog    TO VERTEX Location   VALUES ($"zipcode", $"city", $"state", $"market_name", $"region_name", $"population", $"lat" ,$"lng") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_geog";
        LOAD f_profile TO EDGE profile_datasource VALUES ($"user_id", $"source_id") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_profile";

        ###################################################################
        # Orders
        ###################################################################
        LOAD f_items  TO VERTEX Products VALUES ($"item_id", $"item_desc", $"dept_desc", $"catg_desc", $"prodClass", $"prodBrand", $"item_price", $"item_cost") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_item";
        LOAD f_store  TO VERTEX Store    VALUES ($"store_id", $"store_name", $"store_type", $"shiptostore", $"autocenter", $"salon", $"zipcode", $"city", $"state", $"market", $"region", $"lat", $"lng") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_store";
        LOAD f_orders TO VERTEX Orders   VALUES ($"order_nbr", $"order_source", $"order_timestamp", $"order_type", $"order_qty", $"order_amt", $"order_profit") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_orders";
        LOAD f_store  TO EDGE store_location VALUES ($"store_id", $"zipcode") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_store";
        LOAD f_orders TO EDGE profile_purchase VALUES ($"user_id", $"order_nbr") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_orders";
        LOAD f_orders TO EDGE order_store VALUES ($"order_nbr", $"store_id") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_orders";
        LOAD f_orders TO EDGE order_items VALUES ($"order_nbr", $"item_id", $"order_timestamp", $"item_qty", $"item_price", $"item_cost", $"item_amt", $"item_profit") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_orders";

        ###################################################################
        # Campaigns
        ###################################################################
        LOAD f_camp_promo TO VERTEX Campaign         VALUES ($"campaign_id", $"campaign_year", $"campaign_name", $"campaign_type", $"start_date", $"end_date") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_camp_promo";
        LOAD f_camp_promo TO VERTEX Promotions       VALUES ($"promotion_id", $"promotion_name", $"promotion_channel") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_camp_promo";
        LOAD f_camp_promo TO EDGE campaign_promotion VALUES ($"campaign_id", $"promotion_id") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_camp_promo";
        LOAD f_camp_ads   TO VERTEX Ads              VALUES ($"ad_id", $"ad_name", $"ad_channel") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_camp_ads";
        LOAD f_camp_ads   TO EDGE campaign_ads       VALUES ($"campaign_id", $"ad_id") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_camp_ads";
        LOAD f_solicts    TO EDGE profile_campaign   VALUES ($"user_id", $"campaign_id", $"solicit_timestamp") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_solicits";
        LOAD f_respons    TO EDGE profile_response   VALUES ($"user_id", $"campaign_id", $"resp_timestamp", $"resp_chnl") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_response";

        ###################################################################
        # Sessions and Events
        ###################################################################
        LOAD f_urls      TO VERTEX urlPages          VALUES ($"page_url", $"page_location", $"page_menu", $"page_item") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_urls";
        LOAD f_useragent TO VERTEX UserAgent         VALUES ($"ua_id", $"user_agent") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_useragent";
        LOAD f_browser   TO VERTEX Browser           VALUES ($"browser") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_browser";
        LOAD f_events    TO VERTEX Events            VALUES ($"event_id", $"event_timestamp", $"event_type", $"event_time", $"event_order", $"page_url", $"prev_page_url") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_events";
        LOAD f_events    TO EDGE   event_url         VALUES ($"event_id", $"page_url") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_events";
        LOAD f_events    TO EDGE   event_ads         VALUES ($"event_id", $"ad_id") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_events";
        LOAD f_events    TO EDGE   session_events    VALUES ($"session_id",$"event_id") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_events";
        LOAD f_sessions  TO VERTEX Sessions          VALUES ($"session_id",$"session_timestamp", $"hostname", $"referer", $"session_time", $"ip_address", $"event_cnt", $"view_cnt", $"click_cnt") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_sessions";
        LOAD f_sessions  TO EDGE   session_browser   VALUES ($"session_id",$"browser_id") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_sessions";
        LOAD f_sessions  TO EDGE   session_useragent VALUES ($"session_id",$"ua_id") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_sessions";
        LOAD f_sessions  TO EDGE   session_ipaddr    VALUES ($"session_id",$"ip_address") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_sessions";
        LOAD f_sessions  TO EDGE   session_device    VALUES ($"session_id",$"device_id") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_sessions";
        LOAD f_sessions  TO EDGE  profile_sessions   VALUES ($"user_id", $"session_id") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_sessions";

        ###################################################################
        # Movie and Book Ratings
        ###################################################################
        LOAD f_movies       TO VERTEX Movies         VALUES ($"movie_id", $"movie_title", $"movie_year") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_movies";
        LOAD f_movies       TO VERTEX Genre          VALUES ($"movie_genre") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_movies";
        LOAD f_movies       TO EDGE movie_genre      VALUES ($"movie_id", $"movie_genre") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_movies";
        LOAD f_books        TO VERTEX Books          VALUES ($"book_id", $"BookTitle", $"BookYear", $"BookImage", $"ISBN") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_books";
        LOAD f_books        TO VERTEX Author         VALUES ($"BookAuthor") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_books";
        LOAD f_books        TO VERTEX Publisher      VALUES ($"Publisher") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_books";
        LOAD f_books        TO EDGE book_author      VALUES ($"book_id", $"BookAuthor") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_books";
        LOAD f_books        TO EDGE book_publisher   VALUES ($"book_id", $"Publisher") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_books";
        LOAD f_movieratings TO EDGE movie_ratings    VALUES ($"user_id", $"movie_id", $"movie_date", $"movie_rating") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_movieratings";
        LOAD f_bookratings  TO EDGE book_ratings     VALUES ($"user_id", $"book_id", $"book_date", $"book_rating") USING header="true", separator=",", USER_DEFINED_HEADER="hdr_bookratings";

      }

      RUN LOADING JOB customer360_load
      DROP JOB customer360_load
    #+end_src
*** Add queries
**** UDF functions required for queries
     + ExprUtil
       #+begin_src c++ :tangle solutions/customer360/ExprUtil.hpp
         /******************************************************************************
          ,* Copyright (c) 2016, TigerGraph Inc.
          ,* All rights reserved.
          ,* Project: TigerGraph Query Language
          ,*
          ,* - This library is for defining struct and helper functions that will be used
          ,*   in the user-defined functions in "ExprFunctions.hpp". Note that functions
          ,*   defined in this file cannot be directly called from TigerGraph Query scripts.
          ,*   Please put such functions into "ExprFunctions.hpp" under the same directory
          ,*   where this file is located.
          ,*
          ,* - Please don't remove necessary codes in this file
          ,*
          ,* - A backup of this file can be retrieved at
          ,*     <tigergraph_root_path>/dev_<backup_time>/gdk/gsql/src/QueryUdf/ExprUtil.hpp
          ,*   after upgrading the system.
          ,*
          ,******************************************************************************/

         #ifndef EXPRUTIL_HPP_
         #define EXPRUTIL_HPP_

         #include <stdlib.h>
         #include <stdio.h>
         #include <string>
         #include <gle/engine/cpplib/headers.hpp>

         typedef std::string string; //XXX DON'T REMOVE

         /*
          ,* Define structs that used in the functions in "ExprFunctions.hpp"
          ,* below. For example,
          ,*
          ,*   struct Person {
          ,*     string name;
          ,*     int age;
          ,*     double height;
          ,*     double weight;
          ,*   }
          ,*
          ,*/

           /*******************************************************
            ,* Config this for grid size. The numer is in minunte.
            ,* the area of a geo grid is the square of GRID_SIDE_LENGTH.
            ,* The value of GRID_SIDE_LENGTH should be able to evenly devide 60.
            ,******************************************************/
           static const int GRID_SIDE_LENGTH = 4;

           struct Degree {
             int day;
             int min;
             int sec;

             Degree() {
               day = 0;
               min = 0;
               sec = 0;
             }

             Degree(int d, int m, int s) {
               day = d;
               min = m;
               sec = s;
             }
           };

           const double pi = 3.14159265358979323846;
           static const float  earthRadiusKm = 6373.0;
           static const int MAP_LAT_LOW_BOUND = -90;   // Low Bound of latitude of the map;
           static const int MAP_LAT_HIGH_BOUND = 90;   // High Bound of latitude of the map;
           static const int MAP_LONG_LOW_BOUND = -180; // Low Bound of longitude of the map;
           static const int MAP_LONG_HIGH_BOUND = 180; // High Bound of longitude of the map;
           static const int NUM_OF_COLS = (MAP_LONG_HIGH_BOUND*60 -MAP_LONG_LOW_BOUND*60)/GRID_SIDE_LENGTH;
           static const int NUM_OF_ROWS = (MAP_LAT_HIGH_BOUND*60 - MAP_LAT_LOW_BOUND*60)/GRID_SIDE_LENGTH;

           inline float km2Latitude(float km) {
             return km/110.54;
           }

           inline float km2Longitude(float km, double lat) {
             return km/(111.32*cos(lat));
           }

           inline int gridNumlat (float km) {
             return std::abs(round(km2Latitude(km)/(360.0/NUM_OF_COLS)));
           }

           inline int gridNumLong (float km, double lat) {
             return std::abs(round(km2Longitude(km, lat)/(180.0/NUM_OF_ROWS)));
           }

           inline void deg_to_dms(double deg, Degree& degree) {
             degree.day = int(deg);
             double md = (deg - degree.day) * 60;
             degree.min = int(md);
             degree.sec = (md - degree.min) * 60;
           }

           inline string map_lat_long_grid_id(double latitude,double longitude) {
             Degree lat_degree,long_degree;

             deg_to_dms(latitude,lat_degree);
             deg_to_dms(longitude,long_degree);

             int64_t grid_id = ((lat_degree.day - MAP_LAT_LOW_BOUND) * 60 + lat_degree.min)/GRID_SIDE_LENGTH * NUM_OF_COLS + ((long_degree.day - MAP_LONG_LOW_BOUND) * 60 + long_degree.min)/GRID_SIDE_LENGTH;

             return std::to_string(grid_id);
           }

           // public domain conversion of degrees to radians
           // used in geo-location.
           inline double deg2rad(double d) {
             return (d * pi / 180);
           }


         #endif /* EXPRUTIL_HPP_ */
       #+end_src

     + ExprFunctions
       #+begin_src c++ :tangle solutions/customer360/ExprFunctions.hpp
         /******************************************************************************
          ,* Copyright (c) 2015-2016, TigerGraph Inc.
          ,* All rights reserved.
          ,* Project: TigerGraph Query Language
          ,* udf.hpp: a library of user defined functions used in queries.
          ,*
          ,* - This library should only define functions that will be used in
          ,*   TigerGraph Query scripts. Other logics, such as structs and helper
          ,*   functions that will not be directly called in the GQuery scripts,
          ,*   must be put into "ExprUtil.hpp" under the same directory where
          ,*   this file is located.
          ,*
          ,* - Supported type of return value and parameters
          ,*     - int
          ,*     - float
          ,*     - double
          ,*     - bool
          ,*     - string (don't use std::string)
          ,*     - accumulators
          ,*
          ,* - Function names are case sensitive, unique, and can't be conflict with
          ,*   built-in math functions and reserve keywords.
          ,*
          ,* - Please don't remove necessary codes in this file
          ,*
          ,* - A backup of this file can be retrieved at
          ,*     <tigergraph_root_path>/dev_<backup_time>/gdk/gsql/src/QueryUdf/ExprFunctions.hpp
          ,*   after upgrading the system.
          ,*
          ,******************************************************************************/

         #ifndef EXPRFUNCTIONS_HPP_
         #define EXPRFUNCTIONS_HPP_

         #include <stdlib.h>
         #include <stdio.h>
         #include <string>
         #include <gle/engine/cpplib/headers.hpp>

         /**     XXX Warning!! Put self-defined struct in ExprUtil.hpp **
          ,*  No user defined struct, helper functions (that will not be directly called
          ,*  in the GQuery scripts) etc. are allowed in this file. This file only
          ,*  contains user-defined expression function's signature and body.
          ,*  Please put user defined structs, helper functions etc. in ExprUtil.hpp
          ,*/
         #include "ExprUtil.hpp"

         namespace UDIMPL {
           typedef std::string string; //XXX DON'T REMOVE

           /****** BIULT-IN FUNCTIONS **************/
           /****** XXX DON'T REMOVE ****************/
           inline int str_to_int (string str) {
             return atoi(str.c_str());
           }

           inline int float_to_int (float val) {
             return (int) val;
           }

           inline string to_string (double val) {
             char result[200];
             sprintf(result, "%g", val);
             return string(result);
           }

           inline void getVertexesFromEdge(SetAccum<EDGE>& edgeSet, SetAccum<VERTEX>& res) {
             for (auto it = edgeSet.data_.begin(); it != edgeSet.data_.end(); ++it) {
               res += it->srcVid;
               res += it->tgtVid;
             }
           }

           template<typename EdgeTuple>
             inline bool PathContainsV(ListAccum<EdgeTuple>& pathAccum, VERTEX v) {
               for (uint32_t i = 0; i < pathAccum.data_.size(); ++i) {
                 if (v == pathAccum.data_[i].v) {
                   return true;
                 }
               }
               return false;
             }

         }


         /****************************************/

         #endif /* EXPRFUNCTIONS_HPP_ */
       #+end_src

**** The queries

    #+begin_src sql :tangle solutions/customer360/03-add-queries.gsql :mkdirp yes
      USE GRAPH Customer360
      PUT ExprFunctions FROM "/home/tigergraph/mydata/customer360/ExprFunctions.hpp"
      PUT ExprUtil FROM "/home/tigergraph/mydata/customer360/ExprUtil.hpp"

      CREATE QUERY categorySales(/* Parameters here */) FOR GRAPH Customer360 {
        TYPEDEF TUPLE <STRING categoryName, STRING salesDate, INT itemQty, DOUBLE SalesAmt> info;

        HeapAccum<info>(100, SalesAmt ASC, categoryName DESC) @@result;
        GroupByAccum<STRING categoryName, STRING salesDate, SumAccum<INT> itemQty, SumAccum<DOUBLE> SalesAmt> @@groupBy;
        MinAccum<STRING> @categoryName;
        MinAccum<STRING> @salesDate;
        SumAccum<INT>    @itemQty;
        SumAccum<DOUBLE> @itemSalesAmt;

        products = {Products.*};

        vCategory = SELECT s
                      FROM products:s-(order_items:e)-Orders:t
                     ACCUM s.@categoryName += s.category,
                           s.@salesDate += datetime_format(e.itemDate,"%Y-%m-%d"),
                           s.@itemQty += e.itemQty,
                           s.@itemSalesAmt += e.itemAmt
                POST-ACCUM @@groupBy += (s.@categoryName, s.@salesDate -> s.@itemQty, s.@itemSalesAmt);

        FOREACH c IN @@groupBy DO
          @@result += info(c.categoryName, c.salesDate, c.itemQty, c.SalesAmt);
        END;

        PRINT @@result;
      }


      INSTALL QUERY categorySales


      CREATE QUERY similarCustomers(VERTEX source, STRING e_type, STRING rev_e_type, INT top_k = 100, BOOL print_accum = TRUE, STRING similarity_edge_type = "", STRING file_path = "") FOR GRAPH Customer360 {
      /*
      Calculates the Jaccard Similarity between a given vertex and every other vertex.
        Jaccard similarity = intersection_size / (size_A + size_B - intersection_size)
      Parameters:
       source: start vertex                           top_k: #top scores to report
       e_type: directed edge types to traverse        print_accum: print JSON output
       rev_e_type: reverse edge types to traverse     file_path: file to write CSV output to
       similarity_edge_type: edge type for storing vertex-vertex similarity scores

        This query current supports only a single edge type (not a set of types) - 8/13/20
      ,*/

        SumAccum<INT> @sum_intersection_size, @@sum_set_size_A, @sum_set_size_B;
        SumAccum<FLOAT> @sum_similarity;
        FILE f (file_path);

        Start (ANY) = {source};
        Start = SELECT s
                    FROM Start:s
      //	  ACCUM @@sum_set_size_A += s.outdegree(e_type);
                   ACCUM @@sum_set_size_A += s.outdegree("movie_ratings"),
                       @@sum_set_size_A += @@sum_set_size_A + s.outdegree("book_ratings"),
                       @@sum_set_size_A += @@sum_set_size_A + s.outdegree("profile_sessions"),
                       @@sum_set_size_A += @@sum_set_size_A + s.outdegree("profile_purchase"),
                       @@sum_set_size_A += @@sum_set_size_A + s.outdegree("profile_campaign"),
                       @@sum_set_size_A += @@sum_set_size_A + s.outdegree("profile_response");

        Subjects = SELECT t
                       FROM Start:s-((movie_ratings|book_ratings|profile_sessions|profile_purchase):e)-:t;

        Others = SELECT t
                     FROM Subjects:s -((movie_ratings|book_ratings|profile_sessions|profile_purchase):e)- :t
                    WHERE t != source
                    ACCUM t.@sum_intersection_size += 1,
                          t.@sum_set_size_B = t.outdegree("movie_ratings"),
                        t.@sum_set_size_B = t.@sum_set_size_B + t.outdegree("book_ratings"),
                        t.@sum_set_size_B = t.@sum_set_size_B + t.outdegree("profile_sessions"),
                        t.@sum_set_size_B = t.@sum_set_size_B + t.outdegree("profile_purchase"),
                        t.@sum_set_size_B = t.@sum_set_size_B + t.outdegree("profile_campaign"),
                        t.@sum_set_size_B = t.@sum_set_size_B + t.outdegree("profile_response")
               POST-ACCUM t.@sum_similarity = (t.@sum_intersection_size*1.0) / (@@sum_set_size_A + t.@sum_set_size_B - t.@sum_intersection_size)
                 ORDER BY t.@sum_similarity DESC
                    LIMIT top_k;

        IF file_path != "" THEN
            f.println("Vertex1", "Vertex2", "Similarity");
        END;

        Others = SELECT s
             FROM Others:s
             POST-ACCUM
      //	       IF similarity_edge_type != "" THEN
      //	           INSERT INTO EDGE similarity_edge_type VALUES (source, s, s.@sum_similarity)
      //	       END,
                 IF file_path != "" THEN
                     f.println(source, s, s.@sum_similarity)
                 END;

        IF print_accum THEN
            PRINT Others[Others.@sum_similarity];
        END;
      }

      INSTALL QUERY similarCustomers


      CREATE QUERY productSalesAnalysis(STRING inDepartment, DATETIME inStartDate, DATETIME inEndDate) FOR GRAPH Customer360 {
      /* Traditional BI Style Query that Runs a Product Sales Report
       ,* PARAMETERS: Department, StartDate, EndDate
       ,* Toys, 2020-01-01, 2020-12-31
       ,* Books, 2020-03-14, 2021-01-15
       ,* Health, 2020-01-15, 2020-10-10
      ,*/
        TYPEDEF TUPLE <STRING deptName, STRING categoryName, STRING salesDate, INT itemQty, DOUBLE SalesAmt> info;

        HeapAccum<info>(200, SalesAmt ASC, deptName DESC) @@result;
        GroupByAccum<STRING deptName, STRING categoryName, STRING salesDate, SumAccum<INT> itemQty, SumAccum<DOUBLE> SalesAmt> @@count;
        MinAccum<STRING> @deptName;
        MinAccum<STRING> @categoryName;
        MinAccum<STRING> @salesDate;
        SumAccum<INT> @itemQty;
        SumAccum<DOUBLE> @itemSalesAmt;

        Start = {Products.*};

        vCategory = SELECT s
                      FROM Start:s-(order_items:e)-Orders:t
                     WHERE trim(BOTH s.department) == inDepartment AND
                           e.itemDate >= inStartDate AND
                           e.itemDate <= inEndDate
                     ACCUM s.@deptName += s.department,
                           s.@categoryName += s.category,
                           s.@salesDate += datetime_format(e.itemDate,"%Y-%m-%d"),
                           s.@itemQty += e.itemQty,
                           s.@itemSalesAmt += e.itemAmt
                POST-ACCUM @@count += (s.@deptName, s.@categoryName, s.@salesDate -> s.@itemQty, s.@itemSalesAmt);

        FOREACH c IN @@count DO
       //   f.println(c.categoryName, c.salesDate, c.itemQty, c.SalesAmt);
          @@result += info(c.deptName, c.categoryName, c.salesDate, c.itemQty, c.SalesAmt);
        END;

        PRINT @@result;
      }

      INSTALL QUERY productSalesAnalysis

      # entityResolution_sub must be created and installed prior to creating entityResolution_driver
      CREATE QUERY entityResolution_sub(Vertex<Profile> inProfile, float threshold) FOR GRAPH Customer360 {
          SumAccum<float> @score;
          start = {inProfile};

        all_profiles = SELECT t FROM start-((profile_phone|profile_email|profile_ipaddress|profile_address|profile_device):e)-:t;

          similar_profiles = SELECT t FROM all_profiles-((profile_phone|profile_email|profile_ipaddress|profile_address|profile_device):e)-:t
                                     WHERE t != inProfile
                                             ACCUM CASE e.type
                                                              WHEN "profile_phone"     THEN t.@score += 0.4
                                                              WHEN "profile_email"     THEN t.@score += 0.3
                                                              WHEN "profile_device"    THEN t.@score += 0.2
                                                              WHEN "profile_ipaddress" THEN t.@score += 0.1
                                                              WHEN "profile_address"   THEN t.@score += 0.5
                                                       END
                                 POST-ACCUM
                                              CASE WHEN t.@score >= threshold THEN
                                             INSERT INTO same_profile VALUES (inProfile, t, t.@score)
                                                END;

       }

      INSTALL QUERY entityResolution_sub

      CREATE QUERY entityResolution_driver(FLOAT threshold) FOR GRAPH Customer360 {
      /* Entity Resolution Driver
       ,* Query will Process all Profiles by passing the Profile_id to the sub Query
       ,* with a threshold parameter that will score the Entity Resolution rules
       ,* the sub-query will insert edge entries into the same_profile (Edge)
       ,* The first step of this query will delete all entries in the same_profile (Edge)
       ,* PARAMETERS threshold - defines match confidence level valid values are .5 to 1.5
       ,*
       ,* Print match groups with 3 or more profiles
      ,*/
        ListAccum<VERTEX> @vertexList;
        ListAccum<EDGE> @edgeList;
        SumAccum<INT> @set_size;
        SetAccum<VERTEX<Profile>> @prfl_groups;

          all_profiles = {Profile.*};
          rm = SELECT s FROM all_profiles:s-(same_profile:e)-:t ACCUM delete(e);
          all_profiles = SELECT s FROM all_profiles:s ACCUM entityResolution_sub(s,threshold);

        Start = SELECT s FROM all_profiles:s-(same_profile:e)-:t
                        ACCUM s.@edgeList += e,
                              s.@vertexList += s,
                              s.@set_size += 1;

        x = SELECT s FROM Start:s WHERE s.@set_size >= 2;
        print x;

      }

      INSTALL QUERY entityResolution_driver


      CREATE QUERY customerJourney(/* Parameters here */) FOR GRAPH Customer360 {
        /* Write query logic here */

        OrAccum @shop;
        OrAccum @cart;
        OrAccum @purchase;

        SumAccum <INT> @ordCount;
        SumAccum <DOUBLE> @ordAmount;
        MaxAccum <DATETIME> @lstPurch;
        MinAccum <DATETIME> @fstPurch;

        MapAccum<STRING, SetAccum<DATETIME>> @awareness;
        MapAccum<STRING, SetAccum<DATETIME>> @consider;
        MapAccum<STRING, SetAccum<DATETIME>> @purchases;
        MapAccum<STRING, STRING> @loyaltyRecency;
        MapAccum<STRING, STRING> @loyaltyFrequency;
        MapAccum<STRING, STRING> @loyaltyMonetary;

        Start = {Campaign.*};
        allEvents = {Events.*};
        allProds = {Products.*};

        ###################################################################
        # GroupBy Profile->PurchOrders  (profile, full_name, gender, ord_date, SUM(ord_qty) )
        ###################################################################
      #  pOrders = SELECT t FROM Start:s-(profile_purchase)-Orders:t
      #                    ACCUM t.@profileOrders += (s,s.profile_name, t.order_date, t.order_type ->1);

        acquire = SELECT t
                    FROM Start:s-(profile_campaign:e)-Profile:t
                   ACCUM CASE WHEN s.campaign_type == "Acquisition" THEN t.@awareness += ("Introduction"      -> e.campaign_date)
                              WHEN s.campaign_type == "Other"       THEN t.@awareness += ("Product Awareness" -> e.campaign_date)
                              WHEN s.campaign_type == "Special"     THEN t.@awareness += ("Brand Awareness"   -> e.campaign_date)
                         END;

        events = SELECT s
                   FROM allEvents:s-(event_url:e)-urlPages:t
                   ACCUM CASE WHEN t.page_location == "Cart"     THEN s.@shop     += True
                              WHEN t.page_location == "CheckOut" THEN s.@purchase += True
                              ELSE                                    s.@shop     += True
                         END;

        sess = SELECT t
                   FROM events:s-(session_events:e)-Sessions:t
                   ACCUM IF s.@shop     == True THEN t.@consider += ("Web Shop"     -> t.session_timestamp) END,
                         IF s.@cart     == True THEN t.@consider += ("Web Cart"     -> t.session_timestamp) END,
                         IF s.@purchase == True THEN t.@consider += ("Purchase Web" -> t.session_timestamp) END,
                                                     t.@consider += ("Web Visit"    -> t.session_timestamp);

        sess2prof = SELECT t
                      FROM sess:s-(profile_sessions:e)-Profile:t
                     ACCUM t.@consider += s.@consider;

        orders = SELECT o
                     FROM allProds:s-(order_items:e)-Orders:o
                    ACCUM o.@ordAmount += e.itemAmt,
                          o.@ordCount += e.itemQty,
                          CASE WHEN s.department == "Personal Care"   OR s.department == "Health"  OR s.department == "Beauty" THEN o.@purchases += ("Personal Beaty & Health"  -> o.order_date)
                               WHEN s.department == "Clothing"        OR s.department == "Baby"    OR s.department == "Toys"   THEN o.@purchases += ("Clothes Baby and Toys"    -> o.order_date)
                               WHEN s.department == "Home and Garden" OR s.department == "Household Essentials"                THEN o.@purchases += ("Home and Household"       -> o.order_date)
                               WHEN s.department == "Auto and Tires"  OR s.department == "Sports and Outdoors"                 THEN o.@purchases += ("Auto Sports and Outdoors" -> o.order_date)
                          END;
        prof = SELECT t
                 FROM orders:s-(profile_purchase:e)-Profile:t
                     ACCUM t.@purchases += s.@purchases,
                           t.@lstPurch += s.order_date,
                           t.@fstPurch += s.order_date,
                           t.@ordAmount += s.@ordAmount,
                           t.@ordCount  += s.@ordCount;

        profiles = SELECT t FROM prof:t
                    ACCUM  CASE WHEN (datetime_diff(t.@lstPurch, t.@fstPurch) / 60 ) / 24 BETWEEN  0 AND  10 THEN t.@loyaltyRecency += ("Recency"  -> "10")
                                WHEN (datetime_diff(t.@lstPurch, t.@fstPurch) / 60 ) / 24 BETWEEN 11 AND  30 THEN t.@loyaltyRecency += ("Recency"  -> "30")
                                WHEN (datetime_diff(t.@lstPurch, t.@fstPurch) / 60 ) / 24 BETWEEN 31 AND  60 THEN t.@loyaltyRecency += ("Recency"  -> "60")
                                WHEN (datetime_diff(t.@lstPurch, t.@fstPurch) / 60 ) / 24 BETWEEN 61 AND  90 THEN t.@loyaltyRecency += ("Recency"  -> "90")
                                WHEN (datetime_diff(t.@lstPurch, t.@fstPurch) / 60 ) / 24 BETWEEN 91 AND 120 THEN t.@loyaltyRecency += ("Recency" -> "120")
                                WHEN (datetime_diff(t.@lstPurch, t.@fstPurch) / 60 ) / 24 > 121              THEN t.@loyaltyRecency += ("Recency" -> "999")
                           END,
                           CASE WHEN (datetime_diff(t.@lstPurch, t.@fstPurch) / 60 ) / 24  / t.@ordCount BETWEEN  0 AND  5 THEN t.@loyaltyFrequency += ("Frequency"  -> "5")
                                WHEN (datetime_diff(t.@lstPurch, t.@fstPurch) / 60 ) / 24  / t.@ordCount BETWEEN  6 AND 10 THEN t.@loyaltyFrequency += ("Frequency"  -> "10")
                                WHEN (datetime_diff(t.@lstPurch, t.@fstPurch) / 60 ) / 24  / t.@ordCount BETWEEN 11 AND 30 THEN t.@loyaltyFrequency += ("Frequency"  -> "30")
                                WHEN (datetime_diff(t.@lstPurch, t.@fstPurch) / 60 ) / 24  / t.@ordCount BETWEEN 31 AND 60 THEN t.@loyaltyFrequency += ("Frequency"  -> "60")
                                WHEN (datetime_diff(t.@lstPurch, t.@fstPurch) / 60 ) / 24  / t.@ordCount > 61              THEN t.@loyaltyFrequency += ("Frequency"  -> "999")
                           END,
                           CASE WHEN t.@ordAmount BETWEEN     0 AND  1000 THEN t.@loyaltyMonetary += ("Monetary"  ->  "1000")
                                WHEN t.@ordAmount BETWEEN  1001 AND  2000 THEN t.@loyaltyMonetary += ("Monetary"  ->  "2000")
                                WHEN t.@ordAmount BETWEEN  2001 AND  5000 THEN t.@loyaltyMonetary += ("Monetary"  ->  "5000")
                                WHEN t.@ordAmount BETWEEN  5001 AND 10000 THEN t.@loyaltyMonetary += ("Monetary"  -> "10000")
                                WHEN t.@ordAmount BETWEEN 10001 AND 20000 THEN t.@loyaltyMonetary += ("Monetary"  -> "20000")
                                WHEN t.@ordAmount > 20001                 THEN t.@loyaltyMonetary += ("Monetary"  -> "99999")
                           END;

        print profiles;

      }

      INSTALL QUERY customerJourney



      CREATE QUERY connectionMining(vertex<Profile> A, vertex<Profile> B, int k) FOR GRAPH Customer360 {
        /***********************************
         ,* Variables for phase one
         ,***********************************/
         SumAccum<int> @score = 0;
         OrAccum @knowA = false, @knowB = false;
         SetAccum<edge> @evidence, @@edgeSet;
         SetAccum<vertex> @@verSet;

         bool isFirstIter = true;

         L0 (ANY) = {A,B};

        /**************************
         ,* Iteration one
         ,* 1. find all person relates to A or B
         ,* 2. output top k people relates both A and B
         ,* Iteration two
         ,* 1. Start from the person knows only A or B
         ,* 2. Find those people who knows only A knows a person that knows only B
         ,* 3. Find those people who knows only B knows a person that knows only A
         ,* 4. output top k people from 2 and 3
         ,**************************/
         WHILE true LIMIT 3 DO
           // Data Initialize
           L0 = SELECT v
           FROM L0:v
           POST-ACCUM
             CASE WHEN v == A THEN
               v.@knowA += true
             END
             ,CASE WHEN v == B THEN
               v.@knowB += true
             END
             ,v.@score = 1
           ;

            L1 (ANY) = L0;

          // do it twice, go two steps to find the related citizens via
          // hotel, flight, train, bus, case
          log(true, " email, phone, address, ipaddress, device");
          WHILE true LIMIT 3 DO
            L1 = SELECT t
                 FROM L1:s-((profile_email|profile_phone|profile_ipaddress|profile_address|profile_device):e)-:t
                 WHERE t.@score == 0
                 ACCUM t.@knowA += s.@knowA
                       ,t.@knowB += s.@knowB
                       ,t.@score += s.@score
                       ,t.@evidence += s.@evidence
                       ,t.@evidence += e
            ;
            //print A;
          END;

          RELATED = L1;
          L1 = L0;

          // go 4 steps,
          // A/B-->PHONE NUMBER-->PHONE CALL-->PHONE NUMBER-->RELATED PEOPLE
          // A/B-->BANK ACCOUNT-->MONEY TRANS-->BANK ACCOUNT-->RELATED PEOPLE
          log(true, "Purchae, Book, Movie");
          L1 = SELECT t
                 FROM L1:s-((profile_purchase|book_ratings|movie_ratings):e)-:t
                 WHERE t.@score == 0
                 ACCUM t.@knowA += s.@knowA
                       ,t.@knowB += s.@knowB
                       ,t.@score += s.@score
                       ,t.@evidence += s.@evidence
                       ,t.@evidence += e
          ;
      //    WHILE true LIMIT 3 DO
      //      L1 = SELECT t
      //           FROM L1:s-((profile_email|profile_phone|profile_ipaddress|profile_address|profile_device):e)-:t
      //           WHERE t.@score == 0
      //           ACCUM t.@knowA += s.@knowA
      //                 ,t.@knowB += s.@knowB
      //                 ,t.@score += s.@score
      //                 ,t.@evidence += s.@evidence
      //                 ,t.@evidence += e
      //      ;
      //    END;
          L1 = SELECT t
                 FROM L1:s-((profile_purchase|book_ratings|movie_ratings):e)-:t
                 WHERE t.@score == 0
                 ACCUM t.@knowA += s.@knowA
                       ,t.@knowB += s.@knowB
                       ,t.@score += s.@score
                       ,t.@evidence += s.@evidence
                       ,t.@evidence += e
          ;


          // add the newly found citizens to the related set
          RELATED = RELATED UNION L1;

          log(true, "get COMMON_AB");
          // Extract the people having common relations
          // and keep the top k
          COMMON_AB = SELECT t
               FROM RELATED:t
               WHERE t.@knowA == true AND t.@knowB == true
               ORDER BY t.@score
               LIMIT k
          ;

          COMMON_AB = SELECT s from COMMON_AB:s
                      ACCUM @@edgeSet += s.@evidence
          ;

          IF isFirstIter == true THEN
            // find the person that knows only A or B
            L0 = RELATED MINUS COMMON_AB;
                  log(true,L0.size());
          END;
          isFirstIter = false;
        END;

        PRINT @@edgeSet;
        getVertexesFromEdge(@@edgeSet, @@verSet);
        Start = {@@verSet};
        Start = SELECT s FROM Start:s
                POST-ACCUM s.@evidence.clear();

        PRINT Start;
      }

      INSTALL QUERY connectionMining



      CREATE QUERY movieRecommendations(String v_type, String e_type,FLOAT max_change=0.001, INT max_iter=25, FLOAT damping = 0.85, INT topK = 100, INT numberOfBatch, INT batchID ) FOR GRAPH Customer360  RETURNS (MapAccum<VERTEX, SetAccum<VERTEX>>) {

        TYPEDEF TUPLE<VERTEX ver, FLOAT score> Vertex_Score;
        HeapAccum<Vertex_Score>(topK,score DESC) @topScores;
        MapAccum<Vertex,ListAccum<Vertex_Score>> @@res;
        MaxAccum<FLOAT> @@maxDiff = 9999; # max score change in an iteration
        MapAccum<VERTEX,FLOAT> @received_score; # sum of scores each vertex receives FROM neighbors
        MapAccum<VERTEX,FLOAT> @score ;   # Initial score for every vertex is 0
        SetAccum<EDGE> @@edgeSet;                   # list of all edges, if display is needed
        OrAccum @is_source;
        SetAccum<INT> @@end;
        SetAccum<VERTEX> @@allProducts;
        MapAccum<VERTEX, SetAccum<VERTEX>> @@recommendation;

        All = {v_type};#  All with a set of input vertices

        All = SELECT s
                FROM All:s
                WHERE getvid(s) % numberOfBatch == batchID;
        # Assign unique labels to each vertex
        Start = SELECT s
                FROM All:s
                POST-ACCUM s.@score +=(s->1),   # Only set score of source vertices to 1
                           s.@is_source = true;
        Total = All;

        WHILE @@maxDiff > max_change LIMIT max_iter DO
               V_tmp = SELECT t      # Only update score for activated vertices
                       FROM Start:s -(e_type:e)-> :t
      //                 WHERE (s.type == "person" AND e.timestamp <= s.timestamp) OR
      //                       (s.type == "product" AND e.timestamp <= t.timestamp)
                       ACCUM
                            FOREACH (key,value) IN s.@score DO
                                IF getvid(key) in @@end THEN
                                      continue
                                END,
                                FLOAT rec_score = value/s.outdegree(e_type),#/(s.outdegree(e_type);
                                t.@received_score += (key->rec_score)
                            END;
                  T = Start UNION V_tmp;
                  Start = SELECT s
                          FROM T:s
                          POST-ACCUM
                              IF s.@is_source == TRUE THEN
                                    FLOAT oldscore = s.@score.get(s),
                                    s.@score+=(s->-oldscore),
                                    s.@score+=(s->((1.0-damping) + damping *s.@received_score.get(s)))
                              END,
                              FOREACH (key,value) IN s.@received_score DO
                                  IF key!=s THEN
                                      FLOAT oldscore = s.@score.get(key),
                                      s.@score+=(key->-oldscore),
                                      s.@score+=(key->damping*value),
                                      IF damping*value-oldscore<0.001 THEN
                                          @@end+=getvid(key)
                                      END
                                  END
                               END,
                               IF s.type == "Movies" THEN
                                   @@allProducts += s
                               END,
                               s.@received_score.clear();

                  Total=Total UNION T;
                  END;

          Total = SELECT s
                  FROM Total:s
                  POST-ACCUM
                        FOREACH (key,value) IN s.@score DO
                            @@res+=(key->Vertex_Score(s,value))
                            END,
                            s.@score.clear(),s.@received_score.clear();
          @@end.clear();
         PRINT @@res;
          All = SELECT s
                FROM All:s - (e_type:e)-:t
                ACCUM
                       FOREACH i in @@res.get(s) DO
                            IF @@allProducts.contains(i.ver) AND getvid(i.ver) != getvid(t) THEN
                              s.@topScores+=i
                            END
                        END;

          All = SELECT s
                FROM All:s
                POST-ACCUM
                        FOREACH element IN s.@topScores DO
                            @@recommendation += (s -> element.ver)
                        END;
              //PRINT All [All.@topScores];
            RETURN @@recommendation;
      }

      INSTALL QUERY movieRecommendations


      CREATE QUERY productRecommendation_personalized(VERTEX <Profile> inputProfile) FOR GRAPH Customer360 {
        /***********************************************
         ,* Product Recommendation for each profile
         ,* PARAMETERS Profile_id any value between 1 and 50000
      ,*/

        SumAccum<int> @ProdInCommon;
        SumAccum<float> @SimilarityScore;
        SumAccum<float> @Rank;
        OrAccum @Purchased = false;

        start = {inputProfile};
        allprods = {Products.*};

        #1. mark Products Purchased by the Profile
        #2. calculate log similarity score for each persons share the same interests at Tag level.
        orders = SELECT o FROM start:s-(profile_purchase:e)-Orders:o
                  WHERE s == inputProfile;
        #print orders;

        products = SELECT p FROM orders:s-(order_items:e)-Products:p
                    ACCUM p.@Purchased = true,
                          p.@ProdInCommon += 1
               POST-ACCUM p.@SimilarityScore = log (1 + p.@ProdInCommon);

        #print products;


        # recommend new messages to Viktor that have not liked by him.
        Recommendations = SELECT p FROM allprods:p
                           WHERE p.@Purchased == false
                           ACCUM p.@Rank += p.@SimilarityScore
                        ORDER BY p.@Rank DESC
                   LIMIT 10;

        print Recommendations;

      }

      INSTALL QUERY productRecommendation_personalized




      CREATE QUERY bookRecommendation(VERTEX <Profile> inputProfile) FOR GRAPH Customer360 {
        /***********************************************
         ,* Product Recommendation for each profile
         ,* PARAMETERS Profile_id any value between 1 and 50000
      ,*/

        SumAccum<int> @BookInCommon;
        SumAccum<float> @SimilarityScore;
        SumAccum<float> @Rank;
        OrAccum @Read = false;

        start = {inputProfile};
        allBooks = {Books.*};

        #1. mark Products Purchased by the Profile
        #2. calculate log similarity score for each persons share the same interests at Tag level.
        books =  SELECT b FROM start:s-(book_ratings:e)-Books:b
                  WHERE s == inputProfile;
        #print orders;

        booksRead = SELECT b FROM start:s-(book_ratings:e)-Books:b
                    ACCUM b.@Read = true,
                          b.@BookInCommon += 1
               POST-ACCUM b.@SimilarityScore = log (1 + b.@BookInCommon);

        #print products;


        # recommend new messages to Viktor that have not liked by him.
        Recommendations = SELECT p FROM allBooks:p
                           WHERE p.@Read == false
                           ACCUM p.@Rank += p.@SimilarityScore
                        ORDER BY p.@Rank DESC
                   LIMIT 10;

        print Recommendations;

      }

      INSTALL QUERY bookRecommendation

      CREATE QUERY bestCustomers(/* Parameters here */) FOR GRAPH Customer360 {
        TYPEDEF TUPLE<VERTEX<Profile> profile, STRING last_name, STRING first_name, STRING gender, INT ord_cnt, DOUBLE ord_qty, DOUBLE ord_amt> profile;

        SumAccum<INT> @ordCnt;
        SumAccum<INT> @ordQty;
        SumAccum<DOUBLE> @ordAmt;
        OrAccum @visited;
        HeapAccum<profile>(100, ord_cnt DESC, ord_amt DESC) @@profileTop;

        Start = { Profile.* };

        vPersonHasOrder = SELECT s FROM Start:s-(profile_purchase:e)-Orders:t
                           ACCUM s.@visited += True,
                                 s.@ordCnt += 1,
                                 s.@ordQty += t.order_qty,
                                 s.@ordAmt += t.order_amt
                      POST-ACCUM @@profileTop += profile(s, s.last_name, s.first_name, s.gender, s.@ordCnt, s.@ordQty, s.@ordAmt)
                          LIMIT 100;

        PRINT @@profileTop;
      }

      INSTALL QUERY bestCustomers



      CREATE QUERY community_step1_jaccard_batch ( INT top_k = 10, SET<STRING> v_type, SET<STRING> feat_v_type, SET<STRING> e_type, SET<STRING> re_type, INT src_batch_num = 50, INT nbor_batch_num = 10, BOOL print_accum = true, INT print_limit = 50, STRING file_path = "") FOR GRAPH Customer360 {
        /*
          Calculates the Jaccard Neighborhood Similarity between all vertices using a common feature vertex type.
            Jaccard Similarity = intersection_size / (setSize_all + setSize_self - intersection_size)

          Parameters :
            top_k   : # of top scores to report for each vertex
            v_type  : vertex type to compare
            feat_v_type  : feature vertex type
            e_type  : edge type from source vertex to feature vertex type
            re_type  : edge type from feature vertex to source vertex
            similarity_edge : edge type for storing vertex-vertex similarity scores
            src_batch_num  : how many batches to split the source vertices into
            nbor_batch_num : how many batches to split the 2-hop neighbor vertices into
            print_accum : print JSON output
            print_limit : number of source vertices to print, -1 to print all
            file_path : file to write CSV output to
        ,*/

        TYPEDEF TUPLE<VERTEX <Profile> ver, FLOAT val> Res_Tup; // storing results in the Heap
        MapAccum<VERTEX <Profile>, INT> @@set_size_map, @intersection_size_map; // set sizes of all vertices
        SetAccum<STRING> @@all_e_types_set;
        SumAccum<FLOAT> @sum_outdegree;
        HeapAccum<Res_Tup>(top_k, val DESC) @sim_heap; // stores topK similarity results
        FILE f (file_path);
        INT print_count;

        all_vertices = {Profile.*};
        all_vertices = SELECT s
                       FROM all_vertices:s -(e_type:e)- feat_v_type:t
                       ACCUM
                           s.@sum_outdegree += 1;

        FOREACH i IN RANGE[0, src_batch_num-1] DO
            // store number of features for each source vertex
            src_batch = SELECT s
                        FROM all_vertices:s
                        WHERE getvid(s) % src_batch_num == i
                        ACCUM
                            @@set_size_map += (s -> s.@sum_outdegree);

            // store number of source vertices that share common features
            common_features = SELECT t
                              FROM src_batch:s-(e_type:e)-feat_v_type:t
                              ACCUM t.@intersection_size_map += (s -> 1);

            FOREACH j IN RANGE[0, nbor_batch_num-1] DO
                others = SELECT t
                         FROM common_features:s-(re_type:e)-Profile:t
                         WHERE getvid(t) % nbor_batch_num == j
                         ACCUM
                             t.@intersection_size_map += s.@intersection_size_map;
                others = SELECT s
                         FROM others:s
                         ACCUM
                             // perform similarity computation and store results
                             FLOAT div = 0,
                             FOREACH (k,v) IN s.@intersection_size_map DO
                                 IF k == s THEN
                                     CONTINUE
                                 END,
                                 div = @@set_size_map.get(k) + s.@sum_outdegree - v,
                                 IF div > 0 THEN
                                     k.@sim_heap += Res_Tup(s, v/div)
                                 END
                             END
                         POST-ACCUM
                             s.@intersection_size_map.clear();
            END;

            IF print_accum == TRUE THEN
                IF print_limit == -1 THEN
                    PRINT src_batch[src_batch.@sim_heap];
                ELSE
                    IF print_count < print_limit THEN
                        print_batch = SELECT s
                                      FROM src_batch:s
                                      LIMIT print_limit - print_count;
                        print_count = print_count + src_batch.size();
                        PRINT print_batch[print_batch.@sim_heap];
                    END;
                END;
            END;

            src_batch = SELECT s
                        FROM src_batch:s
                        POST-ACCUM
                            FOREACH tup IN s.@sim_heap DO
                                IF file_path != "" THEN
                                    f.println(s, tup.ver, tup.val)
                                END,
                                INSERT INTO EDGE community_edge VALUES (s, tup.ver, tup.val)
                            END,
                            s.@sim_heap.clear();
                            @@set_size_map.clear();
        END;
      }



      CREATE QUERY community_step2_label_prop (SET<STRING> v_type, SET<STRING> e_type, INT max_iter, INT output_limit, BOOL print_accum = TRUE, STRING file_path = "", STRING attr = "") FOR GRAPH Customer360 {
      # Partition the vertices into communities, according to the Label Propagation method.
      # Indicate community membership by assigning each vertex a community ID.

      OrAccum @@or_changed = true;
      MapAccum<INT, INT> @map;     # <communityId, numNeighbors>
      MapAccum<INT, INT> @@comm_sizes_map;   # <communityId, members>
      SumAccum<INT> @sum_label, @sum_num;
      FILE f (file_path);
      Start = {v_type};

      # Assign unique labels to each vertex
      Start = SELECT s
              FROM Start:s
              ACCUM s.@sum_label = getvid(s);

      # Propagate labels to neighbors until labels converge or the max iterations is reached
      WHILE @@or_changed == true LIMIT max_iter DO
          @@or_changed = false;
          Start = SELECT s
                  FROM Start:s -(e_type:e)-> :t
                  ACCUM t.@map += (s.@sum_label -> 1)  # count the occurrences of neighbor's labels
                  POST-ACCUM
                      INT max_v = 0,
                      INT label = 0,
                      # Iterate over the map to get the neighbor label that occurs most often
                      FOREACH (k,v) IN t.@map DO
                          CASE WHEN v > max_v THEN
                              max_v = v,
                              label = k
                          END
                      END,
                      # When the neighbor search finds a label AND it is a new label
                      # AND the label's count has increased, update the label.
                      CASE WHEN label != 0 AND t.@sum_label != label AND max_v > t.@sum_num THEN
                          @@or_changed += true,
                          t.@sum_label = label,
                          t.@sum_num = max_v
                      END,
                      t.@map.clear();
      END;

      Start = {v_type};
      Start =  SELECT s
               FROM Start:s
               POST-ACCUM
                   IF attr != "" THEN
                       s.setAttr(attr, s.@sum_label)
                   END,

                   IF file_path != "" THEN
                       f.println(s, s.@sum_label)
                   END,

                   IF print_accum THEN
                       @@comm_sizes_map += (s.@sum_label -> 1)
                   END
               LIMIT output_limit;

      IF print_accum THEN
          PRINT @@comm_sizes_map;
          PRINT Start[Start.@sum_label];
      END;
      }

      INSTALL QUERY community_step1_jaccard_batch
      INSTALL QUERY community_step2_label_prop



      CREATE QUERY deleteCust360_data(/* Parameters here */) FOR GRAPH Customer360 {
        brow = {Browser.*};
        DELETE v FROM brow:v;

        dasr = {DataSource.*};
        DELETE v FROM dasr:v;

        camp = {Campaign.*};
        DELETE v FROM camp:v;

        ad = {Ads.*};
        DELETE v FROM ad:v;

        prom = {Promotions.*};
        DELETE v FROM prom:v;

        loc = {Location.*};
        DELETE v FROM loc:v;

        urlp = {urlPages.*};
        DELETE v FROM urlp:v;

        even = {Events.*};
        DELETE s FROM even:s;

        sess = {Sessions.*};
        DELETE s FROM sess:s;

        ordr = {Orders.*};
        DELETE s FROM ordr:s;

        prof = {Profile.*};
        DELETE s FROM prof:s;

        devc = {Device.*};
        DELETE s FROM devc:s;

        emai = {Email.*};
        DELETE s FROM emai:s;

        phon = {Phone.*};
        DELETE s FROM phon:s;

        ipad = {IpAddress.*};
        DELETE s FROM ipad:s;

        addr = {Address.*};
        DELETE v FROM addr:v;

        book = {Books.*};
        DELETE v FROM book:v;

        movi = {Movies.*};
        DELETE v FROM movi:v;

        genr = {Genre.*};
        DELETE v FROM genr:v;

        publ = {Publisher.*};
        DELETE v FROM publ:v;

        auth = {Author.*};
        DELETE v FROM auth:v;

        prod = {Products.*};
        DELETE v FROM prod:v;

        stor = {Store.*};
        DELETE v FROM stor:v;

        usag = {UserAgent.*};
        DELETE v FROM usag:v;


      }

      INSTALL QUERY deleteCust360_data

    #+end_src
** DONE CustExp / Customer 360 / Customer Attribution SFDC
   :LOGBOOK:
   - State "DONE"       from "STARTED"    [2021-10-11 Mon 18:56]
   CLOCK: [2021-10-11 Mon 15:50]--[2021-10-11 Mon 18:56] =>  3:06
   CLOCK: [2021-09-30 Thu 18:30]--[2021-09-30 Thu 18:59] =>  0:29
   - State "STARTED"    from "TODO"       [2021-09-30 Thu 16:18]
   CLOCK: [2021-09-30 Thu 16:18]--[2021-09-30 Thu 18:30] =>  2:12
   :END:
*** Resources
     + data - [[file:/data/data-files/graph-data/cust-360/custattribution_sfdc-data.tar.gz]]
     + GraphStudio export of TGCloud graph: [[file:/data/data-files/graph-data/cust-360/cust-attribution.tar.gz]]
*** Graph creation

     #+begin_src sql :tangle solutions/custexp/01-create-graph.gsql :mkdirp yes
       USE GLOBAL
       DROP GRAPH CustExp

       create graph CustExp()

       use graph CustExp

       create schema_change job custexp_schema for graph CustExp {

              ADD VERTEX Account(PRIMARY_ID Account_id STRING,
                                    Name STRING,
                                    Parentid STRING,
                                    BillingStreet STRING,
                                    BillingCity STRING,
                                    BillingState STRING,
                                    BillingPostalCode STRING,
                                    BillingCountry STRING,
                                    Phone STRING,
                                    Website STRING,
                                    Industry STRING,
                                    AnnualRevenue FLOAT,
                                    NumberofEmployees INT,
                                    Description STRING,
                                    AccountOwnerid STRING,
                                    CreatedDate DATETIME,
                                    CreatedByid STRING,
                                    LastModifiedDate DATETIME,
                                    LastModifiedByid STRING,
                                    LastActivityDate DATETIME,
                                    AccountSource STRING)
                                    WITH STATS="OUTDEGREE_BY_EDGETYPE",
                                    PRIMARY_ID_AS_ATTRIBUTE="false";

              ADD VERTEX Campaign(PRIMARY_ID Campaign_id STRING,
                                     Name STRING,
                                     Parentid STRING,
                                     Campaign_Type STRING,
                                     Status STRING,
                                     StartDate DATETIME,
                                     EndDate DATETIME,
                                     BudgetedCost FLOAT,
                                     ActualCost FLOAT,
                                     IsActive BOOL,
                                     Description STRING,
                                     Number_Of_Leads INT,
                                     Number_Of_Converted_Leads INT,
                                     Number_Of_Responses INT,
                                     Number_Of_Opportunities INT,
                                     Number_Of_Won_Opportunities INT,
                                     Amount_All_Opportunities FLOAT,
                                     Amount_Won_Opportunities FLOAT,
                                     Hierarchy_Number_Of_Leads INT,
                                     Hierarchy_Number_Of_Converted_Leads INT,
                                     Hierarchy_Number_Of_Contacts INT,
                                     Hierarchy_Number_Of_Responses INT,
                                     Hierarchy_Number_Of_Opportunities INT,
                                     Hierarchy_Number_Of_Won_Opportunities INT,
                                     Hierarchy_Amount_Won_Opportunities FLOAT,
                                     Hierarchy_Amount_All_Opportunities FLOAT,
                                     Hierarchy_Budgeted_Cost FLOAT,
                                     Hierarchy_Actual_Cost FLOAT,
                                     Campaign_Owner_id STRING,
                                     CreatedDate DATETIME,
                                     LastModifiedDate DATETIME,
                                     LastModifiedByid STRING)
                            WITH STATS="OUTDEGREE_BY_EDGETYPE",
                            PRIMARY_ID_AS_ATTRIBUTE="false";

              ADD VERTEX Contact(PRIMARY_ID Contact_id STRING,
                                    FirstName STRING,
                                    LastName STRING,
                                    Phone STRING,
                                    Email STRING,
                                    Title STRING,
                                    Department STRING,
                                    LeadSource STRING,
                                    Description STRING,
                                    Contact_Ownerid STRING,
                                    Has_Opted_Out_Of_Email BOOL,
                                    DoNotCall BOOL,
                                    CreatedDate DATETIME,
                                    CreatedByid STRING,
                                    LastModifiedDate DATETIME,
                                    LastModifiedByid STRING,
                                    Free_Trial_Start_date DATETIME,
                                    Free_Trial_Status STRING,
                                    Signed_Up_For_Free_Trial_On DATETIME,
                                    Dev_edition_Date_Signed_Up DATETIME,
                                    Employee_Band STRING,
                                    Original_Lead_Source STRING)
                            WITH STATS="OUTDEGREE_BY_EDGETYPE",
                            PRIMARY_ID_AS_ATTRIBUTE="false";

              ADD VERTEX Lead(PRIMARY_ID Lead_id STRING,
                                 FirstName STRING,
                                 LastName STRING,
                                 Title STRING,
                                 Company STRING,
                                 City STRING,
                                 State STRING,
                                 LeadSource STRING,
                                 Status STRING,
                                 Industry STRING,
                                 Ownerid STRING,
                                 HasOptedOutOfEmail BOOL,
                                 IsConverted BOOL,
                                 ConvertedDate DATETIME,
                                 ConvertedAccountId STRING,
                                 ConvertedContactId STRING,
                                 ConvertedOpportunityId STRING,
                                 IsUnreadByOwner BOOL,
                                 CreatedDate DATETIME,
                                 CreatedById STRING,
                                 LastModifiedDate DATETIME,
                                 LastModifiedById STRING,
                                 LastActivityDate DATETIME,
                                 DoNotCall BOOL,
                                 LastTransferDate DATETIME,
                                 Free_Trial_License_Key__c STRING,
                                 Free_Trial_Start_Date__c DATETIME,
                                 Signed_up_for_free_trial_on__c DATETIME,
                                 Agree_to_FT_LicenseAgreement__c BOOL,
                                 Free_Trial_Project_Notes__c STRING,
                                 Free_Trial_Follow_Up_Notes__c STRING,
                                 Started_Test_Drive__c BOOL,
                                 Free_Trial_Status__c STRING,
                                 LinkedIn_Profile__c STRING,
                                 Dev_Edition_Agree_to_License_Agreement__c BOOL,
                                 Dev_Edition_Date_Signed_Up__c DATETIME,
                                 Free_Trial_Renewed_Date__c DATETIME,
                                 Goals_of_Developer_Edition__c STRING,
                                 Goal_of_Developer_Edition_Other__c STRING,
                                 Employee_Band__c STRING,
                                 Are_you_familar_with_Graph_db__c STRING,
                                 Competitor_Notes__c STRING,
                                 Use_Graph_Score__c STRING,
                                 What_s_your_interest_in_TigerGraph__c STRING,
                                 Interest_Notes__c STRING,
                                 Interest_Score__c STRING,
                                 What_capabilities_are_you_looking_for__c STRING,
                                 Do_you_have_a_timeline__c STRING,
                                 Contacts_Role__c STRING,
                                 Title_Rank__c STRING,
                                 Timeline_Score__c INT,
                                 Role_Score__c INT,
                                 Title_Score__c INT,
                                 How_do_you_want_to_deploy_o__c STRING,
                                 SQL_SCORE__c INT,
                                 Original_Lead_Source__c STRING,
                                 Event_Notes_L__c STRING,
                                 DiscoverOrg_EmployeeID__c STRING,
                                 DiscoverOrg_CompanyID__c STRING)
                        WITH STATS="OUTDEGREE_BY_EDGETYPE",
                        PRIMARY_ID_AS_ATTRIBUTE="false";

               ADD VERTEX CampaignMember(PRIMARY_ID CampaignMemberid STRING,
                                             IsDeleted BOOL,
                                             CampaignId STRING,
                                             LeadId STRING,
                                             ContactId STRING,
                                             Status STRING,
                                             HasResponded BOOL,
                                             IsPrimary BOOL,
                                             CreatedDate DATETIME,
                                             CreatedById STRING,
                                             LastModifiedDate DATETIME,
                                             LastModifiedById STRING,
                                             SystemModstamp STRING,
                                             FirstRespondedDate DATETIME)
                              WITH STATS="OUTDEGREE_BY_EDGETYPE",
                              PRIMARY_ID_AS_ATTRIBUTE="false";

               ADD VERTEX Opportunity(PRIMARY_ID Opportunity_id STRING,
                                         IsDeleted BOOL,
                                         AccountID STRING,
                                         IsPrivate BOOL,
                                         Name STRING,
                                         Description STRING,
                                         StageName STRING,
                                         StageSortOrder INT,
                                         Amount FLOAT,
                                         Probability FLOAT,
                                         ExpectedRevenue FLOAT,
                                         CloseDate DATETIME,
                                         Opportunity_Type STRING,
                                         NextStep STRING,
                                         LeadSource STRING,
                                         IsClosed BOOL,
                                         IsWon BOOL,
                                         ForecastCategory STRING,
                                         ForecastCategoryName STRING,
                                         CampaignId STRING,
                                         HasOpportunityLineItem BOOL,
                                         Pricebook2Id STRING,
                                         Ownerid STRING,
                                         CreatedDate DATETIME,
                                         CreatedById STRING,
                                         LastModified DATETIME,
                                         LastModifiedById STRING,
                                         SystemModstamp STRING,
                                         LastActivityDate DATETIME,
                                         LastStageChangeDate DATETIME,
                                         FiscalYear STRING,
                                         FiscalQuarter STRING,
                                         Budget_Confirmed__c BOOL,
                                         Discovery_Completed__c BOOL,
                                         ROI_Analysis_Completed__c BOOL,
                                         Referral_Partner_Company__c STRING,
                                         Stage_Moved_to_POC__c DATETIME,
                                         Are_you_familiar_with_Graph_db_O__c STRING,
                                         Competitor_Notes_O__c STRING,
                                         Contacts_Role_o__c STRING,
                                         Do_you_have_a_timeline__c STRING,
                                         How_do_you_want_to_deploy_o__c STRING,
                                         Interest_Notes_o__c STRING,
                                         Interest_Score_o__c INT,
                                         Role_Score_o__c INT,
                                         SQL_SCORE_o__c INT,
                                         Timeline_Score_o__c INT,
                                         Title_Rank__c STRING,
                                         Title_Score_o__c INT,
                                         Use_Graph_Score_o__c INT)
                                WITH STATS="OUTDEGREE_BY_EDGETYPE",
                                PRIMARY_ID_AS_ATTRIBUTE="false";

                 ADD VERTEX Industry(PRIMARY_ID id STRING) WITH STATS="OUTDEGREE_BY_EDGETYPE", PRIMARY_ID_AS_ATTRIBUTE="false";

                 ADD VERTEX LeadSource(PRIMARY_ID id STRING) WITH STATS="OUTDEGREE_BY_EDGETYPE", PRIMARY_ID_AS_ATTRIBUTE="false";

                 ADD DIRECTED EDGE belongs_to(FROM Contact, TO Account) WITH REVERSE_EDGE="reverse_belongs_to";

                 ADD DIRECTED EDGE converted(FROM Lead, TO Contact) WITH REVERSE_EDGE="reverse_converted";

                 ADD DIRECTED EDGE is_active_as(FROM Lead, TO CampaignMember) WITH REVERSE_EDGE="reverse_is_active_as";

                 ADD DIRECTED EDGE is_part_of(FROM CampaignMember, TO Campaign) WITH REVERSE_EDGE="reverse_is_part_of";

                 ADD DIRECTED EDGE Has_Role(FROM Contact, TO Opportunity, role STRING, id STRING) WITH REVERSE_EDGE="reverse_Has_Role";

                 ADD DIRECTED EDGE Has(FROM Account, TO Opportunity) WITH REVERSE_EDGE="reverse_Has";

                 ADD DIRECTED EDGE Is_Driven_By(FROM Opportunity, TO Campaign) WITH REVERSE_EDGE="reverse_Is_Driven_By";

                 ADD DIRECTED EDGE is_connected_to(FROM Contact, TO CampaignMember) WITH REVERSE_EDGE="reverse_is_connected_to";

                 ADD DIRECTED EDGE belongs_to_industry(FROM Account, TO Industry) WITH REVERSE_EDGE="reverse_belongs_to_industry";

                 ADD DIRECTED EDGE created_by(FROM Account, TO LeadSource) WITH REVERSE_EDGE="reverse_created_by";

                 ADD DIRECTED EDGE is_from(FROM Contact, TO LeadSource) WITH REVERSE_EDGE="reverse_is_from";

                 ADD DIRECTED EDGE comes_from(FROM LeadSource, TO Lead) WITH REVERSE_EDGE="reverse_comes_from";

                 ADD DIRECTED EDGE comes_from_the(FROM Lead, TO Industry) WITH REVERSE_EDGE="reverse_comes_from_the";

                 ADD DIRECTED EDGE is_for_the(FROM Opportunity, TO Industry) WITH REVERSE_EDGE="reverse_is_for_the";

       }

       run schema_change job custexp_schema

       drop job custexp_schema
    #+end_src
*** data loading
     #+begin_src sql :tangle solutions/custexp/02-load-data.gsql
       use graph CustExp
       drop job load_job_custexp

       CREATE LOADING JOB load_job_custexp FOR GRAPH CustExp {
         DEFINE FILENAME Lead="m1:/home/tigergraph/mydata/custexp/data/Lead.csv";
         DEFINE FILENAME OpportunityContactRole="m1:/home/tigergraph/mydata/custexp/data/OpportunityContactRole.csv";
         DEFINE FILENAME Campaign="m1:/home/tigergraph/mydata/custexp/data/Campaign.csv";
         DEFINE FILENAME CampaignMember="m1:/home/tigergraph/mydata/custexp/data/CampaignMember.csv";
         DEFINE FILENAME Opportunity="m1:/home/tigergraph/mydata/custexp/data/Opportunity.csv";
         DEFINE FILENAME Account="m1:/home/tigergraph/mydata/custexp/data/Account.csv";
         DEFINE FILENAME Contact="m1:/home/tigergraph/mydata/custexp/data/Contact.csv";

         LOAD Lead TO VERTEX Lead VALUES($0, $4, $5, _, $9, $11, $12, $24, $25, $26, $30, $31, $32, $33, $34, $35, $36, $37, $38, $39, $40, $41, $43, $44, $46, $64, $65, $66, $67, $68, $69, $70, $71, $74, $57, $76, $77, $88, $89, $90, $91, $92, $93, $94, $95, $96, $97, $98, $99, $100, $101, $102, $103, $104, $105, $107, $109, $111, $112) USING SEPARATOR=",", HEADER="true", EOL="\n";

         LOAD Lead TO EDGE comes_from VALUES($24, $0) USING SEPARATOR=",", HEADER="true", EOL="\n";

         LOAD Lead TO EDGE converted VALUES($0, $35) USING SEPARATOR=",", HEADER="true", EOL="\n";

         LOAD Lead TO EDGE comes_from_the VALUES($0, $26) USING SEPARATOR=",", HEADER="true", EOL="\n";

         LOAD OpportunityContactRole TO EDGE Has_Role VALUES($2, $1, $3, $0) USING SEPARATOR=",", HEADER="true", EOL="\n";


         LOAD Campaign TO VERTEX Campaign VALUES($0, $2, $3, $4, $5, $6, $7, $9, $10, $13, $14, $15, $16, $18, $19, $20, $21, $22, $23, $24, $25, $26, $27, $28, $29, $30, $33, $34, $35, $36, $38, $39) USING SEPARATOR=",", HEADER="true", EOL="\n";


         LOAD CampaignMember TO VERTEX CampaignMember VALUES($0, $1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13) USING SEPARATOR=",", HEADER="true", EOL="\n";
         LOAD CampaignMember TO EDGE is_part_of VALUES($0, $2) USING SEPARATOR=",", HEADER="true", EOL="\n";
         LOAD CampaignMember TO EDGE is_connected_to VALUES($4, $0) USING SEPARATOR=",", HEADER="true", EOL="\n";
         LOAD CampaignMember TO EDGE is_active_as VALUES($3, $0) USING SEPARATOR=",", HEADER="true", EOL="\n";

         LOAD Opportunity TO VERTEX Opportunity VALUES($0, $1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $12, $13, $14, $15, $16, $17, $18, $19, $20, $21, $22, $23, $24, $25, $26, $27, $28, $29, $30, $31, $32, $36, $37, $38, $41, $43, $45, $46, $47, $48, $49, $50, $51, $52, $53, $54, $55, $56, $57) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
         LOAD Opportunity TO EDGE Has VALUES($2, $0) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
         LOAD Opportunity TO EDGE Is_Driven_By VALUES($0, $20) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";


         LOAD Account TO VERTEX Account VALUES($0, $3, $5, $6, $7, $8, $9, $10, $22, $25, $27, $28, $29, $32, $35, $36, $37, $38, $39, $41, $44) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
         LOAD Account TO VERTEX Industry VALUES($27) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
         LOAD Account TO VERTEX LeadSource VALUES($44) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
         LOAD Account TO EDGE belongs_to_industry VALUES($0, $27) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
         LOAD Account TO EDGE created_by VALUES($0, $44) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";


         LOAD Contact TO VERTEX Contact VALUES($0, $4, $5, $13, $16, $17, $18, $19, $20, $21, $22, $23, $24, $25, $26, $27, $30, $31, $32, $33, $35, $36) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
         LOAD Contact TO VERTEX LeadSource VALUES($19) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
         LOAD Contact TO EDGE belongs_to VALUES($0, $2) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
         LOAD Contact TO EDGE is_from VALUES($0, $19) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
       }

       run loading job load_job_custexp

       drop job load_job_custexp
     #+end_src

*** Queries
      #+begin_src sql :tangle solutions/custexp/03-create-queries.gsql
        USE GRAPH CustExp

        CREATE QUERY CustomerJourney(vertex<Contact> customer, set<string> campaignTypes, datetime startTime, datetime endTime) FOR GRAPH CustExp {
        /*
          Sample input:
                Contact: Sam-Eisenberg
                startTime: 2018-06-01
                endTime: 2018-10-01
        ,*/

            SumAccum<string> @camType, @camName, @camDesc;
            Customer = { customer };
          print Customer;
            Company = select t from Customer -(belongs_to)-> Account:t;
            print Company;
            campaign = select c
                       from Customer-(is_connected_to)-> CampaignMember:c
                       where c.CreatedDate >= startTime and c.CreatedDate <= endTime
                       ;
            campaign = select c from campaign:c -(is_part_of)-> Campaign:t
                       where campaignTypes.size() == 0 or t.Campaign_Type in campaignTypes
                       accum c.@camType = t.Campaign_Type,
                             c.@camName = t.Name,
                             c.@camDesc = t.Description;
            print campaign as Campaign;
        }
        CREATE QUERY SimilarCustomers(VERTEX<Contact> sourceCustomer, set<string> campaignTypes, INT topK) FOR GRAPH CustExp {
        /*
          This query calculates the Jaccard Similarity between a given customer (of type Contact) and
          every other customers (or Contacts) who share similar campaigns (of type Campaign).
            The topK "similar" customers are printed out.

            SAMPLE INPUT:
                Contact: Sam-Eisenberg
                campaignTypes:
                          Webinar
                                    Demo Signup / Trial
                  topK: 5

          A Jaccard Similarity score is calculated for each similar customer (who share similar Campaigns
                              with the input sourceCustomer)
          The set of similar customers is sorted with the topK # customers printed out.

          Jaccard similarity = intersection_size / (size_A + size_B - intersection_size)

            More info:
              How to find Jaccard similarity?
                  https://www.youtube.com/watch?v=5RRyzjvC5z4
              Similarity Algorithms in GSQL
                  https://github.com/tigergraph/gsql-graph-algorithms/tree/master/algorithms/examples/Similarity
        ,*/
                SumAccum<INT> @intersection_size, @@set_size_A, @set_size_B;
                SumAccum<FLOAT> @similarity;

                A(ANY) = {sourceCustomer};

                A = SELECT s
                      FROM A:s
                    ACCUM @@set_size_A += s.outdegree("is_connected_to");

                // From A set (Contact), traverse 'is_connected_to' edges to all CampaignMember s
                CampaignMembersSet = SELECT t
                    FROM A:s -(is_connected_to:e)-> CampaignMember:t;

                  // From CampaignMember s, traverse 'is_part_of' edges to Campaign s, for all
                  // desired campaignTypes (eg. Webinar, Website Direct, Demo Signup/Trial)
                CampaignSet = SELECT t
                    FROM CampaignMembersSet -(is_part_of:e)-> Campaign:t
                      WHERE campaignTypes.size() == 0 OR (t.Campaign_Type in campaignTypes);

                  // From Campaign s, traverse 'reverse_is_part_of' edges back to all CampaignMember s
                CampaignMembersSet = SELECT t
                    FROM CampaignSet:s -(reverse_is_part_of:e)-> CampaignMember:t;

                  // From CampaignMember s, traverse 'reverse_is_connected_to' edges back to Contacts (B set)
                  // For each Contact in B set, accumulate the intersection size of the shared Campaigns, and
                  //   compute it's Jaccard Similarity score as
                  //     Jaccard similarity = intersection_size / size of the Union of (A set + B set)
                  //                        = intersection_size / (size_A + size_B - intersection_size)
                B = SELECT t
                    FROM CampaignMembersSet:s -(reverse_is_connected_to:e)-> Contact:t
                    WHERE t != sourceCustomer
                    ACCUM t.@intersection_size += 1,
                               t.@set_size_B = t.outdegree("is_connected_to")
                    POST-ACCUM t.@similarity = t.@intersection_size*1.0/
                                                (@@set_size_A + t.@set_size_B - t.@intersection_size)
                    ORDER BY t.@similarity DESC
                    LIMIT topK;

                //PRINT B;
                  PRINT B[B.FirstName, B.LastName, B.@similarity];
        }
        CREATE DISTRIBUTED QUERY CustJourney_Subgraph(vertex<Contact> customer, vertex<Opportunity> opportunity) FOR GRAPH CustExp {
        /*
          Sample input:
              Contact: Sam-Eisenberg
              opportunity: 0063600000gEoe0AAC

        ,*/
            SetAccum<edge> @@displaySet;
          SetAccum<vertex> @@vertexSet;

            cust = { customer };

            acct = select t from cust:c -(belongs_to:e)-> Account:t
                   accum @@displaySet += e, @@vertexSet += t;

          opp = select t from cust -(Has_Role:e)-> Opportunity:t
                  accum @@displaySet += e, @@vertexSet += t;

            campaign_members =
                    select t
                    from cust -(is_connected_to:e)-> CampaignMember:t
                    accum @@vertexSet += cust, @@vertexSet += t, @@displaySet += e;

            campaigns = select t from campaign_members -(is_part_of:e)-> Campaign:t
                        accum @@vertexSet += t, @@displaySet += e;

            Verts = @@vertexSet;

            print Verts;
            //print@@vertexSet;

            print @@displaySet;
        }
     #+end_src
*** Install on current TigerGraph cluster
    #+begin_src emacs-lisp
    #+end_src

** Crunchbase enterprise knowledge
*** Resources
    + Data file: s3://gjgeksbackup/crunchbase-data.tgz or /data/data-files/graph-data/crunchbase-data.tgz
*** Create graph
    #+begin_src sql :tangle solutions/crunchbase/01-create-graph.gsql :mkdirp yes
      USE GLOBAL
      DROP GRAPH CrunchBasePre_2013

      CREATE GRAPH CrunchBasePre_2013()
      USE GRAPH CrunchBasePre_2013
      CREATE SCHEMA_CHANGE JOB schema_crunchbase FOR GRAPH CrunchBasePre_2013 {
        ADD VERTEX company(PRIMARY_ID id STRING, name STRING, normalized_name STRING, permalink STRING, category_code STRING, status STRING, founded_at DATETIME, closed_at DATETIME, domain STRING, homepage_url STRING, twitter_username STRING, logo_url STRING, short_description STRING, description STRING, overview STRING, tag_list STRING, country STRING, state STRING, city STRING, region STRING, first_investment_at DATETIME, last_investment_at DATETIME, first_funding_at DATETIME, last_funding_at DATETIME, funding_rounds UINT, funding_total_usd DOUBLE, first_milestone_at DATETIME, last_milestone_at DATETIME, relationships UINT, created_by STRING, created_at DATETIME, update_at DATETIME) WITH STATS="OUTDEGREE_BY_EDGETYPE", PRIMARY_ID_AS_ATTRIBUTE="false";
        ADD VERTEX person(PRIMARY_ID id STRING, fullname STRING, normalized_name STRING, firstname STRING, lastname STRING, birthplace STRING, affiliation_name STRING, permalink STRING, status STRING, domain STRING, homepage_url STRING, twitter_username STRING, logo_url STRING, overview STRING, tag_list STRING, first_milestone_at DATETIME, last_milestone_at DATETIME, created_by STRING, created_at DATETIME, updated_at DATETIME) WITH STATS="OUTDEGREE_BY_EDGETYPE", PRIMARY_ID_AS_ATTRIBUTE="false";
        ADD VERTEX financialORG(PRIMARY_ID id STRING, name STRING, normalized_name STRING, permalink STRING, category_code STRING, status STRING DEFAULT "operating", founded_at DATETIME, closed_at DATETIME, domain STRING, homepage_url STRING, twitter_username STRING, logo_url STRING, short_description STRING, description STRING, overview STRING, tag_list STRING, country_code STRING, state_code STRING, city STRING, region STRING, first_investment_at DATETIME, last_investment_at DATETIME, first_funding_at DATETIME, last_funding_at DATETIME, funding_rounds INT, funding_total_usd DOUBLE, first_milestone_at DATETIME, last_milestone_at DATETIME, relationships INT, created_by STRING, created_at DATETIME, updated_at DATETIME) WITH STATS="OUTDEGREE_BY_EDGETYPE", PRIMARY_ID_AS_ATTRIBUTE="false";
        ADD VERTEX product(PRIMARY_ID id STRING, name STRING, normalized_name STRING, permalink STRING, status STRING DEFAULT "operating", founded_at DATETIME, closed_at DATETIME, domain STRING, homepage_url STRING, twitter_username STRING, logo_url STRING, overview STRING, tag_list STRING, created_at DATETIME, updates_at DATETIME) WITH STATS="OUTDEGREE_BY_EDGETYPE", PRIMARY_ID_AS_ATTRIBUTE="false";
        ADD VERTEX university(PRIMARY_ID id STRING, name STRING) WITH STATS="OUTDEGREE_BY_EDGETYPE", PRIMARY_ID_AS_ATTRIBUTE="false";
        ADD VERTEX IPO(PRIMARY_ID id UINT, valuation_amount DOUBLE, valuation_currency_code STRING, raised_amount DOUBLE, raised_currency_code STRING, public_at DATETIME, stock_symbol STRING, source_url STRING, source_description STRING, created_at DATETIME, updated_at DATETIME) WITH STATS="OUTDEGREE_BY_EDGETYPE", PRIMARY_ID_AS_ATTRIBUTE="false";
        ADD VERTEX funding_rounds(PRIMARY_ID id UINT, funded_at DATETIME, funding_round_type STRING, funding_round_code STRING, raised_amount_usd DOUBLE, raised_amount DOUBLE, raised_currency_code STRING, pre_money_valuation_usd DOUBLE, pre_money_valuation DOUBLE, pre_money_currency_code STRING, post_money_valuation_usd DOUBLE, post_money_valuation DOUBLE, post_money_currency_code STRING, is_first_round INT, is_last_round INT, source_url STRING, source_description STRING, created_by STRING, created_at DATETIME, updated_at DATETIME) WITH STATS="OUTDEGREE_BY_EDGETYPE", PRIMARY_ID_AS_ATTRIBUTE="false";
        ADD VERTEX office(PRIMARY_ID id UINT, description STRING, region STRING, address1 STRING, address2 STRING, city STRING, zip_code STRING, state_code STRING, country_code STRING, latitude DOUBLE, longitude DOUBLE) WITH STATS="OUTDEGREE_BY_EDGETYPE", PRIMARY_ID_AS_ATTRIBUTE="false";
        ADD VERTEX funds(PRIMARY_ID id UINT, name STRING, funded_at DATETIME, raised_amount DOUBLE, raised_currency_code STRING, source_url STRING, source_description STRING, created_at DATETIME, updated_at DATETIME) WITH STATS="OUTDEGREE_BY_EDGETYPE", PRIMARY_ID_AS_ATTRIBUTE="false";
        ADD VERTEX milestone(PRIMARY_ID id UINT, milestone_at DATETIME, milestone_code STRING, description STRING, source_url STRING, source_description STRING, created_at DATETIME, updated_at DATETIME) WITH STATS="OUTDEGREE_BY_EDGETYPE", PRIMARY_ID_AS_ATTRIBUTE="false";

        ADD UNDIRECTED EDGE work_for_company(FROM person, TO company, start_at DATETIME, end_at DATETIME, is_past INT, sequence INT DEFAULT "0", title STRING, created_at DATETIME, updated_at DATETIME);
        ADD UNDIRECTED EDGE hasDegree(FROM person, TO university, degree_type STRING, subject STRING, graduated_at DATETIME, created_at DATETIME, updated_at DATETIME);
        ADD UNDIRECTED EDGE company_ipo(FROM company, TO IPO);
        ADD UNDIRECTED EDGE company_funding_rounds(FROM company, TO funding_rounds);
        ADD UNDIRECTED EDGE financial_funds(FROM financialORG, TO funds);
        ADD UNDIRECTED EDGE company_office(FROM company, TO office);
        ADD UNDIRECTED EDGE financial_office(FROM financialORG, TO office);
        ADD UNDIRECTED EDGE company_product(FROM company, TO product);
        ADD UNDIRECTED EDGE person_milestone(FROM person, TO milestone);
        ADD UNDIRECTED EDGE company_milestone(FROM company, TO milestone);
        ADD UNDIRECTED EDGE product_milestone(FROM product, TO milestone);
        ADD UNDIRECTED EDGE financial_milestone(FROM financialORG, TO milestone);
        ADD UNDIRECTED EDGE investment_from_person(FROM person, TO funding_rounds);
        ADD UNDIRECTED EDGE investment_from_company(FROM company, TO funding_rounds);
        ADD UNDIRECTED EDGE investment_from_financialORG(FROM financialORG, TO funding_rounds);
        ADD UNDIRECTED EDGE work_for_fOrg(FROM person, TO financialORG, start_at DATETIME, end_at DATETIME, is_past INT, sequence UINT DEFAULT "0", title STRING, created_at DATETIME, updated_at DATETIME);
        ADD UNDIRECTED EDGE invested_by_person(FROM person, TO company);
        ADD UNDIRECTED EDGE invested_by_financialORG(FROM financialORG, TO company);
        ADD DIRECTED EDGE acquire(FROM company, TO company, term_code STRING, price_amout DOUBLE, price_currency_code STRING, acquired_at DATETIME, source_url STRING, source_description STRING, created_at DATETIME, updated_at DATETIME) WITH REVERSE_EDGE="acquired_by";
        ADD DIRECTED EDGE invest(FROM company, TO company) WITH REVERSE_EDGE="invested_by_company";
      }
      RUN SCHEMA_CHANGE JOB schema_crunchbase
      DROP JOB schema_crunchbase
    #+end_src
*** Load data
    #+begin_src sql :tangle solutions/crunchbase/02-load-data.gsql :mkdirp yes
      USE GRAPH CrunchBasePre_2013
      CREATE LOADING JOB load_crunchbase {
        DEFINE FILENAME cb_acquisitions = "m1:/home/tigergraph/mydata/crunchbase/data/cb_acquisitions.csv";
        DEFINE FILENAME cb_company = "m1:/home/tigergraph/mydata/crunchbase/data/cb_company.csv";
        DEFINE FILENAME cb_company_milestones = "m1:/home/tigergraph/mydata/crunchbase/data/cb_company_milestones.csv";
        DEFINE FILENAME cb_company_offices = "m1:/home/tigergraph/mydata/crunchbase/data/cb_company_offices.csv";
        DEFINE FILENAME cb_degrees = "m1:/home/tigergraph/mydata/crunchbase/data/cb_degrees.csv";
        DEFINE FILENAME cb_financialorg = "m1:/home/tigergraph/mydata/crunchbase/data/cb_financialorg.csv";
        DEFINE FILENAME cb_financialorg_milestones = "m1:/home/tigergraph/mydata/crunchbase/data/cb_financialorg_milestones.csv";
        DEFINE FILENAME cb_fOrg_offices = "m1:/home/tigergraph/mydata/crunchbase/data/cb_fOrg_offices.csv";
        DEFINE FILENAME cb_funding_rounds = "m1:/home/tigergraph/mydata/crunchbase/data/cb_funding_rounds.csv";
        DEFINE FILENAME cb_funds = "m1:/home/tigergraph/mydata/crunchbase/data/cb_funds.csv";
        DEFINE FILENAME cb_invested_by_company = "m1:/home/tigergraph/mydata/crunchbase/data/cb_invested_by_company.csv";
        DEFINE FILENAME cb_invested_by_fOrg = "m1:/home/tigergraph/mydata/crunchbase/data/cb_invested_by_fOrg.csv";
        DEFINE FILENAME cb_invested_by_person = "m1:/home/tigergraph/mydata/crunchbase/data/cb_invested_by_person.csv";
        DEFINE FILENAME cb_ipos = "m1:/home/tigergraph/mydata/crunchbase/data/cb_ipos.csv";
        DEFINE FILENAME cb_milestones = "m1:/home/tigergraph/mydata/crunchbase/data/cb_milestones.csv";
        DEFINE FILENAME cb_offices = "m1:/home/tigergraph/mydata/crunchbase/data/cb_offices.csv";
        DEFINE FILENAME cb_people = "m1:/home/tigergraph/mydata/crunchbase/data/cb_people.csv";
        DEFINE FILENAME cb_person = "m1:/home/tigergraph/mydata/crunchbase/data/cb_person.csv";
        DEFINE FILENAME cb_person_milestones = "m1:/home/tigergraph/mydata/crunchbase/data/cb_person_milestones.csv";
        DEFINE FILENAME cb_personwithcompany = "m1:/home/tigergraph/mydata/crunchbase/data/cb_personwithcompany.csv";
        DEFINE FILENAME cb_personwithfOrg = "m1:/home/tigergraph/mydata/crunchbase/data/cb_personwithfOrg.csv";
        DEFINE FILENAME cb_product = "m1:/home/tigergraph/mydata/crunchbase/data/cb_product.csv";
        DEFINE FILENAME cb_product_milestones = "m1:/home/tigergraph/mydata/crunchbase/data/cb_product_milestones.csv";
        DEFINE FILENAME degrees = "m1:/home/tigergraph/mydata/crunchbase/data/degrees.csv";
        DEFINE FILENAME university = "m1:/home/tigergraph/mydata/crunchbase/data/university.csv";
        LOAD cb_people TO VERTEX person VALUES($1, _, _, $2, $3, $4, $5, _, _, _, _, _, _, _, _, _, _, _, _, _) USING SEPARATOR="\t", HEADER="false", EOL="\n";
        LOAD cb_degrees TO EDGE hasDegree VALUES($1, $4, $2, $3, $5, $6, $7) USING SEPARATOR="\t", HEADER="false", EOL="\n";
        LOAD university TO VERTEX university VALUES($0, $1) USING SEPARATOR="\t", HEADER="false", EOL="\n";
        LOAD cb_personwithcompany TO EDGE work_for_company VALUES($2, $3, $4, $5, $6, $7, $8, $9, $10) USING SEPARATOR="\t", HEADER="false", EOL="\n";
        LOAD cb_invested_by_company TO EDGE investment_from_company VALUES($3, $1) USING SEPARATOR="\t", HEADER="false", EOL="\n";
        LOAD cb_invested_by_company TO EDGE invest VALUES($3, $2) USING SEPARATOR="\t", HEADER="false", EOL="\n";
        LOAD cb_person_milestones TO EDGE person_milestone VALUES($1, $0) USING SEPARATOR="\t", HEADER="false", EOL="\n";
        LOAD cb_product TO VERTEX product VALUES($0, $4, $5, $6, $8, $9, $10, $11, $12, $13, $14, $19, $20, $38, $39) USING SEPARATOR="\t", HEADER="false", EOL="\n";
        LOAD cb_product TO EDGE company_product VALUES($3, $0) USING SEPARATOR="\t", HEADER="false", EOL="\n";
        LOAD cb_product_milestones TO EDGE product_milestone VALUES($1, $0) USING SEPARATOR="\t", HEADER="false", EOL="\n";
        LOAD cb_milestones TO VERTEX milestone VALUES($0, $2, $3, $4, $5, $6, $7, $8) USING SEPARATOR="\t", HEADER="false", EOL="\n";
        LOAD cb_company_milestones TO EDGE company_milestone VALUES($1, $0) USING SEPARATOR="\t", HEADER="false", EOL="\n";
        LOAD cb_ipos TO VERTEX IPO VALUES($0, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12) USING SEPARATOR="\t", HEADER="false", EOL="\n";
        LOAD cb_ipos TO EDGE company_ipo VALUES($2, $0) USING SEPARATOR="\t", HEADER="false", EOL="\n";
        LOAD cb_acquisitions TO EDGE acquire VALUES($2, $3, $4, $5, $6, $7, $8, $9, $10, $11) USING SEPARATOR="\t", HEADER="false", EOL="\n";
        LOAD cb_financialorg_milestones TO EDGE financial_milestone VALUES($1, $0) USING SEPARATOR="\t", HEADER="false", EOL="\n";
        LOAD cb_company_offices TO EDGE company_office VALUES($1, $2) USING SEPARATOR="\t", HEADER="false", EOL="\n";
        LOAD cb_offices TO VERTEX office VALUES($2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12) USING SEPARATOR="\t", HEADER="false", EOL="\n";
        LOAD cb_fOrg_offices TO EDGE financial_office VALUES($1, $2) USING SEPARATOR="\t", HEADER="false", EOL="\n";
        LOAD cb_funds TO VERTEX funds VALUES($0, $3, $4, $5, $6, $7, $8, $9, $10) USING SEPARATOR="\t", HEADER="false", EOL="\n";
        LOAD cb_funds TO EDGE financial_funds VALUES($2, $0) USING SEPARATOR="\t", HEADER="false", EOL="\n";
        LOAD cb_financialorg TO VERTEX financialORG VALUES($0, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $17, $18, $19, $20, $21, $22, $23, $24, $25, $26, $29, $30, $31, $32, $33, $34, $36, $37, $38, $39) USING SEPARATOR="\t", HEADER="false", EOL="\n";
        LOAD cb_invested_by_fOrg TO EDGE invested_by_financialORG VALUES($3, $2) USING SEPARATOR="\t", HEADER="false", EOL="\n";
        LOAD cb_invested_by_fOrg TO EDGE investment_from_financialORG VALUES($3, $1) USING SEPARATOR="\t", HEADER="false", EOL="\n";
        LOAD cb_company TO VERTEX company VALUES($0, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $17, $18, $19, $20, $21, $22, $23, $24, $25, $26, $29, $30, $31, $32, $33, $34, $36, $37, $38, $39) USING SEPARATOR="\t", HEADER="false", EOL="\n";
        LOAD cb_funding_rounds TO VERTEX funding_rounds VALUES($0, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12, $13, $14, $16, $17, $18, $19, $20, $21, $22) USING SEPARATOR="\t", HEADER="false", EOL="\n";
        LOAD cb_funding_rounds TO EDGE company_funding_rounds VALUES($2, $0) USING SEPARATOR="\t", HEADER="false", EOL="\n";
        LOAD cb_personwithfOrg TO EDGE work_for_fOrg VALUES($2, $3, $4, $5, $6, $7, $8, $9, $10) USING SEPARATOR="\t", HEADER="false", EOL="\n";
        LOAD cb_invested_by_person TO EDGE invested_by_person VALUES($3, $2) USING SEPARATOR="\t", HEADER="false", EOL="\n";
        LOAD cb_invested_by_person TO EDGE investment_from_person VALUES($3, $1) USING SEPARATOR="\t", HEADER="false", EOL="\n";
        LOAD cb_person TO VERTEX person VALUES($0, $4, $5, _, _, _, _, $6, $8, $11, $12, $13, $14, $19, $20, $33, $34, $37, $38, $39) USING SEPARATOR="\t", HEADER="false", EOL="\n";
      }
      RUN LOADING JOB load_crunchbase
      DROP JOB load_crunchbase

    #+end_src
*** Add queries
    #+begin_src sql :tangle solutions/crunchbase/03-add-queries.gsql :mkdirp yes
      USE GRAPH CrunchBasePre_2013
      CREATE QUERY FindPromisingStartupBasedOnLeader(String roundStage, int k, String sector) FOR GRAPH CrunchBasePre_2013 {
          /*
       DISCLAIMER: Data is from Crunchbase 2013 Snapshot under the Creative Commons Attribution License [CC-BY].
       --- Dataset has been shrunk on Nov.20th,2020 ---

       This query is to help people find the top k promising startups led by sucessful founders,
       within a round cutoff and sector filtering.
       E.g.  If top k is 3, round cutoff is "c", and indurstry sector is "software", we evaluate the company
       based on how many IPOs happened after its founders' contribution to their past companies.

       Given a round_stage, company_sector, and top k to find,
       Step:
          (1) find all companies has IPOed or got Acquired.
          (2) find employees who contributed to the companies in step(1) for IPOed/Acquired.
          (3) find startups whose founders have worked for the companies in step(1) before.
          (4) select startups based on the input round cutoff and category sector.
          (5) Find the top K companies whose founders have the most successful trails.

          Path:
          funding_rounds -> company (check IPO/acquired) -> person -> company -> IPO/acquired

          Sample input
              1.  roundStage: c       k: 3     sector: software
              2.  roundStage: angel   k: 5     sector: mobile
              3.  roundStage: c       k: 3     sector: security
          ,*/
          //declare tuple to combine the information, with relevant company and it's IPO/acquired numbers
          typedef tuple<vertex<company> company, int score> scoreResults;
          //declare heap to keep the top k companies
          HeapAccum<scoreResults>(k,score desc) @@topScoreResults;

          //declare set to store vertex and edge
          SetAccum<vertex> @@tempV;
          SetAccum<vertex> @@verSet;
          SetAccum<edge> @@edgeSet;
          SetAccum<vertex> @parentV;
          SetAccum<edge> @parentE;
          SetAccum<String> @@roundSet;

          //decalre variable to record IPO/acquired numbers
          SumAccum<int> @num;
          //declare variable to tag the vertex
          MinAccum<datetime> @successTime;
          AndAccum @startup = true;
          AndAccum @selectCom = true;
          //declare map to store the pair information with vertex and the datetime limitation
          MapAccum<vertex<person>, MapAccum<vertex<company>, datetime>> @@beforeWorkTime;
          MapAccum<vertex<person>, MapAccum<vertex<company>, edge>> @@beforeWorkEdge;
          ListAccum<vertex<company>> @companyList;

          //declare list for checking roundStage
          ListAccum<String> @@times;
          int index = 8;

          @@times = ["angel","seed","a","b","c","d","e","f","g"];
          //get the roundStage index
          foreach i in range[0,@@times.size()-1] do
              if(roundStage == @@times.get(i)) then index = i;
              else if(i > index) then @@roundSet += @@times.get(i);
              end;
          end;

          IPOs = {IPO.*};
          companies = {company.*};

          //1.find all companies has IPOed/Acquired.
          //company <- IPO
          B11 = select c
                  from IPOs:i-(company_ipo:e)->company:c
                  //filter null time
                  where datetime_diff(i.public_at,to_datetime("1970-01-01 00:00:00")) != 0
                  accum
                      c.@parentV += i, c.@parentE += e, c.@successTime = i.public_at, c.@startup += false;
          //company <- acquire
          B12 = select c
                  from companies:s-(acquire:e)->company:c
                  //filter null time
                  where datetime_diff(e.acquired_at,to_datetime("1970-01-01 00:00:00")) != 0
                  accum
                      c.@parentV += s, c.@parentE += e, c.@successTime = e.acquired_at, c.@startup += false;

          B1 = B11 union B12;

          //filter companies according to roundStage cutoff
          Startup = companies minus B1;
          Filt = select f
                      from Startup:c-(company_funding_rounds:e)->funding_rounds:f
                      accum
                          case when @@roundSet.contains(f.funding_round_code) then c.@selectCom += false end;

          //2. find employees who contributed to the companies in step(1) for IPOed/Acquired.
          B2 = select p
                  from B1:c-(work_for_company:e)->person:p
                  where datetime_diff(e.start_at,to_datetime("1970-01-01 00:00:00")) != 0 and datetime_diff(e.end_at,to_datetime("1970-01-01 00:00:00")) != 0 and datetime_diff(e.start_at,c.@successTime) < 0
                  accum
                      //p.@parentV += c,
                      @@beforeWorkEdge += (p -> (c -> e)),
                      @@beforeWorkTime += (p -> (c -> e.end_at));

          //3. find startups whose founders have worked for the companies in step(1) before.
          //4. select startups based on the input round cutoff and category sector.
          B3 = select c
                  from B2:p-(work_for_company:e)->company:c
                  where c.@startup and c.@selectCom and c.status != "acquired" and c.status != "ipo"
                          and e.title like "%ounder%"
                          and lower(trim(c.category_code)) == lower(trim(sector))
                          and datetime_diff(e.start_at,to_datetime("1970-01-01 00:00:00")) != 0
                          and datetime_diff(e.end_at,to_datetime("1970-01-01 00:00:00")) != 0
                  accum
                      foreach (key,value) in @@beforeWorkTime.get(p) do
                          if datetime_diff(e.start_at,value) > 0 then //c.@parentV += p,
                                  p.@parentE += @@beforeWorkEdge.get(p).get(key),
                                  //c.@nextPersonCompany += (person -> [key]),
                                  p.@companyList += key,
                                  c.@parentV += p,
                                  c.@parentE += e,
                                  c.@num += 1
                          end
                      end;

          //filter the companies which not subject to the employees working timeline
          dd = select c
                  from B3:c
                  where c.@num == 0
                  accum @@tempV += c;
          result = {@@tempV};
          @@tempV.clear();
          B3 = B3 minus result;

          ///5. Find the top K companies whose founders have the most successful trails.
          Process = select c
                              from B3:c
                              accum @@topScoreResults += scoreResults(c,c.@num);
          print @@topScoreResults;

          foreach item in @@topScoreResults do
                  @@verSet += item.company;
          end;

          //the top K companies
          result = {@@verSet};

          //add company's funding_founds
          B4 = select f
                  from result:c-(company_funding_rounds:e)->funding_rounds:f
                  accum @@verSet += f, @@edgeSet += e;

          // trace back to the source senders from the vertexes that joint multiple paths
          r = select c
                  from result:c
                  accum
                      @@tempV += c.@parentV, @@verSet += c.@parentV, @@edgeSet += c.@parentE;

          result = {@@tempV};
          @@tempV.clear();
          r = select p
                  from result:p
                  accum @@edgeSet += p.@parentE,
                          foreach item in p.@companyList do
                              @@tempV += item, @@verSet += item
                          end;

          result = {@@tempV};
          @@tempV.clear();
          //clean the parent of parent
          r = select c
                  from result:c
                  accum @@tempV += c.@parentV;
          del = {@@tempV};
          @@tempV.clear();
          r = select c
                  from del:c
                  where c.@parentV.size() != 0
                  post-accum c.@parentV.clear(), c.@parentE.clear();
          //add the parent
          r = select c
                  from result:c
                  accum @@verSet += c.@parentV, @@edgeSet += c.@parentE;
          result = {@@verSet};
          print result;
          print @@edgeSet;
      }
      CREATE QUERY KeyRoleDiscovery(STRING companyName, INT k) FOR GRAPH CrunchBasePre_2013 {
      /*
            DISCLAIMER: Data is from Crunchbase 2013 Snapshot under the Creative Commons Attribution License [CC-BY].
          --- Dataset has been shrunk on Nov.20th,2020 ---

            This query aims to find key roles of the input company and its parent companies (those investors or acquirer).
            Given a  company name companName, return K-step subgraph, which displays all the key roles (CEO, founder, board 		director, executive, etc.)
            of the companies on the k-step subgraph.

            Sample Input:
            companyName: LinkedIn			k: 2;
            companyName: LuckyCal,		k: 8;
          companyName: FriendFeed,	k: 5;
      ,*/
          OrAccum @seen;
          SetAccum<VERTEX> @@vertices;
          SetAccum<EDGE> @@edges;
          DATETIME present;

          present = to_datetime("1970-01-01 00:00:00");
        companies = {company.*};

          // find the company whose name is the input string
          Start (ANY) = SELECT tgt
                      FROM companies : tgt
                      WHERE lower(trim(tgt.name)) == lower(trim(companyName))
                      ACCUM @@vertices += tgt
                      POST-ACCUM tgt.@seen = TRUE;

          // find all key roles and investor/acquirer company's key roles in k-step
          WHILE TRUE LIMIT k DO
              Start = SELECT tgt
                      FROM Start: s - ((invested_by_company | acquired_by | work_for_company) :e) - (company | person):tgt
                      WHERE tgt.@seen == FALSE AND s.type != "person" AND
                                  (
                                      (e.type == "work_for_company" AND (e.title LIKE "%founder%" OR e.title LIKE "%Founder%" OR e.title LIKE "%CEO%" OR e.title LIKE "% ceo%" OR e.title LIKE "%CTO%" OR e.title LIKE "% cto%" OR ((e.title LIKE "%oard%irectors%" OR e.title LIKE "%xecutive%") AND datetime_diff(e.end_at, present) == 0))) OR
                                    e.type == "invested_by_company" OR e.type == "acquired_by"
                                  )
                      ACCUM @@vertices += tgt, @@edges += e
                      POST-ACCUM tgt.@seen = TRUE;
          END;
          IF @@vertices.size() != 0 THEN
                  results = {@@vertices};
                  print results;
                  print @@edges;
          ELSE
                  PRINT "Can't find any companies or people have key relations with this company in max steps.";
          END;
      }
      CREATE QUERY FindPromisingStartupBasedOnBoard(INT K1, INT K2, STRING cutoffRound, INT pastNYears) FOR GRAPH CrunchBasePre_2013 {
      /*
          DISCLAIMER: Data is from Crunchbase 2013 Snapshot under the Creative Commons Attribution License [CC-BY].
        --- Dataset has been shrunk on Nov.20th,2020 ---

        Suppose a person want to join a startup that has a good board memeber.
        In this query, we search crunchbase data, and target those board members who have served on startup
          boards that have successful exits (means they are good) AND these people belong to a currentlyrich
          financial organization (means they are active), and look at what other pre-IPO startups they are directing now.

        Brief steps:
        1. Find people who work for top K1 financial organizations that have raised most money in past N years.
        2. Find the top K2 persons who has the most board experience of succesful startups.
        3. Display the current startups whose board have the people discovered in step 2.
        4. We can further select the startup based on an input round cutoff.

        Sample Input:
        pastNYears means year restriction, K1 means top K1 financial organizations, K2 means top K2 people.
        K1: 30,  K2: 2,  cutoffRound: b, 	pastNYears: 15;
          K1: 8,   K2: 1,  cutoffRound: a pastNYears: 15;
        K1: 10,  K2: 3,  cutoffRound: d,  pastNYears: 10;
        K1: 20,  K2: 1,  cutoffRound: b pastNYears: 20;
      ,*/
        TYPEDEF TUPLE<VERTEX fOrg, DOUBLE amounts> rank;
        TYPEDEF TUPLE<VERTEX people, INT counts> tmost;
        HeapAccum<rank>(K1, amounts DESC) @@topF;
        HeapAccum<tmost>(K2, counts DESC) @@topP;
        MapAccum<STRING, DOUBLE> @@currency2USD;
        ListAccum<STRING> @@codeList;
        ListAccum<STRING> @@nptList;
        ListAccum<DATETIME> @time;
        SumAccum<DOUBLE> @amount;
        SumAccum<INT> @count;
        OrAccum @visited = FALSE;
        OrAccum @belongto = FALSE;
          OrAccum @potential = FALSE;
        SetAccum<VERTEX> @@forgs;
        SetAccum<VERTEX> @@pres;
        SetAccum<VERTEX> @@vsets;
        SetAccum<EDGE> @@esets;
        SetAccum<VERTEX> @@csets;
        SetAccum<VERTEX> @@tmp;
        SetAccum<VERTEX> @@psets;
        SetAccum<VERTEX> @myset;
        DATETIME present;
        DATETIME none;
        SumAccum<INT> @total;
        SumAccum<INT> @before;

        present = to_datetime("2013-12-31 23:59:59");
        none = to_datetime("1970-01-01 00:00:00");
        @@currency2USD += ("USD" -> 1);
        @@currency2USD += ("AUD" -> 1.28);
        @@currency2USD += ("CAD" -> 1.25);
        @@currency2USD += ("EUR" -> 0.85);
        @@currency2USD += ("GBP" -> 0.76);
        @@currency2USD += ("JPY" -> 112.84);
        @@currency2USD += ("SEK" -> 8.14);

        @@codeList = ["angel", "seed", "a", "b", "c", "d", "e", "f", "g"];
        FOREACH i in range[0, @@codeList.size()-1] DO
            IF @@codeList.get(i) == lower(trim(cutoffRound)) THEN
                @@nptList += @@codeList.get(i);
                BREAK;
            ELSE  @@nptList += @@codeList.get(i);
            END;
        END;

        Orgs (ANY) = {financialORG.*};
        // find top k financial organizations who raised most money in past N years
        Orgs = SELECT tgt
              FROM Orgs: s - (financial_funds: e) - funds: tgt
              WHERE datetime_diff(present, tgt.funded_at) <= pastNYears*31536000
              ACCUM s.@amount += (tgt.raised_amount / @@currency2USD.get(tgt.raised_currency_code)), tgt.@visited = TRUE/*,
                    @@d += tgt.created_at, @@t += datetime_diff(present, tgt.created_at)*/
              POST-ACCUM @@topF += rank(s, s.@amount);

        FOREACH item IN @@topF DO
            @@forgs += item.fOrg;
        END;

        TopORGs = {@@forgs};
        TopORGs = SELECT tgt
                  FROM TopORGs: tgt
                  POST-ACCUM tgt.@visited = TRUE;

        // tag ipo time or acquisition time on all companies
        Pre (ANY) = {company.*};
        Pre = SELECT tgt
              FROM Pre:s - ((company_ipo | acquired_by): e) - (IPO | company): tgt
              ACCUM CASE WHEN e.type == "company_ipo" AND datetime_diff(tgt.public_at, none) != 0 THEN
                          s.@time += tgt.public_at
                    END,
                    CASE WHEN e.type == "acquired_by" AND datetime_diff(e.acquired_at, none) != 0 THEN
                          s.@time += e.acquired_at
                    END;

        // find people who work for the top k financial organizations
        S0 = SELECT tgt
                FROM TopORGs: s - (work_for_fOrg: e) - person: tgt
                ACCUM tgt.@belongto = TRUE;

        // find selected companies whose board has person in S1
        S1 = SELECT tgt
            FROM S0: s - (work_for_company: e) - company: tgt
            WHERE (e.title LIKE "%Board%" OR e.title LIKE "%board%") AND tgt.@time.size() != 0 AND datetime_diff(e.start_at, none) != 0
            ACCUM IF datetime_diff(tgt.@time.get(0), e.start_at) > 0 THEN
                      @@csets += tgt, tgt.@myset += s
                  END;

        // count successful exits for each discovered person and do the ranking
        S2 (ANY) = {@@csets};
        S2 = SELECT tgt
              FROM S2 :s - (work_for_company :e) - person: tgt
              WHERE (e.title LIKE "%Board%" OR e.title LIKE "%board%") AND tgt.@belongto == TRUE
              ACCUM tgt.@amount += 1
              POST-ACCUM @@topP += tmost(tgt, tgt.@amount);
        FOREACH item in @@topP DO
            @@psets += item.people;
        END;

        // find rest companies without ipo or acquisition that the person serves
        S3 (ANY) = {@@psets};
        S3 = SELECT tgt
            FROM S3: s - (work_for_company: e) - company: tgt
            WHERE (e.title LIKE "%Board%" OR e.title LIKE "%board%") AND e.start_at != none AND tgt.status == "operating" AND tgt NOT IN @@csets;

        // find other pre-IPO startups that top persons are directing now before certain round
        S4 = SELECT tgt
            FROM S3: s - (company_funding_rounds: e) - funding_rounds: tgt
            ACCUM s.@total += 1,
                  IF @@nptList.contains(tgt.funding_round_code)
                    THEN s.@before += 1, tgt.@visited = TRUE
                  END
            POST-ACCUM @@tmp += s;
        S5 = {@@tmp};
        S5 = SELECT tgt
            FROM S5: tgt
            WHERE tgt.@before == tgt.@total
            ACCUM tgt.@potential = TRUE, @@csets += tgt;

        // back traverse to find paths displayed in the subgraph
        Paths (ANY) = {@@psets};
        WHILE TRUE LIMIT 2 DO
            Paths = SELECT tgt
              FROM Paths :s - ((company_funding_rounds | acquired_by | company_ipo | work_for_company | work_for_fOrg | financial_funds): e) - :tgt
              WHERE (e.type == "work_for_company" AND ((datetime_diff(e.start_at, none) != 0 AND tgt.@myset.contains(s)) OR tgt.@potential == TRUE)) OR
                  ((e.type == "work_for_fOrg" OR e.type == "financial_funds" OR e.type == "company_funding_rounds") AND tgt.@visited == TRUE)
                  OR e.type == "company_ipo" OR e.type == "acquired_by"
                ACCUM @@vsets += tgt, @@esets += e;
        END;
        Result1 = {@@psets};
        Result2 = {@@vsets};
        PRINT Result1;
        PRINT Result2;
        PRINT @@esets;
      }
      CREATE QUERY InvestorSuccessfulExits(String investorName, String investorType, int years) FOR GRAPH CrunchBasePre_2013 {
          /*
       DISCLAIMER: Data is from Crunchbase 2013 Snapshot under the Creative Commons Attribution License [CC-BY].
       --- Dataset has been shrunk on Nov.20th,2020 ---

       This query aims to find a given investor's achievements measured by their succesful investment
       exits (IPO, acquisition) within a fixed number of years.
       E.g.  If fixed number of years is 4. We evaluate the investor based on how many IPOs
       happened within 4 years after their investments.

       Given an investor name, input type (company/person/fiancialORG), and maximum years before
       the investment exit event.

       First step, find the funding rounds participated by the investor.
       Second step, find the companies funded by the rounds discovered in the first step.
       Third step, find the IPO info (if exists) of the companies discovered in the second step

       The traversal path looks like below.
        company/fiancialORG/person -> funding_rounds -> company -> IPO

        Sample input:
          1.  investorName: General Electric ; investorType: company; years: 2;
          2.  investorName: Accel Partners; investorType: financialORG; years: 5;
          3.  investorName: Ted Leonsis ; investorType: person; years: 3;
      ,*/
          //declare set to store vertex and edge
          SetAccum<vertex> @@verSet;
          SetAccum<vertex> @@addV;
          SetAccum<edge> @@edgeSet;
          SetAccum<vertex> @parentV;
          SetAccum<edge> @parentE;
          //declare variable to store the earliest time invested by this investor
          MinAccum<datetime> @investedTime;
          //declare variable to tag visited or not
          OrAccum @visited = false;
          //declare a default time to investedTime

          //assign different variable according to the input type
          start = {};
          case
              when lower(trim(investorType)) == "person" then start = {person.*};
              when lower(trim(investorType)) == "company" then start = {company.*};
              when lower(trim(investorType)) == "financialorg" then start = {financialORG.*};
        end;

          //find the investor whose name is input
          //and assign the "Start" variable, which is a SET.
          Start (ANY) = select c
                          from start:c
                          where (c.type == "person" and lower(trim(c.fullname)) == lower(trim(investorName))) or lower(trim(c.name)) == lower(trim(investorName))
                          accum c.@visited = true;
          print Start;

          //to get invested funding rounds (investor -> funding_rounds)
          Start = select tgt
                      from Start:s-((investment_from_company|investment_from_person|investment_from_financialORG):e)-funding_rounds:tgt
                      accum
                          tgt.@parentE += e, tgt.@visited = true,
                          case
                              //update investedTime
                              when not tgt.@visited then tgt.@investedTime = tgt.funded_at, tgt.@parentV += s
                              else tgt.@investedTime += tgt.created_at, tgt.@parentV += s
                          end;

          //To get invested companies (funding_rounds -> company)
          Start = select tgt
                      from Start:s-((company_funding_rounds):e)-company:tgt
                      accum
                          tgt.@parentV += s, tgt.@parentE += e, tgt.@investedTime = s.@investedTime;

          //To get company IPOs (company -> IPO)
          Start = select tgt
                      from Start:s-((company_ipo|acquired_by):e)-:tgt
                      accum
                          tgt.@parentV += s, tgt.@parentE += e,
                          //select the IPO which created within input years aftering being invested
                          if (e.type == "company_ipo" and datetime_diff(tgt.public_at, s.@investedTime) > 0 and datetime_diff(tgt.public_at, s.@investedTime) <= years*31556952)
                              or (e.type == "acquired_by" and datetime_diff(e.acquired_at, s.@investedTime) > 0 and datetime_diff(e.acquired_at, s.@investedTime) <= years*31556952)
                                  then @@addV += tgt
                          end;

          @@verSet += @@addV;
          Start = {@@addV};
          @@addV.clear();

          // trace back to the source senders from the vertices that joint multiple paths
          while(Start.size()>0) do
                  Start = select s
                          from Start:s
                          accum @@addV += s.@parentV, @@edgeSet += s.@parentE;
                  @@verSet += @@addV;
                  Start = {@@addV};
                  @@addV.clear();
          end;
          //output the result
          Start = {@@verSet};
          print Start;
          print @@edgeSet;
      }
    #+end_src

** Enterprise Knowledge (non-crunchbase)
   + data: /data/data-files/graph-data/enterprise-cloud_data.tar.gz  or s3://gjgeksbackup/enterprise-cloud_data.tar.gz
*** Create graph schema
    #+begin_src sql :tangle solutions/enterprise/01-create-graph.gsql :mkdirp yes
      USE GLOBAL
      DROP GRAPH EnterpriseGraph
      CREATE GRAPH EnterpriseGraph()
      USE GRAPH EnterpriseGraph
          pCREATE SCHEMA_CHANGE JOB schema_enterprise_graph FOR GRAPH EnterpriseGraph {
        ADD VERTEX Person(PRIMARY_ID name STRING, gender STRING) WITH STATS="OUTDEGREE_BY_EDGETYPE", PRIMARY_ID_AS_ATTRIBUTE="false";
        ADD VERTEX Company(PRIMARY_ID name STRING, registered_capital UINT) WITH STATS="OUTDEGREE_BY_EDGETYPE", PRIMARY_ID_AS_ATTRIBUTE="false";
        ADD VERTEX Project(PRIMARY_ID name STRING) WITH STATS="OUTDEGREE_BY_EDGETYPE", PRIMARY_ID_AS_ATTRIBUTE="false";

        ADD UNDIRECTED EDGE WorkFor(FROM Person, TO Company, title STRING);
        ADD UNDIRECTED EDGE PersonInvestCompany(FROM Person, TO Company, invest_year UINT, amount DOUBLE, control_type STRING);
        ADD DIRECTED EDGE CompanyInvestCompany(FROM Company, TO Company, invest_year UINT, amount DOUBLE, control_type STRING) WITH REVERSE_EDGE="reverse_CompanyInvestCompany";
        ADD UNDIRECTED EDGE BidFor(FROM Company, TO Project, price DOUBLE, solution STRING);
      }

      RUN SCHEMA_CHANGE JOB schema_enterprise_graph
      DROP JOB schema_enterprise_graph
    #+end_src
*** Load data
    #+begin_src sql :tangle solutions/enterprise/02-load-data.gsql :mkdirp yes
      USE GRAPH EnterpriseGraph
      CREATE LOADING JOB load_enterprise_knowledge FOR GRAPH EnterpriseGraph {
        DEFINE FILENAME enterprise  = "m1:/home/tigergraph/mydata/enterprise/data/enterprise.csv";
        DEFINE FILENAME working  = "m1:/home/tigergraph/mydata/enterprise/data/working.csv";
        DEFINE FILENAME person_invest_enterprise  = "m1:/home/tigergraph/mydata/enterprise/data/person_invest_enterprise.csv";
        DEFINE FILENAME enterprise_invest_enterprise  = "m1:/home/tigergraph/mydata/enterprise/data/enterprise_invest_enterprise.csv";
        DEFINE FILENAME enterprise_bid_for_project  = "m1:/home/tigergraph/mydata/enterprise/data/enterprise_bid_for_project.csv";
            LOAD enterprise TO VERTEX Company VALUES($0, $1) USING SEPARATOR=",", HEADER="true", EOL="\n";
            LOAD working TO EDGE WorkFor VALUES($0, $1, $2) USING SEPARATOR=",", HEADER="true", EOL="\n";
            LOAD person_invest_enterprise TO EDGE PersonInvestCompany VALUES($0, $1, $2, $3, $4) USING SEPARATOR=",", HEADER="true", EOL="\n";
            LOAD enterprise_invest_enterprise TO EDGE CompanyInvestCompany VALUES($0, $1, $2, $3, $4) USING SEPARATOR=",", HEADER="true", EOL="\n";
            LOAD enterprise_bid_for_project TO EDGE BidFor VALUES($0, $1, $2, gsql_concat($3,$4)) USING SEPARATOR=",", HEADER="true", EOL="\n";
      }

      RUN LOADING JOB load_enterprise_knowledge
      DROP JOB load_enterprise_knowledge
    #+end_src
*** Add queries
    #+begin_src sql :tangle solutions/enterprise/03-add-queries.gsql :mkdirp yes
      USE GRAPH EnterpriseGraph
      CREATE QUERY KeyRelationship(vertex<Company> company, int step) FOR GRAPH EnterpriseGraph {
          /**
         ,* KeyRelationship query finds all key investors or leaders (CEO, VP, Director) of a company within several steps.
           ,* --- Dataset has been shrunk on Nov.20th,2020 ---
         ,* Some interesting input parameters you can try:
         ,* 1. company: Hospice Mocha Frame, step: 5
           ,* 2. company: Psychoanalyst Purse Prior, step: 4
           ,* 3. company: Hospice Loyalty Decongestant, step: 2
           ,* 4. company: Discipline Base Perfume, step 1
           ,* 5. company: Discipline Base Perfume, step 2
           ,* 6. company: Discipline Base Perfume, step 3
           ,* 7. company: Discipline Base Perfume, step 4
         ,*/

        OrAccum @visited = false;
          SetAccum<edge> @@edges;

        int loopStep;

        // limit the maximum traverse steps
        IF (step > 8) THEN
          loopStep = 8;
        ELSE
          loopStep = step;
        END;


          StartSet (any) = { company };

          VertexResult = StartSet;

          StartSet = 	SELECT StartSet
                                  FROM StartSet
                                  ACCUM StartSet.@visited = true;

          WHILE (true) LIMIT loopStep DO
              StartSet =	SELECT tgt
                                      FROM StartSet
                                               -((WorkFor | PersonInvestCompany | reverse_CompanyInvestCompany): e)->
                                               (Person | Company): tgt
                                      WHERE (
                                                      (
                                                          e.type == "WorkFor"
                                                        AND
                                                        (e.title == "Director" OR e.title == "CEO" OR e.title == "Vice President")
                                                      )
                                                      OR
                                                      e.control_type == "holding"
                                                  )
                                                  AND
                                                  tgt.@visited == FALSE
                                      ACCUM @@edges += e
                                      POST-ACCUM tgt.@visited = TRUE
                                      ;
              VertexResult = VertexResult union StartSet;
          END;

          PRINT VertexResult;
          PRINT @@edges;
      }
      CREATE QUERY CompanyHolders(vertex<Company> company, uint step) FOR GRAPH EnterpriseGraph {
        /**
         ,* CompanyHolders query finds all key investors of a company within several steps.
           ,* --- Dataset has been shrunk on Nov.20th,2020 ---
         ,* Some interesting input parameters you can try:
         ,* 1. company: Hospice Mocha Frame, step: 5
           ,* 2. company: Psychoanalyst Purse Prior, step: 4
           ,* 3. company: Hospice Loyalty Decongestant, step: 2
           ,* 4. company: Discipline Base Perfume, step 1
           ,* 5. company: Discipline Base Perfume, step 2
           ,* 6. company: Discipline Base Perfume, step 3
           ,* 7. company: Discipline Base Perfume, step 4
         ,*/

          // @visited is used to mark visited vertices
        OrAccum @visited = false;
          // @@edges is used to hold all touched edges
        SetAccum<edge> @@edges;

        int loopStep;

        // limit the maximum traverse steps
        IF (step > 8) THEN
          loopStep = 8;
        ELSE
          loopStep = step;
        END;

        // Start from the input company
        StartSet (any) = { company };

          // VertexResults contains all vertices we touched during traversal
        VertexResult = StartSet;

          // Mark input company as visited
        StartSet =  SELECT StartSet
                    FROM StartSet
                    ACCUM StartSet.@visited = true;

          // Traverse multiple steps
        WHILE (true) LIMIT loopStep DO
              // Find the investors (either people or companies) that is a key investor (control_type == "holding")
          StartSet =  SELECT tgt
                      FROM StartSet
                           -((PersonInvestCompany | reverse_CompanyInvestCompany): e)->
                           (Person | Company): tgt
                      WHERE (e.control_type == "holding") AND tgt.@visited == FALSE
                      ACCUM @@edges += e
                      POST-ACCUM tgt.@visited = TRUE
                      ;
              // Merge touched vertices into the result
          VertexResult = VertexResult union StartSet;
        END;

          // Print result as a graph
        PRINT VertexResult;
        PRINT @@edges;
      }
    #+end_src

** STARTED LDBC Social Graph small sample size
   :LOGBOOK:
   - State "STARTED"    from "TODO"       [2021-11-08 Mon 10:26]
   CLOCK: [2021-11-08 Mon 10:26]--[2021-11-08 Mon 11:59] =>  1:33
   :END:
   Refs:
   + [[https://docs.tigergraph.com/v/3.2/start/gsql-102/define-the-schema][Define the Schema - TigerGraph Documentation]]
   + [[https://ldbcouncil.org/benchmarks/snb/][LDBC Social Network Benchmark (LDBC SNB)]]

*** Get the data
    #+begin_src bash
      wget https://s3-us-west-1.amazonaws.com/tigergraph-benchmark-dataset/LDBC/SF-1/ldbc_snb_data-sf1.tar.gz
    #+end_src
*** Create the schema
    #+begin_src sql :tangle solutions/ldbc-social/01-create-graph.gsql :mkdirp yes
      //clear the current catalog.
      // It may take a while since it restarts the subsystem services.
      USE GLOBAL
      DROP GRAPH ldbc_snb

      # 1. Create graph
      CREATE GRAPH ldbc_snb ()
      USE GRAPH ldbc_snb

      # 2. Create schema_change job to include all vertex/edge types
      CREATE SCHEMA_CHANGE JOB change_schema_of_ldbc  FOR GRAPH ldbc_snb {
        ## Post and Comment
        ADD VERTEX Comment (PRIMARY_ID id UINT, creationDate DATETIME, locationIP STRING,
          browserUsed STRING, content STRING, length UINT) WITH primary_id_as_attribute="TRUE";

        ADD VERTEX Post (PRIMARY_ID id UINT, imageFile STRING, creationDate DATETIME,
          locationIP STRING, browserUsed STRING, lang STRING, content STRING,
          length UINT) WITH primary_id_as_attribute="TRUE";
        ## organisation
        ADD VERTEX Company (PRIMARY_ID id UINT, name STRING, url STRING) WITH primary_id_as_attribute="TRUE";
        ADD VERTEX University (PRIMARY_ID id UINT, name STRING, url STRING) WITH primary_id_as_attribute="TRUE";
        ## place
        ADD VERTEX City (PRIMARY_ID id UINT, name STRING, url STRING) WITH primary_id_as_attribute="TRUE";
        ADD VERTEX Country (PRIMARY_ID id UINT, name STRING, url STRING) WITH primary_id_as_attribute="TRUE";
        ADD VERTEX Continent (PRIMARY_ID id UINT, name STRING, url STRING) WITH primary_id_as_attribute="TRUE";
        ## etc
        ADD  VERTEX Forum (PRIMARY_ID id UINT, title STRING, creationDate DATETIME) WITH primary_id_as_attribute="TRUE";
        ADD  VERTEX Person (PRIMARY_ID id UINT, firstName STRING, lastName STRING, gender STRING, birthday DATETIME,
         creationDate DATETIME, locationIP STRING, browserUsed STRING, speaks set<STRING>, email set<STRING>)
         WITH primary_id_as_attribute="TRUE";
        ADD VERTEX Tag (PRIMARY_ID id UINT, name STRING, url STRING) WITH primary_id_as_attribute="TRUE";
        ADD VERTEX TagClass (PRIMARY_ID id UINT, name STRING, url STRING) WITH primary_id_as_attribute="TRUE";

        // create edge types
        ADD DIRECTED EDGE CONTAINER_OF (FROM Forum, TO Post) WITH REVERSE_EDGE="CONTAINER_OF_REVERSE";
        ADD  DIRECTED EDGE HAS_CREATOR (FROM Comment|Post, TO Person) WITH REVERSE_EDGE="HAS_CREATOR_REVERSE";
        ADD  DIRECTED EDGE HAS_INTEREST (FROM Person, TO Tag) WITH REVERSE_EDGE="HAS_INTEREST_REVERSE";
        ADD DIRECTED EDGE HAS_MEMBER (FROM Forum, TO Person, joinDate DATETIME) WITH REVERSE_EDGE="HAS_MEMBER_REVERSE";
        ADD DIRECTED EDGE HAS_MODERATOR (FROM Forum, TO Person) WITH REVERSE_EDGE="HAS_MODERATOR_REVERSE";
        ADD DIRECTED EDGE HAS_TAG (FROM Comment|Post|Forum, TO Tag) WITH REVERSE_EDGE="HAS_TAG_REVERSE";
        ADD DIRECTED EDGE HAS_TYPE (FROM Tag, TO TagClass) WITH REVERSE_EDGE="HAS_TYPE_REVERSE";
        ADD  DIRECTED EDGE IS_LOCATED_IN (FROM Comment, TO Country
                                        | FROM Post, TO Country
                                        | FROM Company, TO Country
                                        | FROM Person, TO City
                                        | FROM University, TO City) WITH REVERSE_EDGE="IS_LOCATED_IN_REVERSE";
        ADD DIRECTED EDGE IS_PART_OF (FROM City, TO Country
                                     | FROM Country, TO Continent) WITH REVERSE_EDGE="IS_PART_OF_REVERSE";
        ADD DIRECTED EDGE IS_SUBCLASS_OF (FROM TagClass, TO TagClass) WITH REVERSE_EDGE="IS_SUBCLASS_OF_REVERSE";
        ADD UNDIRECTED EDGE KNOWS (FROM Person, TO Person, creationDate DATETIME);
        ADD DIRECTED EDGE LIKES (FROM Person, TO Comment|Post, creationDate DATETIME) WITH REVERSE_EDGE="LIKES_REVERSE";
        ADD DIRECTED EDGE REPLY_OF (FROM Comment, TO Comment|Post) WITH REVERSE_EDGE="REPLY_OF_REVERSE";
        ADD DIRECTED EDGE STUDY_AT (FROM Person, TO University, classYear INT) WITH REVERSE_EDGE="STUDY_AT_REVERSE";
        ADD DIRECTED EDGE WORK_AT (FROM Person, TO Company, workFrom INT) WITH REVERSE_EDGE="WORK_AT_REVERSE";
      }

      # 3. Run schema_change job
      RUN SCHEMA_CHANGE JOB change_schema_of_ldbc

      # 4. Drop schema_change job
      DROP JOB change_schema_of_ldbc

    #+end_src

*** Load the data
    #+begin_src sql :tangle solutions/ldbc-social/02-load-data.gsql :mkdirp yes
      USE GRAPH ldbc_snb
      CREATE LOADING JOB load_ldbc_snb FOR GRAPH ldbc_snb {
        DEFINE FILENAME v_person_file="m1:/home/tigergraph/mydata/ldbc-social/data/person_0_0.csv";
        DEFINE FILENAME v_post_file="m1:/home/tigergraph/mydata/ldbc-social/data/post_0_0.csv";
        DEFINE FILENAME v_tag_file="m1:/home/tigergraph/mydata/ldbc-social/data/tag_0_0.csv";
        DEFINE FILENAME v_place_file="m1:/home/tigergraph/mydata/ldbc-social/data/place_0_0.csv";
        DEFINE FILENAME v_comment_file="m1:/home/tigergraph/mydata/ldbc-social/data/comment_0_0.csv";
        DEFINE FILENAME v_forum_file="m1:/home/tigergraph/mydata/ldbc-social/data/forum_0_0.csv";
        DEFINE FILENAME v_organisation_file="m1:/home/tigergraph/mydata/ldbc-social/data/organisation_0_0.csv";
        DEFINE FILENAME v_tagclass_file="m1:/home/tigergraph/mydata/ldbc-social/data/tagclass_0_0.csv";
        DEFINE FILENAME person_knows_person_file="m1:/home/tigergraph/mydata/ldbc-social/data/person_knows_person_0_0.csv";
        DEFINE FILENAME comment_replyOf_post_file="m1:/home/tigergraph/mydata/ldbc-social/data/comment_replyOf_post_0_0.csv";
        DEFINE FILENAME comment_replyOf_comment_file="m1:/home/tigergraph/mydata/ldbc-social/data/comment_replyOf_comment_0_0.csv";
        DEFINE FILENAME post_hasCreator_person_file="m1:/home/tigergraph/mydata/ldbc-social/data/post_hasCreator_person_0_0.csv";
        DEFINE FILENAME post_hasTag_tag_file="m1:/home/tigergraph/mydata/ldbc-social/data/post_hasTag_tag_0_0.csv";
        DEFINE FILENAME comment_hasCreator_person_file="m1:/home/tigergraph/mydata/ldbc-social/data/comment_hasCreator_person_0_0.csv";
        DEFINE FILENAME post_isLocatedIn_place_file="m1:/home/tigergraph/mydata/ldbc-social/data/post_isLocatedIn_place_0_0.csv";
        DEFINE FILENAME comment_hasTag_tag_file="m1:/home/tigergraph/mydata/ldbc-social/data/comment_hasTag_tag_0_0.csv";
        DEFINE FILENAME comment_isLocatedIn_place_file="m1:/home/tigergraph/mydata/ldbc-social/data/comment_isLocatedIn_place_0_0.csv";
        DEFINE FILENAME forum_containerOf_post_file="m1:/home/tigergraph/mydata/ldbc-social/data/forum_containerOf_post_0_0.csv";
        DEFINE FILENAME forum_hasMember_person_file="m1:/home/tigergraph/mydata/ldbc-social/data/forum_hasMember_person_0_0.csv";
        DEFINE FILENAME forum_hasModerator_person_file="m1:/home/tigergraph/mydata/ldbc-social/data/forum_hasModerator_person_0_0.csv";
        DEFINE FILENAME forum_hasTag_tag_file="m1:/home/tigergraph/mydata/ldbc-social/data/forum_hasTag_tag_0_0.csv";
        DEFINE FILENAME organisation_isLocatedIn_place_file="m1:/home/tigergraph/mydata/ldbc-social/data/organisation_isLocatedIn_place_0_0.csv";
        DEFINE FILENAME person_hasInterest_tag_file="m1:/home/tigergraph/mydata/ldbc-social/data/person_hasInterest_tag_0_0.csv";
        DEFINE FILENAME person_isLocatedIn_place_file="m1:/home/tigergraph/mydata/ldbc-social/data/person_isLocatedIn_place_0_0.csv";
        DEFINE FILENAME person_likes_comment_file="m1:/home/tigergraph/mydata/ldbc-social/data/person_likes_comment_0_0.csv";
        DEFINE FILENAME person_likes_post_file="m1:/home/tigergraph/mydata/ldbc-social/data/person_likes_post_0_0.csv";
        DEFINE FILENAME person_studyAt_organisation_file="m1:/home/tigergraph/mydata/ldbc-social/data/person_studyAt_organisation_0_0.csv";
        DEFINE FILENAME person_workAt_organisation_file="m1:/home/tigergraph/mydata/ldbc-social/data/person_workAt_organisation_0_0.csv";
        DEFINE FILENAME place_isPartOf_place_file="m1:/home/tigergraph/mydata/ldbc-social/data/place_isPartOf_place_0_0.csv";
        DEFINE FILENAME tag_hasType_tagclass_file="m1:/home/tigergraph/mydata/ldbc-social/data/tag_hasType_tagclass_0_0.csv";
        DEFINE FILENAME tagclass_isSubclassOf_tagclass_file="m1:/home/tigergraph/mydata/ldbc-social/data/tagclass_isSubclassOf_tagclass_0_0.csv";





        // load vertex
        LOAD v_comment_file
          TO VERTEX Comment VALUES ($0, $1, $2, $3, $4, $5) USING header="true", separator="|";
        LOAD v_post_file
          TO VERTEX Post VALUES ($0, $1, $2, $3, $4, $5, $6, $7) USING header="true", separator="|";
        LOAD v_organisation_file
          TO VERTEX Company VALUES ($0, $2, $3) WHERE $1=="company",
          TO VERTEX University VALUES ($0, $2, $3) WHERE $1=="university" USING header="true", separator="|";
        LOAD v_place_file
          TO VERTEX City VALUES ($0, $1, $2) WHERE $3=="city",
          TO VERTEX Country VALUES ($0, $1, $2) WHERE $3=="country",
          TO VERTEX Continent VALUES ($0, $1, $2) WHERE $3=="continent" USING header="true", separator="|";
        LOAD v_forum_file
          TO VERTEX Forum VALUES ($0, $1, $2) USING header="true", separator="|";
        LOAD v_person_file
          TO VERTEX Person VALUES ($0, $1, $2, $3, $4, $5, $6, $7, SPLIT($8,";"), SPLIT($9,";")) USING header="true", separator="|";
        LOAD v_tag_file
          TO VERTEX Tag VALUES ($0, $1, $2) USING header="true", separator="|";
        LOAD v_tagclass_file
          TO VERTEX TagClass VALUES ($0, $1, $2) USING header="true", separator="|";

        // load edge
        LOAD forum_containerOf_post_file
          TO EDGE CONTAINER_OF VALUES ($0, $1) USING header="true", separator="|";
        LOAD comment_hasCreator_person_file
          TO EDGE HAS_CREATOR VALUES ($0 Comment, $1) USING header="true", separator="|";
        LOAD post_hasCreator_person_file
          TO EDGE HAS_CREATOR VALUES ($0 Post, $1) USING header="true", separator="|";
        LOAD person_hasInterest_tag_file
          TO EDGE HAS_INTEREST VALUES ($0, $1) USING header="true", separator="|";
        LOAD forum_hasMember_person_file
          TO EDGE HAS_MEMBER VALUES ($0, $1, $2) USING header="true", separator="|";
        LOAD forum_hasModerator_person_file
          TO EDGE HAS_MODERATOR VALUES ($0, $1) USING header="true", separator="|";
        LOAD comment_hasTag_tag_file
          TO EDGE HAS_TAG VALUES ($0 Comment, $1) USING header="true", separator="|";
        LOAD post_hasTag_tag_file
          TO EDGE HAS_TAG VALUES ($0 Post, $1) USING header="true", separator="|";
        LOAD forum_hasTag_tag_file
          TO EDGE HAS_TAG VALUES ($0 Forum, $1) USING header="true", separator="|";
        LOAD tag_hasType_tagclass_file
          TO EDGE HAS_TYPE VALUES ($0, $1) USING header="true", separator="|";
        LOAD organisation_isLocatedIn_place_file
          TO EDGE IS_LOCATED_IN VALUES ($0 Company, $1 Country) WHERE to_int($1) < 111,
          TO EDGE IS_LOCATED_IN VALUES ($0 University, $1 City) WHERE to_int($1) > 110 USING header="true", separator="|";
        LOAD comment_isLocatedIn_place_file
          TO EDGE IS_LOCATED_IN VALUES ($0 Comment, $1 Country) USING header="true", separator="|";
        LOAD post_isLocatedIn_place_file
          TO EDGE IS_LOCATED_IN VALUES ($0 Post, $1 Country) USING header="true", separator="|";
        LOAD person_isLocatedIn_place_file
          TO EDGE IS_LOCATED_IN VALUES ($0 Person, $1 City) USING header="true", separator="|";
        LOAD place_isPartOf_place_file
          TO EDGE IS_PART_OF VALUES ($0 Country, $1 Continent) WHERE to_int($0) < 111,
          TO EDGE IS_PART_OF VALUES ($0 City, $1 Country) WHERE to_int($0) > 110 USING header="true", separator="|";
        LOAD tagclass_isSubclassOf_tagclass_file
          TO EDGE IS_SUBCLASS_OF VALUES ($0, $1) USING header="true", separator="|";
        LOAD person_knows_person_file
          TO EDGE KNOWS VALUES ($0, $1, $2) USING header="true", separator="|";
        LOAD person_likes_comment_file
          TO EDGE LIKES VALUES ($0, $1 Comment, $2) USING header="true", separator="|";
        LOAD person_likes_post_file
          TO EDGE LIKES VALUES ($0, $1 Post, $2) USING header="true", separator="|";
        LOAD comment_replyOf_comment_file
          TO EDGE REPLY_OF VALUES ($0, $1 Comment) USING header="true", separator="|";
        LOAD comment_replyOf_post_file
          TO EDGE REPLY_OF VALUES ($0, $1 Post) USING header="true", separator="|";
        LOAD person_studyAt_organisation_file
          TO EDGE STUDY_AT VALUES ($0, $1, $2) USING header="true", separator="|";
        LOAD person_workAt_organisation_file
          TO EDGE WORK_AT VALUES ($0, $1, $2) USING header="true", separator="|";
      }

      RUN LOADING JOB load_ldbc_snb

      DROP JOB load_ldbc_snb
    #+end_src
** Northwind
*** Resources
    + data: s3://gjgeksbackup/northwind-data.tar.gz, or /data/data-files/graph-data/northwind-data.tar.gz
*** Create graph
    #+begin_src sql :tangle solutions/northwind/01-create-graph.gsql :mkdirp yes
      # -*- mode: sql; -*-
      drop graph Northwind
      create graph Northwind()
      use graph Northwind
      create schema_change job schema_change_northwind {
        ADD VERTEX Reps(PRIMARY_ID id INT, LastName STRING, FirstName STRING, Title STRING, CourtesyTitle STRING, BirthDate DATETIME, HireDate DATETIME, Address STRING, City STRING, Region STRING, PostalCode STRING, Country STRING, HomePhone STRING, PhoneExtension STRING, ReportsTo INT, group_id INT, is_root BOOL) WITH STATS="OUTDEGREE_BY_EDGETYPE", PRIMARY_ID_AS_ATTRIBUTE="true";

        ADD VERTEX Customers(PRIMARY_ID id STRING, CompanyName STRING, ContactName STRING, Address STRING, City STRING, Region STRING, PostalCode STRING, Country STRING, Phone STRING, referred_by STRING, group_id INT, is_root BOOL, Type STRING DEFAULT "DIRECT") WITH STATS="OUTDEGREE_BY_EDGETYPE", PRIMARY_ID_AS_ATTRIBUTE="true";

        ADD VERTEX Orders(PRIMARY_ID id INT, CustomerID STRING, EmployeeID INT, OrderDate DATETIME, RequiredDate DATETIME, ShippedDate DATETIME, ShipVia INT, Freight DOUBLE, ShipName STRING, ShipAddress STRING, ShipCity STRING, ShipRegion STRING, ShipPostalCode STRING, ShipCountry STRING, OrderAmount DOUBLE, CommissionAmount DOUBLE, fraud_score DOUBLE, ReturnedDate STRING) WITH STATS="OUTDEGREE_BY_EDGETYPE", PRIMARY_ID_AS_ATTRIBUTE="true";

        ADD VERTEX Products(PRIMARY_ID id INT, ProductName STRING, SupplierID INT, CategoryID INT, QuantityPerUnit STRING, UnitPrice DOUBLE, UnitsInStock INT, UnitsOnOrder INT, ReorderLevel INT, Discontinued INT) WITH STATS="OUTDEGREE_BY_EDGETYPE", PRIMARY_ID_AS_ATTRIBUTE="true";

        ADD VERTEX Territories(PRIMARY_ID id STRING, Description STRING) WITH STATS="OUTDEGREE_BY_EDGETYPE";

        ADD VERTEX Regions(PRIMARY_ID id INT, Description STRING) WITH STATS="OUTDEGREE_BY_EDGETYPE", PRIMARY_ID_AS_ATTRIBUTE="true";

        ADD VERTEX Suppliers(PRIMARY_ID id STRING, CompanyName STRING, ContactName STRING, ContactTitle STRING, Address STRING, City STRING, Region STRING, PostalCode STRING, Phone STRING, Country STRING, HomePage STRING) WITH STATS="OUTDEGREE_BY_EDGETYPE";

        ADD VERTEX Categories(PRIMARY_ID CategoryID INT, CategoryName STRING, Description STRING) WITH STATS="OUTDEGREE_BY_EDGETYPE", PRIMARY_ID_AS_ATTRIBUTE="true";

        ADD DIRECTED EDGE referredby(FROM Customers, TO Customers, referral_date DATETIME) WITH REVERSE_EDGE="reverse_referredby";

        ADD UNDIRECTED EDGE place(FROM Customers, TO Orders, CustomerID STRING, OrderID INT);

        ADD UNDIRECTED EDGE operate_in(FROM Reps, TO Territories);

        ADD UNDIRECTED EDGE located_in(FROM Territories, TO Regions);

        ADD UNDIRECTED EDGE rep_customer(FROM Reps, TO Customers);

        ADD UNDIRECTED EDGE order_product(FROM Orders, TO Products, UnitPrice DOUBLE, Quantity INT, Discount DOUBLE, LineTotalNoDiscount DOUBLE, LineTotal DOUBLE);

        ADD UNDIRECTED EDGE product_supplier(FROM Products, TO Suppliers);

        ADD UNDIRECTED EDGE rep_order(FROM Reps, TO Orders);

        ADD UNDIRECTED EDGE supplier_region(FROM Suppliers, TO Regions);

        ADD UNDIRECTED EDGE product_category(FROM Categories, TO Products);

      }
      run schema_change job schema_change_northwind
      drop job schema_change_northwind
#+end_src
*** Load data
    #+begin_src sql :tangle solutions/northwind/02-load-data.gsql :mkdirp yes
      # -*- mode: sql; -*-
      use graph Northwind
      CREATE LOADING JOB load_all_north {
        DEFINE FILENAME categories="m1:/home/tigergraph/mydata/northwind/data/categories.csv";
        DEFINE HEADER   categories = "CategoryID","CategoryName","Description","Picture";

        DEFINE FILENAME customers="m1:/home/tigergraph/mydata/northwind/data/customers.csv";
        DEFINE HEADER   customers = "CustomerID","CompanyName","ContactName","ContactTitle","Address","City","Region","PostalCode","Country","Phone","Fax","ReferredBy";

        DEFINE FILENAME employees="m1:/home/tigergraph/mydata/northwind/data/employees.csv";
        DEFINE HEADER   employees = "EmployeeID","LastName","FirstName","Title","TitleOfCourtesy","BirthDate","HireDate","Address","City","Region","PostalCode","Country","HomePhone","Extension","Photo","Notes","ReportsTo","PhotoPath";

        DEFINE FILENAME employee_territories="m1:/home/tigergraph/mydata/northwind/data/employee-territories.csv";
        DEFINE HEADER   employee_territories = "EmployeeID","TerritoryID";

        DEFINE FILENAME order_details="m1:/home/tigergraph/mydata/northwind/data/order-details.csv";
        DEFINE HEADER   order_details = "OrderID","ProductID","UnitPrice","Quantity","Discount","LineTotalNoDiscount","LineTotal";

        DEFINE FILENAME orders="m1:/home/tigergraph/mydata/northwind/data/orders.csv";
        DEFINE HEADER   orders = "OrderID","CustomerID","EmployeeID","OrderDate","RequiredDate","ShippedDate","ShipVia","Freight","ShipName","ShipAddress","ShipCity","ShipRegion","ShipPostalCode","ShipCountry","OrderAmt","CommissionAmt";

        DEFINE FILENAME products="m1:/home/tigergraph/mydata/northwind/data/products.csv";
        DEFINE HEADER   products = "ProductID","ProductName","SupplierID","CategoryID","QuantityPerUnit","UnitPrice","UnitsInStock","UnitsOnOrder","ReorderLevel","Discontinued";

        DEFINE FILENAME referrals="m1:/home/tigergraph/mydata/northwind/data/referrals.csv";
        DEFINE HEADER   referrals = "from_rep","to_rep","date";

        DEFINE FILENAME regions="m1:/home/tigergraph/mydata/northwind/data/regions.csv";
        DEFINE HEADER   regions = "RegionID","RegionDescription";

        DEFINE FILENAME suppliers="m1:/home/tigergraph/mydata/northwind/data/suppliers.csv";
        DEFINE HEADER   suppliers = "SupplierID","CompanyName","ContactName","ContactTitle","Address","City","Region","PostalCode","Country","Phone","Fax","HomePage";

        DEFINE FILENAME territories="m1:/home/tigergraph/mydata/northwind/data/territories.csv";
        DEFINE HEADER   territories = "TerritoryID","TerritoryDescription","RegionID";

      # File categories
        LOAD categories TO VERTEX Categories VALUES($"CategoryID", $"CategoryName", $"Description") USING SEPARATOR=",", HEADER="true", EOL="\n", USER_DEFINED_HEADER="categories";

      # File customers
        LOAD customers TO VERTEX Customers VALUES($"CustomerID", $"CompanyName", $"ContactName", $"Address", $"City", $"Region", $"PostalCode", $"Country", $"Phone", $"ReferredBy", _, _, _) USING SEPARATOR=",", HEADER="true", EOL="\n", USER_DEFINED_HEADER="customers";

      # File employees
        LOAD employees TO VERTEX Reps VALUES($"EmployeeID", $"LastName", $"FirstName", $"Title", $"TitleOfCourtesy", $"BirthDate", $"HireDate", $"Address", $"City", $"Region", $"PostalCode", $"Country", $"HomePhone", $"Extension", $"ReportsTo", _, _) USING SEPARATOR=",", HEADER="true", EOL="\n";

      # File employee_territories
        LOAD employee_territories TO EDGE operate_in VALUES($"EmployeeID", $"TerritoryID") USING SEPARATOR=",", HEADER="true", EOL="\n";

      # file orders
        LOAD orders TO VERTEX Orders VALUES($"OrderID", $"CustomerID", $"EmployeeID", $"OrderDate", $"RequiredDate", $"ShippedDate", $"ShipVia", $"Freight", $"ShipName", $"ShipAddress", $"ShipCity", $"ShipRegion", $"ShipPostalCode", $"ShipCountry", $"OrderAmt", $"CommissionAmt",_,_) USING SEPARATOR=",", HEADER="true", EOL="\n";
        LOAD orders TO EDGE place VALUES($"CustomerID", $"OrderID", $"CustomerID", $"OrderID") USING SEPARATOR=",", HEADER="true", EOL="\n";
        LOAD orders TO EDGE rep_order VALUES($"EmployeeID", $"OrderID") USING SEPARATOR=",", HEADER="true", EOL="\n";
        LOAD orders TO EDGE rep_customer VALUES($"EmployeeID", $"CustomerID") USING SEPARATOR=",", HEADER="true", EOL="\n";

      # file order_details
        LOAD order_details TO VERTEX Orders VALUES($"OrderID", _, _, _, _, _, _, _, _, _, _, _, _, _, _, _,_,_) USING SEPARATOR=",", HEADER="true", EOL="\n";
        LOAD order_details TO VERTEX Products VALUES($"ProductID", _, _, _, _, _, _, _, _, _) USING SEPARATOR=",", HEADER="true", EOL="\n";
        LOAD order_details TO EDGE order_product VALUES($"OrderID", $"ProductID", $"Quantity", $"UnitPrice", $"Discount", $"LineTotalNoDiscount", $"LineTotal") USING SEPARATOR=",", HEADER="true", EOL="\n";

      # File products
        LOAD products TO VERTEX Products VALUES($"ProductID", $"ProductName", $"SupplierID", $"CategoryID", $"QuantityPerUnit", $"UnitPrice", $"UnitsInStock", $"UnitsOnOrder", $"ReorderLevel", $"Discontinued") USING SEPARATOR=",", HEADER="true", EOL="\n";
        LOAD products TO VERTEX Suppliers VALUES($"SupplierID",_,_,_,_,_,_,_,_,_,_) USING SEPARATOR=",", HEADER="true", EOL="\n";
        LOAD products TO VERTEX Categories VALUES($"CategoryID",_,_) USING SEPARATOR=",", HEADER="true", EOL="\n";
        LOAD products TO EDGE product_supplier VALUES($"ProductID", $"SupplierID") USING SEPARATOR=",", HEADER="true", EOL="\n";
        LOAD products TO EDGE product_category VALUES($"CategoryID", $"ProductID") USING SEPARATOR=",", HEADER="true", EOL="\n";

      # File referrals
        LOAD referrals TO EDGE referredby VALUES("$from_rep", $"to_rep", $"date") USING SEPARATOR=",", HEADER="true", EOL="\n";

      # File regions
        LOAD regions TO VERTEX Regions VALUES($"RegionID", $"RegionDescription") USING SEPARATOR=",", HEADER="true", EOL="\n";

      # File suppliers
        LOAD suppliers TO VERTEX Suppliers VALUES($"SupplierID", $"CompanyName", $"ContactName", $"ContactTitle", $"Address", $"City", $"Region", $"PostalCode", $"Phone", $"Country", $"HomePage") USING SEPARATOR=",", HEADER="true", EOL="\n";

      # File territories
        LOAD territories TO VERTEX Territories VALUES($"TerritoryID", $"TerritoryDescription") USING SEPARATOR=",", HEADER="true", EOL="\n";
        LOAD territories TO EDGE located_in VALUES($"TerritoryID", $"RegionID") USING SEPARATOR=",", HEADER="true", EOL="\n";
      }
      run loading job load_all_north
      drop job load_all_north
    #+end_src
*** Add queries
** Patents Graph by Dan Barkus
*** Resources
*** Schema
    #+begin_src sql :tangle solutions/patents/01-create-schema.gsql :mkdirp yes
      USE GRAPH Patents
      CREATE SCHEMA_CHANGE JOB patent_schema_change FOR GRAPH Patents {
        # Vertices
        ADD VERTEX Application(PRIMARY_ID id STRING, filingDate DATETIME, confirmationNumber STRING, docketNumber STRING, title STRING) WITH PRIMARY_ID_AS_ATTRIBUTE="true";
        ADD VERTEX ContinuationType(PRIMARY_ID id STRING) WITH PRIMARY_ID_AS_ATTRIBUTE="true";
        ADD VERTEX USPCClass(PRIMARY_ID id STRING) WITH PRIMARY_ID_AS_ATTRIBUTE="true";
        ADD VERTEX USPCSubclass(PRIMARY_ID id STRING) WITH PRIMARY_ID_AS_ATTRIBUTE="true";
        ADD VERTEX EventCode(PRIMARY_ID id STRING, description STRING) WITH PRIMARY_ID_AS_ATTRIBUTE="true";
        ADD VERTEX PTAEvent(PRIMARY_ID id STRING, description STRING, applicantDelay FLOAT, usptoDelay FLOAT) WITH PRIMARY_ID_AS_ATTRIBUTE="true";
        ADD VERTEX ExtensionIndicator(PRIMARY_ID id STRING) WITH PRIMARY_ID_AS_ATTRIBUTE="true";
        ADD VERTEX PTASummary(PRIMARY_ID id STRING, ptoDelayA FLOAT, ptoDelayB FLOAT, ptoDelayC FLOAT, overlapDelay FLOAT, nonOverlapDelay FLOAT, manualAdjustment FLOAT, applicationDelay FLOAT, PTA FLOAT) WITH PRIMARY_ID_AS_ATTRIBUTE="true";
        ADD VERTEX PTESummay(PRIMARY_ID id STRING, ptoAdjustment FLOAT, ptoDelay FLOAT, applicantDelay FLOAT, PTE FLOAT) WITH PRIMARY_ID_AS_ATTRIBUTE="true";
        ADD VERTEX ApplicationStatus(PRIMARY_ID id STRING) WITH PRIMARY_ID_AS_ATTRIBUTE="true";
        ADD VERTEX PatentNumber(PRIMARY_ID id STRING) WITH PRIMARY_ID_AS_ATTRIBUTE="true";
        ADD VERTEX Examiner(PRIMARY_ID id STRING, fullName STRING) WITH PRIMARY_ID_AS_ATTRIBUTE="true";
        ADD VERTEX ArtUnit(PRIMARY_ID id STRING) WITH PRIMARY_ID_AS_ATTRIBUTE="true";
        ADD VERTEX Attorney(PRIMARY_ID id STRING, firstName STRING, middleName STRING, lastName STRING, suffix STRING, phone STRING) WITH PRIMARY_ID_AS_ATTRIBUTE="true";
        ADD VERTEX PracticeCategory(PRIMARY_ID id STRING) WITH PRIMARY_ID_AS_ATTRIBUTE="true";
        ADD VERTEX SmallEntity(PRIMARY_ID id STRING) WITH PRIMARY_ID_AS_ATTRIBUTE="true";
        ADD VERTEX First_toFile(PRIMARY_ID id STRING) WITH PRIMARY_ID_AS_ATTRIBUTE="true";
        ADD VERTEX FileLocation(PRIMARY_ID id STRING) WITH PRIMARY_ID_AS_ATTRIBUTE="true";
        ADD VERTEX PGPUBNumber(PRIMARY_ID id STRING) WITH PRIMARY_ID_AS_ATTRIBUTE="true";
        ADD VERTEX WIPONumber(PRIMARY_ID id STRING) WITH PRIMARY_ID_AS_ATTRIBUTE="true";
        ADD VERTEX Inventor(PRIMARY_ID id STRING, first STRING, middle STRING, last STRING) WITH PRIMARY_ID_AS_ATTRIBUTE="true";
        ADD VERTEX ForeignParent(PRIMARY_ID id STRING) WITH PRIMARY_ID_AS_ATTRIBUTE="true";
        ADD VERTEX City(PRIMARY_ID id STRING) WITH PRIMARY_ID_AS_ATTRIBUTE="true";
        ADD VERTEX Region(PRIMARY_ID id STRING) WITH PRIMARY_ID_AS_ATTRIBUTE="true";
        ADD VERTEX Country(PRIMARY_ID id STRING) WITH PRIMARY_ID_AS_ATTRIBUTE="true";
        ADD VERTEX PostalCode(PRIMARY_ID id STRING) WITH PRIMARY_ID_AS_ATTRIBUTE="true";
        ADD VERTEX Correspondence(PRIMARY_ID id STRING, name STRING, customerNumber STRING) WITH PRIMARY_ID_AS_ATTRIBUTE="true";
        ADD VERTEX Address(PRIMARY_ID id STRING, line1 STRING, line2 STRING, line3 STRING) WITH PRIMARY_ID_AS_ATTRIBUTE="true";
        # Edges
        ADD DIRECTED EDGE has_child(FROM Application, TO Application, date DATETIME) WITH REVERSE_EDGE="reverse_has_child";
        ADD DIRECTED EDGE has_parent(FROM Application, TO Application, date DATETIME) WITH REVERSE_EDGE="reverse_has_parent";
        ADD UNDIRECTED EDGE is_continuation_type(FROM Application, TO ContinuationType);
        ADD UNDIRECTED EDGE has_class(FROM Application, TO USPCClass);
        ADD UNDIRECTED EDGE has_subclass(FROM Application, TO USPCSubclass);
        ADD DIRECTED EDGE is_subclass(FROM USPCSubclass, TO USPCClass) WITH REVERSE_EDGE="reverse_is_subclass";
        ADD UNDIRECTED EDGE has_code(FROM Application, TO EventCode, date DATETIME);
        ADD UNDIRECTED EDGE has_pta_event(FROM Application, TO PTAEvent, date DATETIME);
        ADD UNDIRECTED EDGE is_extension(FROM PTAEvent, TO ExtensionIndicator);
        ADD DIRECTED EDGE has_start(FROM PTAEvent, TO PTAEvent) WITH REVERSE_EDGE="reverse_has_start";
        ADD UNDIRECTED EDGE has_pta_summary(FROM Application, TO PTASummary);
        ADD UNDIRECTED EDGE has_pte_summary(FROM Application, TO PTESummay);
        ADD UNDIRECTED EDGE has_status(FROM Application, TO ApplicationStatus, date DATETIME);
        ADD UNDIRECTED EDGE has_patent(FROM Application, TO PatentNumber, date DATETIME);
        ADD UNDIRECTED EDGE has_examiner(FROM Application, TO Examiner);
        ADD UNDIRECTED EDGE from_unit(FROM Examiner, TO ArtUnit);
        ADD UNDIRECTED EDGE has_attorney(FROM Application, TO Attorney);
        ADD UNDIRECTED EDGE has_practice_category(FROM Attorney, TO PracticeCategory);
        ADD UNDIRECTED EDGE is_small(FROM Application, TO SmallEntity);
        ADD UNDIRECTED EDGE follows_ftf(FROM Application, TO First_toFile);
        ADD UNDIRECTED EDGE at_location(FROM Application, TO FileLocation, date DATETIME);
        ADD UNDIRECTED EDGE has_PGPUB(FROM Application, TO PGPUBNumber, date DATETIME);
        ADD UNDIRECTED EDGE has_WIPO(FROM Application, TO WIPONumber, date DATETIME);
        ADD UNDIRECTED EDGE filed_application(FROM Application, TO Inventor, rank INT);
        ADD UNDIRECTED EDGE has_foreign_parent(FROM Application, TO ForeignParent, date DATETIME);
        ADD UNDIRECTED EDGE from_city(FROM Inventor, TO City);
        ADD UNDIRECTED EDGE from_region(FROM Inventor, TO Region);
        ADD UNDIRECTED EDGE from_country(FROM Inventor, TO Country);
        ADD UNDIRECTED EDGE filed_country(FROM ForeignParent, TO Country);
        ADD UNDIRECTED EDGE has_correspondence(FROM Inventor, TO Correspondence);
        ADD UNDIRECTED EDGE has_address(FROM Address, TO Correspondence);
        # Compound edges
        ADD UNDIRECTED EDGE in_code(FROM City, TO PostalCode | FROM Correspondence, TO PostalCode | FROM Address, TO PostalCode);
        ADD UNDIRECTED EDGE in_region(FROM PostalCode, TO Region | FROM Correspondence, TO Region | FROM Address, TO Region);
        ADD UNDIRECTED EDGE in_country(FROM PostalCode, TO Country | FROM Country, TO Region | FROM Correspondence, TO Country | FROM Address, TO Country);
        ADD UNDIRECTED EDGE in_city(FROM Region, TO City | FROM Correspondence, TO City | FROM Address, TO City);
      }
      RUN SCHEMA_CHANGE JOB patent_schema_change
      DROP JOB patent_schema_change
    #+end_src
*** Data
    #+begin_src sql :tangle solutions/patents/02-load-data.gsql :mkdirp yes
      USE GRAPH Patents
      CREATE LOADING JOB load_patents_all FOR GRAPH Patents {
        DEFINE FILENAME inventors = "m1:/home/tigergraph/mydata/patents/data/all_inventors.csv";
        DEFINE FILENAME application_data = "m1:/home/tigergraph/mydata/patents/data/application_data.csv";
        DEFINE FILENAME attorney_agent = "m1:/home/tigergraph/mydata/patents/data/attorney_agent.csv";
        DEFINE FILENAME continuity_children = "m1:/home/tigergraph/mydata/patents/data/continuity_children.csv";
        DEFINE FILENAME continuity_parents = "m1:/home/tigergraph/mydata/patents/data/continuity_parents.csv";
        DEFINE FILENAME correspondence_address = "m1:/home/tigergraph/mydata/patents/data/correspondence_address.csv";
        DEFINE FILENAME event_codes = "m1:/home/tigergraph/mydata/patents/data/event_codes.csv";
        DEFINE FILENAME foreign_priority = "m1:/home/tigergraph/mydata/patents/data/foreign_priority.csv";
        DEFINE FILENAME pat_term_adj = "m1:/home/tigergraph/mydata/patents/data/pat_term_adj.csv";
        DEFINE FILENAME pta_summary = "m1:/home/tigergraph/mydata/patents/data/pta_summary.csv";
        DEFINE FILENAME pte_summary = "m1:/home/tigergraph/mydata/patents/data/pte_summary.csv";
        DEFINE FILENAME transactions = "m1:/home/tigergraph/mydata/patents/data/transactions.csv";

            LOAD inventors TO VERTEX Inventor VALUES(gsql_concat($1,",",$3," ",$2), $1, $2, $3) USING SEPARATOR=",", HEADER="false", EOL="\n", QUOTE="double";
            LOAD inventors TO EDGE filed_application VALUES($0, gsql_concat($1,",",$3," ",$2), $4) USING SEPARATOR=",", HEADER="false", EOL="\n", QUOTE="double";
            LOAD inventors TO EDGE from_city VALUES(gsql_concat($1,",",$3," ",$2), $5) USING SEPARATOR=",", HEADER="false", EOL="\n", QUOTE="double";
            LOAD inventors TO EDGE from_region VALUES(gsql_concat($1,",",$3," ",$2), $6) USING SEPARATOR=",", HEADER="false", EOL="\n", QUOTE="double";
            LOAD inventors TO EDGE from_country VALUES(gsql_concat($1,",",$3," ",$2), $7) USING SEPARATOR=",", HEADER="false", EOL="\n", QUOTE="double";

            LOAD application_data TO VERTEX Application VALUES($0, $1, $7, $8, $19) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
            LOAD application_data TO EDGE has_examiner VALUES($0, $3) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
            LOAD application_data TO EDGE from_unit VALUES($3, $4) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
            LOAD application_data TO EDGE has_class VALUES($0, $5) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
            LOAD application_data TO EDGE has_subclass VALUES($0, $6) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
            LOAD application_data TO EDGE is_subclass VALUES($6, $5) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
            LOAD application_data TO EDGE has_status VALUES($0, $9, $10) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
            LOAD application_data TO EDGE has_patent VALUES($0, $17, $18) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
            LOAD application_data TO EDGE is_small VALUES($0, $20) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
            LOAD application_data TO EDGE follows_ftf VALUES($0, $21) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
            LOAD application_data TO EDGE at_location VALUES($0, $11, $12) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
            LOAD application_data TO EDGE has_PGPUB VALUES($0, $13, $14) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";

            LOAD attorney_agent TO VERTEX Attorney VALUES($5, $0, $2, $1, $3, $4) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
            LOAD attorney_agent TO EDGE has_attorney VALUES($7, $5) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
            LOAD attorney_agent TO EDGE has_practice_category VALUES($5, $6) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";

            LOAD continuity_children TO EDGE has_child VALUES($0, $1, $2) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";

            LOAD continuity_parents TO EDGE has_parent VALUES($0, $1, $2) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
            LOAD continuity_parents TO EDGE is_continuation_type VALUES($0, $3) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";

            LOAD correspondence_address TO VERTEX Correspondence VALUES($9, $1, $9) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
            LOAD correspondence_address TO VERTEX Address VALUES(gsql_concat($2,$3,$4), $2, $3, $4) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
            LOAD correspondence_address TO EDGE has_address VALUES(gsql_concat($2,$3,$4), $9) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
            LOAD correspondence_address TO EDGE in_code VALUES($9 Correspondence, $6 PostalCode) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
            LOAD correspondence_address TO EDGE in_city VALUES($9 Correspondence, $5 City) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
            LOAD correspondence_address TO EDGE in_region VALUES($9 Correspondence, $7 Region) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
            LOAD correspondence_address TO EDGE in_country VALUES($9 Correspondence, $8 Country) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
            LOAD correspondence_address TO EDGE in_code VALUES(gsql_concat($2,$3,$4) Address, $6 PostalCode) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
            LOAD correspondence_address TO EDGE in_city VALUES(gsql_concat($2,$3,$4) Address, $5 City) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
            LOAD correspondence_address TO EDGE in_region VALUES(gsql_concat($2,$3,$4) Address, $7 Region) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
            LOAD correspondence_address TO EDGE in_country VALUES(gsql_concat($2,$3,$4) Address, $8 Country) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";

            LOAD event_codes TO VERTEX EventCode VALUES($0, $1) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";

            LOAD foreign_priority TO EDGE has_foreign_parent VALUES($0, $1, $2) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
            LOAD foreign_priority TO EDGE filed_country VALUES($1, $3) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";

            LOAD pat_term_adj TO VERTEX PTAEvent VALUES(gsql_uuid_v4($1), $3, $4, $5) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
            LOAD pat_term_adj TO EDGE has_pta_event VALUES($0, gsql_uuid_v4($1), $2) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";
            LOAD pat_term_adj TO EDGE is_extension VALUES(gsql_uuid_v4($1), $7) USING SEPARATOR=",", HEADER="true", EOL="\n", QUOTE="double";

            LOAD transactions TO EDGE has_code VALUES($0, $1, $2) USING SEPARATOR=",", HEADER="false", EOL="\n", QUOTE="double";
      }

      RUN LOADING JOB load_patents_all
      DROP JOB load_patents_all

    #+end_src
*** Queries
** STARTED Real estate leasing
   :LOGBOOK:
   - State "STARTED"    from "DONE"       [2021-11-02 Tue 12:13]
   - State "DONE"       from "TODO"       [2021-11-02 Tue 12:13]
   :END:
   - private repo
** TODO Social Graph
   :LOGBOOK:
   - State "DONE"       from "TODO"       [2021-11-02 Tue 12:13]
   :END:
*** Resources
*** Create graph
    #+begin_src sql :tangle solutions/ldbc-social/01-create-graph.gsql :mkdirp yes
      //clear the current catalog.
      // It may take a while since it restarts the subsystem services.
      USE GLOBAL
      DROP GRAPH ldbc_snb

      # 1. Create graph
      CREATE GRAPH ldbc_snb ()
      USE GRAPH ldbc_snb

      # 2. Create schema_change job to include all vertex/edge types
      CREATE SCHEMA_CHANGE JOB change_schema_of_ldbc  FOR GRAPH ldbc_snb {
        ## Post and Comment
        ADD VERTEX Comment (PRIMARY_ID id UINT, creationDate DATETIME, locationIP STRING,
          browserUsed STRING, content STRING, length UINT) WITH primary_id_as_attribute="TRUE";

        ADD VERTEX Post (PRIMARY_ID id UINT, imageFile STRING, creationDate DATETIME,
          locationIP STRING, browserUsed STRING, lang STRING, content STRING,
          length UINT) WITH primary_id_as_attribute="TRUE";
        ## organisation
        ADD VERTEX Company (PRIMARY_ID id UINT, name STRING, url STRING) WITH primary_id_as_attribute="TRUE";
        ADD VERTEX University (PRIMARY_ID id UINT, name STRING, url STRING) WITH primary_id_as_attribute="TRUE";
        ## place
        ADD VERTEX City (PRIMARY_ID id UINT, name STRING, url STRING) WITH primary_id_as_attribute="TRUE";
        ADD VERTEX Country (PRIMARY_ID id UINT, name STRING, url STRING) WITH primary_id_as_attribute="TRUE";
        ADD VERTEX Continent (PRIMARY_ID id UINT, name STRING, url STRING) WITH primary_id_as_attribute="TRUE";
        ## etc
        ADD  VERTEX Forum (PRIMARY_ID id UINT, title STRING, creationDate DATETIME) WITH primary_id_as_attribute="TRUE";
        ADD  VERTEX Person (PRIMARY_ID id UINT, firstName STRING, lastName STRING, gender STRING, birthday DATETIME,
         creationDate DATETIME, locationIP STRING, browserUsed STRING, speaks set<STRING>, email set<STRING>)
         WITH primary_id_as_attribute="TRUE";
        ADD VERTEX Tag (PRIMARY_ID id UINT, name STRING, url STRING) WITH primary_id_as_attribute="TRUE";
        ADD VERTEX TagClass (PRIMARY_ID id UINT, name STRING, url STRING) WITH primary_id_as_attribute="TRUE";

        // create edge types
        ADD DIRECTED EDGE CONTAINER_OF (FROM Forum, TO Post) WITH REVERSE_EDGE="CONTAINER_OF_REVERSE";
        ADD  DIRECTED EDGE HAS_CREATOR (FROM Comment|Post, TO Person) WITH REVERSE_EDGE="HAS_CREATOR_REVERSE";
        ADD  DIRECTED EDGE HAS_INTEREST (FROM Person, TO Tag) WITH REVERSE_EDGE="HAS_INTEREST_REVERSE";
        ADD DIRECTED EDGE HAS_MEMBER (FROM Forum, TO Person, joinDate DATETIME) WITH REVERSE_EDGE="HAS_MEMBER_REVERSE";
        ADD DIRECTED EDGE HAS_MODERATOR (FROM Forum, TO Person) WITH REVERSE_EDGE="HAS_MODERATOR_REVERSE";
        ADD DIRECTED EDGE HAS_TAG (FROM Comment|Post|Forum, TO Tag) WITH REVERSE_EDGE="HAS_TAG_REVERSE";
        ADD DIRECTED EDGE HAS_TYPE (FROM Tag, TO TagClass) WITH REVERSE_EDGE="HAS_TYPE_REVERSE";
        ADD  DIRECTED EDGE IS_LOCATED_IN (FROM Comment, TO Country
                                        | FROM Post, TO Country
                                        | FROM Company, TO Country
                                        | FROM Person, TO City
                                        | FROM University, TO City) WITH REVERSE_EDGE="IS_LOCATED_IN_REVERSE";
        ADD DIRECTED EDGE IS_PART_OF (FROM City, TO Country
                                     | FROM Country, TO Continent) WITH REVERSE_EDGE="IS_PART_OF_REVERSE";
        ADD DIRECTED EDGE IS_SUBCLASS_OF (FROM TagClass, TO TagClass) WITH REVERSE_EDGE="IS_SUBCLASS_OF_REVERSE";
        ADD UNDIRECTED EDGE KNOWS (FROM Person, TO Person, creationDate DATETIME);
        ADD DIRECTED EDGE LIKES (FROM Person, TO Comment|Post, creationDate DATETIME) WITH REVERSE_EDGE="LIKES_REVERSE";
        ADD DIRECTED EDGE REPLY_OF (FROM Comment, TO Comment|Post) WITH REVERSE_EDGE="REPLY_OF_REVERSE";
        ADD DIRECTED EDGE STUDY_AT (FROM Person, TO University, classYear INT) WITH REVERSE_EDGE="STUDY_AT_REVERSE";
        ADD DIRECTED EDGE WORK_AT (FROM Person, TO Company, workFrom INT) WITH REVERSE_EDGE="WORK_AT_REVERSE";
      }

      # 3. Run schema_change job
      RUN SCHEMA_CHANGE JOB change_schema_of_ldbc

      # 4. Drop schema_change job
      DROP JOB change_schema_of_ldbc
    #+end_src
*** Load data
    #+begin_src sql :tangle solutions/ldbc-social/02-load-data.gsql :mkdirp yes
      USE GRAPH ldbc_snb
      CREATE LOADING JOB load_ldbc_snb FOR GRAPH ldbc_snb {
        DEFINE FILENAME v_person_file="m1:/home/tigergraph/mydata/ldbc-social/data/person_0_0.csv";
        DEFINE FILENAME v_post_file="m1:/home/tigergraph/mydata/ldbc-social/data/post_0_0.csv";
        DEFINE FILENAME v_tag_file="m1:/home/tigergraph/mydata/ldbc-social/data/tag_0_0.csv";
        DEFINE FILENAME v_place_file="m1:/home/tigergraph/mydata/ldbc-social/data/place_0_0.csv";
        DEFINE FILENAME v_comment_file="m1:/home/tigergraph/mydata/ldbc-social/data/comment_0_0.csv";
        DEFINE FILENAME v_forum_file="m1:/home/tigergraph/mydata/ldbc-social/data/forum_0_0.csv";
        DEFINE FILENAME v_organisation_file="m1:/home/tigergraph/mydata/ldbc-social/data/organisation_0_0.csv";
        DEFINE FILENAME v_tagclass_file="m1:/home/tigergraph/mydata/ldbc-social/data/tagclass_0_0.csv";
        DEFINE FILENAME person_knows_person_file="m1:/home/tigergraph/mydata/ldbc-social/data/person_knows_person_0_0.csv";
        DEFINE FILENAME comment_replyOf_post_file="m1:/home/tigergraph/mydata/ldbc-social/data/comment_replyOf_post_0_0.csv";
        DEFINE FILENAME comment_replyOf_comment_file="m1:/home/tigergraph/mydata/ldbc-social/data/comment_replyOf_comment_0_0.csv";
        DEFINE FILENAME post_hasCreator_person_file="m1:/home/tigergraph/mydata/ldbc-social/data/post_hasCreator_person_0_0.csv";
        DEFINE FILENAME post_hasTag_tag_file="m1:/home/tigergraph/mydata/ldbc-social/data/post_hasTag_tag_0_0.csv";
        DEFINE FILENAME comment_hasCreator_person_file="m1:/home/tigergraph/mydata/ldbc-social/data/comment_hasCreator_person_0_0.csv";
        DEFINE FILENAME post_isLocatedIn_place_file="m1:/home/tigergraph/mydata/ldbc-social/data/post_isLocatedIn_place_0_0.csv";
        DEFINE FILENAME comment_hasTag_tag_file="m1:/home/tigergraph/mydata/ldbc-social/data/comment_hasTag_tag_0_0.csv";
        DEFINE FILENAME comment_isLocatedIn_place_file="m1:/home/tigergraph/mydata/ldbc-social/data/comment_isLocatedIn_place_0_0.csv";
        DEFINE FILENAME forum_containerOf_post_file="m1:/home/tigergraph/mydata/ldbc-social/data/forum_containerOf_post_0_0.csv";
        DEFINE FILENAME forum_hasMember_person_file="m1:/home/tigergraph/mydata/ldbc-social/data/forum_hasMember_person_0_0.csv";
        DEFINE FILENAME forum_hasModerator_person_file="m1:/home/tigergraph/mydata/ldbc-social/data/forum_hasModerator_person_0_0.csv";
        DEFINE FILENAME forum_hasTag_tag_file="m1:/home/tigergraph/mydata/ldbc-social/data/forum_hasTag_tag_0_0.csv";
        DEFINE FILENAME organisation_isLocatedIn_place_file="m1:/home/tigergraph/mydata/ldbc-social/data/organisation_isLocatedIn_place_0_0.csv";
        DEFINE FILENAME person_hasInterest_tag_file="m1:/home/tigergraph/mydata/ldbc-social/data/person_hasInterest_tag_0_0.csv";
        DEFINE FILENAME person_isLocatedIn_place_file="m1:/home/tigergraph/mydata/ldbc-social/data/person_isLocatedIn_place_0_0.csv";
        DEFINE FILENAME person_likes_comment_file="m1:/home/tigergraph/mydata/ldbc-social/data/person_likes_comment_0_0.csv";
        DEFINE FILENAME person_likes_post_file="m1:/home/tigergraph/mydata/ldbc-social/data/person_likes_post_0_0.csv";
        DEFINE FILENAME person_studyAt_organisation_file="m1:/home/tigergraph/mydata/ldbc-social/data/person_studyAt_organisation_0_0.csv";
        DEFINE FILENAME person_workAt_organisation_file="m1:/home/tigergraph/mydata/ldbc-social/data/person_workAt_organisation_0_0.csv";
        DEFINE FILENAME place_isPartOf_place_file="m1:/home/tigergraph/mydata/ldbc-social/data/place_isPartOf_place_0_0.csv";
        DEFINE FILENAME tag_hasType_tagclass_file="m1:/home/tigergraph/mydata/ldbc-social/data/tag_hasType_tagclass_0_0.csv";
        DEFINE FILENAME tagclass_isSubclassOf_tagclass_file="m1:/home/tigergraph/mydata/ldbc-social/data/tagclass_isSubclassOf_tagclass_0_0.csv";





        // load vertex
        LOAD v_comment_file
          TO VERTEX Comment VALUES ($0, $1, $2, $3, $4, $5) USING header="true", separator="|";
        LOAD v_post_file
          TO VERTEX Post VALUES ($0, $1, $2, $3, $4, $5, $6, $7) USING header="true", separator="|";
        LOAD v_organisation_file
          TO VERTEX Company VALUES ($0, $2, $3) WHERE $1=="company",
          TO VERTEX University VALUES ($0, $2, $3) WHERE $1=="university" USING header="true", separator="|";
        LOAD v_place_file
          TO VERTEX City VALUES ($0, $1, $2) WHERE $3=="city",
          TO VERTEX Country VALUES ($0, $1, $2) WHERE $3=="country",
          TO VERTEX Continent VALUES ($0, $1, $2) WHERE $3=="continent" USING header="true", separator="|";
        LOAD v_forum_file
          TO VERTEX Forum VALUES ($0, $1, $2) USING header="true", separator="|";
        LOAD v_person_file
          TO VERTEX Person VALUES ($0, $1, $2, $3, $4, $5, $6, $7, SPLIT($8,";"), SPLIT($9,";")) USING header="true", separator="|";
        LOAD v_tag_file
          TO VERTEX Tag VALUES ($0, $1, $2) USING header="true", separator="|";
        LOAD v_tagclass_file
          TO VERTEX TagClass VALUES ($0, $1, $2) USING header="true", separator="|";

        // load edge
        LOAD forum_containerOf_post_file
          TO EDGE CONTAINER_OF VALUES ($0, $1) USING header="true", separator="|";
        LOAD comment_hasCreator_person_file
          TO EDGE HAS_CREATOR VALUES ($0 Comment, $1) USING header="true", separator="|";
        LOAD post_hasCreator_person_file
          TO EDGE HAS_CREATOR VALUES ($0 Post, $1) USING header="true", separator="|";
        LOAD person_hasInterest_tag_file
          TO EDGE HAS_INTEREST VALUES ($0, $1) USING header="true", separator="|";
        LOAD forum_hasMember_person_file
          TO EDGE HAS_MEMBER VALUES ($0, $1, $2) USING header="true", separator="|";
        LOAD forum_hasModerator_person_file
          TO EDGE HAS_MODERATOR VALUES ($0, $1) USING header="true", separator="|";
        LOAD comment_hasTag_tag_file
          TO EDGE HAS_TAG VALUES ($0 Comment, $1) USING header="true", separator="|";
        LOAD post_hasTag_tag_file
          TO EDGE HAS_TAG VALUES ($0 Post, $1) USING header="true", separator="|";
        LOAD forum_hasTag_tag_file
          TO EDGE HAS_TAG VALUES ($0 Forum, $1) USING header="true", separator="|";
        LOAD tag_hasType_tagclass_file
          TO EDGE HAS_TYPE VALUES ($0, $1) USING header="true", separator="|";
        LOAD organisation_isLocatedIn_place_file
          TO EDGE IS_LOCATED_IN VALUES ($0 Company, $1 Country) WHERE to_int($1) < 111,
          TO EDGE IS_LOCATED_IN VALUES ($0 University, $1 City) WHERE to_int($1) > 110 USING header="true", separator="|";
        LOAD comment_isLocatedIn_place_file
          TO EDGE IS_LOCATED_IN VALUES ($0 Comment, $1 Country) USING header="true", separator="|";
        LOAD post_isLocatedIn_place_file
          TO EDGE IS_LOCATED_IN VALUES ($0 Post, $1 Country) USING header="true", separator="|";
        LOAD person_isLocatedIn_place_file
          TO EDGE IS_LOCATED_IN VALUES ($0 Person, $1 City) USING header="true", separator="|";
        LOAD place_isPartOf_place_file
          TO EDGE IS_PART_OF VALUES ($0 Country, $1 Continent) WHERE to_int($0) < 111,
          TO EDGE IS_PART_OF VALUES ($0 City, $1 Country) WHERE to_int($0) > 110 USING header="true", separator="|";
        LOAD tagclass_isSubclassOf_tagclass_file
          TO EDGE IS_SUBCLASS_OF VALUES ($0, $1) USING header="true", separator="|";
        LOAD person_knows_person_file
          TO EDGE KNOWS VALUES ($0, $1, $2) USING header="true", separator="|";
        LOAD person_likes_comment_file
          TO EDGE LIKES VALUES ($0, $1 Comment, $2) USING header="true", separator="|";
        LOAD person_likes_post_file
          TO EDGE LIKES VALUES ($0, $1 Post, $2) USING header="true", separator="|";
        LOAD comment_replyOf_comment_file
          TO EDGE REPLY_OF VALUES ($0, $1 Comment) USING header="true", separator="|";
        LOAD comment_replyOf_post_file
          TO EDGE REPLY_OF VALUES ($0, $1 Post) USING header="true", separator="|";
        LOAD person_studyAt_organisation_file
          TO EDGE STUDY_AT VALUES ($0, $1, $2) USING header="true", separator="|";
        LOAD person_workAt_organisation_file
          TO EDGE WORK_AT VALUES ($0, $1, $2) USING header="true", separator="|";
      }

      RUN LOADING JOB load_ldbc_snb

      DROP JOB load_ldbc_snb
    #+end_src
*** Add queries
    #+begin_src sql :tangle solutions/ldbc-social/03-create-queries.gsql
    #+end_src
** STARTED Supply Chain
   :LOGBOOK:
   - State "STARTED"    from "TODO"       [2021-10-11 Mon 19:47]
   CLOCK: [2021-10-11 Mon 19:47]--[2021-10-11 Mon 20:52] =>  1:05
   :END:
*** Resources
     + Data: /data/data-files/graph-data/supply-chain/supplychain-data.tar.gz
*** Graph creation
     #+begin_src sql :tangle solutions/supplychain/01-create-graph.gsql :mkdirp yes
       USE GLOBAL
       DROP GRAPH SupplyChain

       CREATE GRAPH SupplyChain()
       USE GRAPH SupplyChain
       CREATE SCHEMA_CHANGE JOB schema_change_job_RwBCZV FOR GRAPH SupplyChain {
         ADD VERTEX models(PRIMARY_ID ModelNumber STRING) WITH STATS="OUTDEGREE_BY_EDGETYPE", PRIMARY_ID_AS_ATTRIBUTE="true";
         ADD VERTEX warehouse(PRIMARY_ID WarehouseId STRING) WITH STATS="OUTDEGREE_BY_EDGETYPE", PRIMARY_ID_AS_ATTRIBUTE="true";
         ADD VERTEX BillofMaterial(PRIMARY_ID id STRING, item_number STRING, model_number STRING, model_year STRING, model_name STRING, parent_item_number STRING, level_in_bom STRING, site_id STRING, quantity_per INT, cost FLOAT) WITH STATS="OUTDEGREE_BY_EDGETYPE", PRIMARY_ID_AS_ATTRIBUTE="true";
         ADD VERTEX BuildSchedule(PRIMARY_ID id STRING, item_number STRING, order_number STRING, order_duedate DATETIME, warehouse STRING, open_qty INT, order_qty INT, deviation_qty INT, received_qty INT, load_date DATETIME) WITH STATS="OUTDEGREE_BY_EDGETYPE", PRIMARY_ID_AS_ATTRIBUTE="true";
         ADD VERTEX item(PRIMARY_ID id STRING) WITH STATS="OUTDEGREE_BY_EDGETYPE", PRIMARY_ID_AS_ATTRIBUTE="true";
         ADD UNDIRECTED EDGE part_of(FROM BillofMaterial, TO BillofMaterial);
         ADD UNDIRECTED EDGE build_plan(FROM BuildSchedule, TO warehouse);
         ADD UNDIRECTED EDGE scheduled_to_build(FROM BuildSchedule, TO models);
         ADD UNDIRECTED EDGE model_bom(FROM BillofMaterial, TO models);
         ADD UNDIRECTED EDGE item_in_bom(FROM BillofMaterial, TO item);
         ADD UNDIRECTED EDGE in_warehouse_inventory(FROM item, TO warehouse, qtyonhand INT);
         ADD UNDIRECTED EDGE used_by(FROM models, TO item);
         ADD UNDIRECTED EDGE at_risk_used_by(FROM models, TO item, SITEID STRING, ORDERMONTH STRING, DEMAND FLOAT, BOMQTY FLOAT, REQUIREDPARTQTY FLOAT);
       }
       RUN SCHEMA_CHANGE JOB schema_change_job_RwBCZV
       DROP JOB schema_change_job_RwBCZV
     #+end_src
*** Load data
    #+begin_src sql :tangle solutions/supplychain/02-load-data.gsql
      set exit_on_error = "false"
      USE GRAPH SupplyChain
      CREATE LOADING JOB load_job_supplychain FOR GRAPH SupplyChain {
      DEFINE FILENAME Model_Number_Only="m1:/home/tigergraph/mydata/supplychain/data/Model Number Only.csv";
      DEFINE FILENAME ModelItemNumber="m1:/home/tigergraph/mydata/supplychain/data/ModelItemNumber.csv";
      DEFINE FILENAME Inventory="m1:/home/tigergraph/mydata/supplychain/data/Inventory.csv";
      DEFINE FILENAME BuildSchedule_July="m1:/home/tigergraph/mydata/supplychain/data/BuildSchedule July 21.csv";
      DEFINE FILENAME ItemMaster="m1:/home/tigergraph/mydata/supplychain/data/ItemMaster POC.csv";
      DEFINE FILENAME BOM="m1:/home/tigergraph/mydata/supplychain/data/BOM with Unique Identifier.csv";
      DEFINE FILENAME demandtopartusedby="m1:/home/tigergraph/mydata/supplychain/data/demandtopartusedby.csv";


      LOAD Model_Number_Only TO VERTEX models VALUES($0) USING SEPARATOR=",", HEADER="true", EOL="\n";

      LOAD ModelItemNumber TO EDGE used_by VALUES($0, $1) USING SEPARATOR=",", HEADER="true", EOL="\n";


      LOAD Inventory TO VERTEX warehouse VALUES($1) USING SEPARATOR=",", HEADER="true", EOL="\n";
      LOAD Inventory TO EDGE in_warehouse_inventory VALUES($0, $1, $2) USING SEPARATOR=",", HEADER="true", EOL="\n";

      LOAD BuildSchedule_July TO VERTEX BuildSchedule VALUES(gsql_concat($0,$3,$2), $0, $1, $2, _, $4, $5, $6, $7, $8) USING SEPARATOR=",", HEADER="true", EOL="\n";
      LOAD BuildSchedule_July TO EDGE build_plan VALUES(gsql_concat($0,$3,$2), $3) USING SEPARATOR=",", HEADER="true", EOL="\n";
      LOAD BuildSchedule_July TO EDGE scheduled_to_build VALUES(gsql_concat($0,$3,$2), $0) USING SEPARATOR=",", HEADER="true", EOL="\n";

      LOAD ItemMaster TO VERTEX item VALUES($0) USING SEPARATOR=",", HEADER="true", EOL="\n";

      LOAD BOM TO VERTEX BillofMaterial VALUES($9, $0, $1, $2, _, $4, $5, $6, $7, $8) USING SEPARATOR=",", HEADER="true", EOL="\n";
      LOAD BOM TO EDGE model_bom VALUES($9, $1) USING SEPARATOR=",", HEADER="true", EOL="\n";
      LOAD BOM TO EDGE item_in_bom VALUES($9, $0) USING SEPARATOR=",", HEADER="true", EOL="\n";

      LOAD demandtopartusedby TO EDGE at_risk_used_by VALUES($0, $2, $1, $3, $4, $5, $6) USING SEPARATOR=",", HEADER="true", EOL="\n";
      }
      RUN LOADING JOB load_job_supplychain
      DROP JOB load_job_supplychain
      set exit_on_error = "true"
    #+end_src
*** Queries
    #+begin_src sql :tangle solutions/supplychain/queries.gsql
      set exit_on_error = "true"
      CREATE QUERY scc (SET<STRING> v_type, SET<STRING> e_type, SET<STRING> rev_e_type,
        INT top_k_dist, INT output_limit, INT max_iter = 500, INT iter_wcc = 5, BOOL print_accum = TRUE, STRING result_attr= "", STRING file_path=""){ //INT iter_end_trim = 3
      /* This query detects strongly connected components based on the following papers:
       ,* https://www.sandia.gov/~apinar/papers/irreg00.pdf
       ,* https://www.sciencedirect.com/science/article/pii/S0743731505000535
       ,* https://stanford-ppl.github.io/website/papers/sc13-hong.pdf

       ,* iter: number of iteration of the algorithm
       ,* iter_wcc: find weakly connected components for the active vertices in this iteration, since the largest sccs are already found after several iterations; usually a small number(3 to 10)
       ,* top_k_dist: top k result in scc distribution

       ,* DISTRIBUTED QUERY mode for this query is supported from TG 2.4.
       ,*/
          TYPEDEF TUPLE <INT csize, INT num> cluster_num;
          MapAccum<INT, INT> @@cluster_size_map, @@cluster_dist_map;
          HeapAccum<cluster_num>(top_k_dist, csize DESC) @@cluster_dist_heap;
          OrAccum @is_forward, @is_backward, @detached, @has_pos_indegree, @has_pos_outdegree, @wcc_active;
          SumAccum<INT> @cid, @vid;
          MinAccum<INT> @@min_vid, @wcc_id_curr, @wcc_id_prev;
          SumAccum<STRING> @flag;
          MapAccum<INT, MinAccum<INT>> @@f_cid_map, @@b_cid_map, @@n_cid_map, @@s_cid_map;
          FILE f (file_path);
          INT i = 1;
          v_all = {v_type};
          tmp(ANY) ={};

          active = SELECT s
                  FROM v_all:s
                  ACCUM s.@vid = getvid(s),
                        @@min_vid += getvid(s)
                  POST-ACCUM s.@cid = @@min_vid;

          WHILE active.size()>0 LIMIT max_iter DO

              WHILE TRUE DO
                  tmp =  SELECT s
                         FROM active:s -(e_type:e) -> :t
                         WHERE t.@detached == FALSE AND s.@cid == t.@cid
                         ACCUM s.@has_pos_outdegree = TRUE;

                  tmp =  SELECT s
                         FROM active:s -(rev_e_type:e) -> :t
                         WHERE t.@detached == FALSE AND s.@cid == t.@cid
                         ACCUM s.@has_pos_indegree = TRUE;
                  trim_set = SELECT s
                         FROM active:s
                         WHERE s.@has_pos_indegree == FALSE OR s.@has_pos_outdegree == FALSE
                         ACCUM s.@detached = TRUE,
                               s.@cid = s.@vid;


                  IF trim_set.size() == 0 THEN  // no single SCC anymore, terminate the while loop
                          BREAK;
                  END;
                  active = SELECT s
                           FROM active:s
                           WHERE s.@detached == FALSE
                           ACCUM @@n_cid_map += (s.@cid -> s.@vid)
                           POST-ACCUM s.@cid = @@n_cid_map.get(s.@cid),
                                      s.@has_pos_indegree = FALSE,
                                      s.@has_pos_outdegree = FALSE;
                  @@n_cid_map.clear();
              END;
              //END;
              // get WCC
              IF i == iter_wcc THEN
                      active = SELECT s
                               FROM active:s
                               POST-ACCUM s.@wcc_id_curr = s.@vid,
                                          s.@wcc_id_prev = s.@vid;
                      curr = active;
                      WHILE (curr.size()>0) DO
                              curr = SELECT t
                                     FROM curr:s -((e_type|rev_e_type):e)-> :t
                                     WHERE s.@cid == t.@cid AND t.@detached == FALSE
                                     ACCUM t.@wcc_id_curr += s.@wcc_id_prev // If s has a smaller id than t, copy the id to t
                                     POST-ACCUM
                                              CASE WHEN t.@wcc_id_prev != t.@wcc_id_curr THEN // If t's id has changed
                                                        t.@wcc_id_prev = t.@wcc_id_curr,
                                                        t.@wcc_active = true
                                              ELSE
                                                        t.@wcc_active = false
                                              END
                                      HAVING t.@wcc_active == true;
                      END;
                      active = SELECT s
                               FROM active:s
                               ACCUM s.@cid = s.@wcc_id_curr;
              END;
              i = i + 1;

              pivots = SELECT s
                       FROM active:s
                       WHERE s.@cid == s.@vid
                       ACCUM s.@is_forward = TRUE,
                             s.@is_backward = TRUE;

              // mark forward set
              curr = pivots;
              WHILE curr.size()>0 DO
                  curr = SELECT t
                         FROM curr:s -(e_type:e)->:t  // edge
                         WHERE t.@detached == FALSE AND t.@is_forward == FALSE AND s.@cid == t.@cid // not traversed
                         ACCUM t.@is_forward = TRUE;
              END;

              // mark backward set
              curr = pivots;
              WHILE curr.size()>0 DO
                  curr = SELECT t
                         FROM curr:s -(rev_e_type:e)->:t  // reverse edge
                         WHERE t.@detached == FALSE AND t.@is_backward == FALSE AND s.@cid == t.@cid // not traversed
                         ACCUM t.@is_backward = TRUE;
              END;

              active =
                  SELECT s
                  FROM active:s
                  ACCUM IF s.@is_forward == TRUE AND s.@is_backward == TRUE THEN  // scc
                             s.@detached = TRUE,
                             @@s_cid_map += (s.@cid -> s.@vid)
                     ELSE IF s.@is_forward == TRUE THEN  // forward set
                          @@f_cid_map += (s.@cid -> s.@vid)
                      ELSE IF s.@is_backward == TRUE THEN    // backward set
                          @@b_cid_map += (s.@cid -> s.@vid)
                      ELSE
                          @@n_cid_map += (s.@cid -> s.@vid)   // null set
                      END
                      POST-ACCUM IF s.@is_forward == TRUE AND s.@is_backward == TRUE THEN //scc
                              s.@cid = @@s_cid_map.get(s.@cid)
                          END,
                          IF s.@is_forward == TRUE THEN
                              IF s.@is_backward == FALSE THEN   // forward set
                                   s.@cid = @@f_cid_map.get(s.@cid)
                              END
                           ELSE
                              IF s.@is_backward == TRUE THEN    // backward set
                                   s.@cid = @@b_cid_map.get(s.@cid)
                              ELSE                              // null set
                                   s.@cid = @@n_cid_map.get(s.@cid)
                              END
                          END,
                          s.@is_forward = FALSE,
                          s.@is_backward = FALSE
                      HAVING s.@detached == FALSE;

              @@s_cid_map.clear();
              @@f_cid_map.clear();
              @@b_cid_map.clear();
              @@n_cid_map.clear();
          END;

          // result
          v_all = SELECT s
                  FROM v_all:s
                  POST-ACCUM @@cluster_size_map += (s.@cid -> 1);

          FOREACH (cid, csize) IN @@cluster_size_map DO
          @@cluster_dist_map += (csize -> 1);
          END;
          FOREACH (csize, number) IN @@cluster_dist_map DO
          @@cluster_dist_heap += cluster_num(csize, number);
          END;
          PRINT @@cluster_dist_heap;

          IF file_path != "" THEN
              f.println("Vertex_ID","Component_ID");
          END;

          v_all = SELECT s
                  FROM v_all:s
                  POST-ACCUM
                  IF result_attr != "" THEN s.setAttr(result_attr, s.@cid) END,
                  IF file_path != "" THEN f.println(s, s.@cid) END
                  LIMIT output_limit;

          IF print_accum THEN
              PRINT v_all[v_all.@cid];
          END;
      }
      CREATE DISTRIBUTED QUERY louvain_distributed_vf(SET<STRING> v_type, SET<STRING> e_type, STRING wt_attr, INT max_iter = 10, FLOAT tolerence = 0.0001) FOR GRAPH SupplyChain {
          TYPEDEF TUPLE <FLOAT deltaQ, FLOAT weight, VERTEX cc> move;
          MapAccum<VERTEX, SumAccum<INT>> @@communityMap;
          MapAccum<INT, SumAccum<INT>> @@communitySizeCount;
          SetAccum<INT> @@communitySet;
          SumAccum<FLOAT> @ac; #sum of the degrees of all the vertices in community C of the vertex
          ListAccum<VERTEX> @cc; #the community center
          SumAccum<FLOAT> @weight; # total weight incident to this vertex
          SumAccum<FLOAT> @ccWeight; # total weight incident to the cc vertex
          MapAccum<VERTEX,SumAccum<FLOAT>> @A; #A[c]: sum of the edge weights for the edges in community c
          MaxAccum<move> @best_move; # highest dQ, highest -Outdegree, highest cc
          ListAccum<VERTEX> @cm;  #community member list
          SumAccum<FLOAT> @@m; # total edge weight
          SumAccum<INT> @outdegree;   # helper variable for outdegree calculation
          SumAccum<INT> @@ccChange;
          String date;

          file f1 ("/home/tigergraph/results.csv");
          f1.println("VertexType,primaryID,communityID");

          // initialize
          Start = {v_type};
          Start = SELECT s FROM Start:s -(e_type:e)- :t
              ACCUM
                  @@m += /*e.getAttr(wt_attr,"INT")**/0.5,
                  s.@weight += /*e.getAttr(wt_attr,"INT")**/1.0,
                  s.@ccWeight += /*e.getAttr(wt_attr,"INT")**/1.0,
                  s.@outdegree += 1
              POST-ACCUM
                  IF s.@outdegree > 1 THEN s.@cc += s END;
          PRINT Start.size() AS AllVertexCount;
          // special @cc update in the first iteration
          Start = SELECT t FROM Start:s -(e_type:e)- :t
              WHERE s.@outdegree > 1 AND t.@outdegree > 1
              ACCUM
                  t.@best_move += move(/*e.getAttr(wt_attr,"INT")**/1.0 + @@m*t.@weight * (t.@weight - s.@weight), -s.@ccWeight, s.@cc.get(0))
              POST-ACCUM
                  IF getDeltaQ(t.@best_move) > 0 THEN
                      IF -getWeight(t.@best_move) < t.@ccWeight THEN
                          t.@cc.clear(),
                          t.@cc += getCc(t.@best_move),
                          t.@ccWeight = -getWeight(t.@best_move),
                          @@ccChange += 1
                      ELSE
                          IF -getWeight(t.@best_move) == t.@ccWeight AND getvid(t) < getvid(getCc(t.@best_move))  THEN
                              t.@cc.clear(),
                              t.@cc += getCc(t.@best_move),
                              t.@ccWeight = -getWeight(t.@best_move),
                              @@ccChange += 1
                          END
                      END
                  END;
          PRINT @@ccChange AS InitChangeCount;

          // main loop
          WHILE @@ccChange > 0 LIMIT max_iter DO
              // initialize for iteration
              @@ccChange = 0;
              Start = SELECT s FROM Start:s
                  WHERE s.@outdegree > 1
                  POST-ACCUM
                      s.@ac = 0,
                      s.@cm.clear(),
                      s.@A.clear();
              Start = SELECT s FROM Start:s
                  ACCUM
                      FOREACH v IN s.@cc DO
                          CASE WHEN getvid(v) != -1 THEN v.@cm += s END
                      END;
              Start = SELECT s FROM Start:s -(e_type:e)- :t
                  WHERE t.@outdegree > 1
                  ACCUM s.@A += (t.@cc.get(0) -> /*e.getAttr(wt_attr,"INT")**/1.0);
              Start = SELECT s FROM Start:s
                  ACCUM
                      FOREACH v IN s.@cc DO
                          CASE WHEN getvid(v) != -1 THEN v.@ac += s.@weight END
                      END;
              Start = SELECT s FROM Start:s
                  ACCUM
                      FOREACH v IN s.@cm DO
                          CASE WHEN getvid(v) != -1 THEN v.@ac = s.@ac END
                      END;
              // compute @max_dQ
              Start = SELECT s FROM Start:s -(e_type:e)- :t
                      WHERE t.@outdegree > 1
                  ACCUM
                      INT A_s = 0,
                      IF s.@A.containsKey(s) THEN A_s = s.@A.get(s) END,
                      s.@best_move += move(s.@A.get(t.@cc.get(0)) - A_s + 1/@@m*s.@weight*(s.@ac-t.@ac), -t.@ccWeight, t.@cc.get(0))
                  POST-ACCUM
                      IF getDeltaQ(s.@best_move) > 0 THEN
                          IF -getWeight(s.@best_move) < s.@ccWeight THEN   // smallest best_move weight < current weight
                              s.@cc.clear(),
                              s.@cc += getCc(s.@best_move),
                              s.@ccWeight = -getWeight(s.@best_move),
                              @@ccChange += 1
                          ELSE
                              IF -getWeight(s.@best_move) == s.@ccWeight AND getvid(s.@cc.get(0)) < getvid(getCc(s.@best_move))  THEN
                                  s.@cc.clear(),
                                  s.@cc += getCc(s.@best_move),
                                  s.@ccWeight = -getWeight(s.@best_move),
                                  @@ccChange += 1
                              END
                          END
                      END;
              //PRINT @@ccChange AS IterChangeCount;
          END;
          // process node with outdegree = 1
          Start = {v_type};
          Start = SELECT s FROM Start:s -(e_type:e)- :t
              WHERE s.@outdegree == 1 AND t.@outdegree != 1
              ACCUM s.@cc += t.@cc.get(0);
          //PRINT Start.size() AS VertexFollowedToCommunity;
          Start = {v_type};
          Start = SELECT s FROM Start:s -(e_type:e)- :t
              WHERE s.@outdegree == 1 AND t.@outdegree == 1
              ACCUM
                  IF getvid(s) <= getvid(t) THEN
                      s.@cc += s
                  ELSE
                      s.@cc += t
                  END;
          //PRINT Start.size() AS VertexFollowedToVertex;
          // print result satistic
          Start={v_type};
          Start={v_type};
          Start = SELECT s FROM Start:s
              WHERE s.@cc.size() > 0
              POST-ACCUM
                  //@@communityMap += (getvid(s.@cc.get(0)) -> 1);
                  @@communityMap += (s.@cc.get(0) -> 1);
          PRINT @@communityMap.size() AS FinalCommunityCount;

      //  print @@communityMap;


      //     FOREACH (cid, cSize) IN @@communityMap DO
      //         @@communitySizeCount += (cSize -> 1);
      //     END;


      //     PRINT @@communitySizeCount;

        printSet = {models.*, item.*};
        printSet = select s from printSet:s
                   post-accum f1.println(s.type,s,s.@cc);

        //print printSet[printSet.@cc];
      }
      CREATE QUERY closeness(SET<STRING> v_type, SET<STRING> e_type, STRING re_type,INT max_hops=10,
        INT top_k=100, BOOL wf = TRUE, BOOL print_accum = True, STRING result_attr = "",
        STRING file_path = "", BOOL display_edges = FALSE){
        /* Compute Closeness Centrality for each VERTEX.
        Use multi-sourse BFS.
        Link of the paper: http://www.vldb.org/pvldb/vol8/p449-then.pdf
        Parameters:
        v_type: vertex types to traverse                 print_accum: print JSON output
        e_type: edge types to traverse                   result_attr: INT attr to store results to
        max_hops: look only this far from each vertex    file_path: file to write CSV output to
        top_k: report only this many top scores          display_edges: output edges for visualization
        wf: Wasserman and Faust normalization factor for multi-component graphs */
        TYPEDEF TUPLE<VERTEX Vertex_ID, FLOAT score> VertexScore;
        HeapAccum<VertexScore>(top_k, score DESC) @@topScores;
        SumAccum<INT> @@currDist; #current distance
        BitwiseOrAccum @visitNext; #use bitwise instead of setAccum
        SumAccum<INT> @res; #Result, sum of distance
        SumAccum<INT> @size; #get graph size
        SumAccum<FLOAT> @score;
        BitwiseOrAccum @seen;
        BitwiseOrAccum @visit;
        SumAccum<INT> @@count=1;#used to set unique ID
        SumAccum<INT> @id; #store the unique ID
        SetAccum<INT> @@batchSet; #used to set unique ID
        MapAccum<INT,INT> @@map; #used to set unique ID
        SetAccum<EDGE> @@edgeSet;
        INT empty=0;
        FILE f (file_path);
        INT numVert;
        INT batch_number;
      # Compute closeness
        all = {v_type};

        numVert = all.size();
        batch_number = numVert/60;
        IF batch_number==0 THEN batch_number=1; END;

        #Calculate the sum of distance to other vertex for each vertex
        FOREACH i IN RANGE[0, batch_number-1] do
                Start = SELECT s FROM all:s
                        WHERE getvid(s)%batch_number == i
                        POST-ACCUM @@map+=(getvid(s)->0),@@batchSet+=getvid(s);

                FOREACH ver in @@batchSet DO @@map+=(ver->@@count); @@count+=1;END; #set a unique ID for each vertex, ID from 1-63
                Start = SELECT s FROM Start:s POST-ACCUM s.@id=@@map.get(getvid(s));
                Start = Select s FROM Start:s
                        POST-ACCUM s.@seen=1<<s.@id,s.@visit=1<<s.@id; # set initial seen and visit s.@seen1 s.@seen2
                @@batchSet.clear();
                @@map.clear();
                @@count=0;

                WHILE (Start.size() > 0) LIMIT max_hops DO
                      @@currDist+=1;
                      Start = SELECT t FROM Start:s -(re_type:e)-v_type:t
                              WHERE s.@visit&-t.@seen-1>0 and s!=t #use -t.@seen-1 to get the trverse of t.@seen
                              ACCUM
                                    INT c = s.@visit&-t.@seen-1,
                                    IF c>0 THEN
                                        t.@visitNext+=c,
                                        t.@seen+=c
                                    END
                              POST-ACCUM
                                    t.@visit=t.@visitNext,
                                    INT r = t.@visitNext,
                                    WHILE r>0 DO
                                          r=r&(r-1),t.@res+=@@currDist,t.@size+=1 #count how many 1 in the number, same as setAccum,size()
                                    END,
                                    t.@visitNext=0;
                END;
                @@currDist=0;
                Start = SELECT s FROM all:s
                        POST-ACCUM s.@seen=0,s.@visit=0;
        END;

        Start = SELECT s FROM all:s
                  # Calculate Closeness Centrality for each vertex
                WHERE s.@res>0
                POST-ACCUM
                          IF wf THEN s.@score = (s.@size*1.0/(numVert-1))*(s.@size*1.0/s.@res) ELSE s.@score = s.@size*1.0/s.@res*1.0 END,
                  IF result_attr != "" THEN s.setAttr(result_attr, s.@score) END,
                      IF print_accum THEN @@topScores += VertexScore(s, s.@score) END,
                  IF file_path != "" THEN f.println(s, s.@score) END;
            #test

      #Output
          IF file_path != "" THEN
                  f.println("Vertex_ID", "Closeness");
          END;

          IF print_accum THEN
              PRINT @@topScores AS top_scores;
              IF display_edges THEN
                  PRINT Start[Start.@score];
                  Start = SELECT s
                  FROM Start:s -(e_type:e)->:t
                      ACCUM @@edgeSet += e;
                  PRINT @@edgeSet;
              END;
          END;
      }
      CREATE QUERY wcc (SET<STRING> v_type, SET<STRING> e_type, INT output_limit = 100,
       BOOL print_accum = TRUE, STRING result_attr = "", STRING file_path = "") {
      /*
       This query identifies the Connected Components (undirected edges). When finished, each
       vertex is assigned an INT label = its component ID number.
        v_type: vertex types to traverse          print_accum: print JSON output
        e_type: edge types to traverse            result_attr: INT attr to store results to
        file_path: file to write CSV output to    display_edges: output edges for visualization
        output_limit: max #vertices to output (-1 = all)
      ,*/

          MinAccum<INT> @cc_id = 0;       //each vertex's tentative component id
          MapAccum<INT, INT> @@compSizes;
          MapAccum<INT, ListAccum<INT>> @@compGroupBySize;
          FILE f(file_path);

          Start = {v_type};

          # Initialize: Label each vertex with its own internal ID
          S = SELECT x
              FROM Start:x
              POST-ACCUM x.@cc_id = getvid(x)
          ;

          # Propagate smaller internal IDs until no more ID changes can be Done
          WHILE (S.size()>0) DO
                  S = SELECT t
                      FROM S:s -(e_type:e)- v_type:t
                      ACCUM t.@cc_id += s.@cc_id // If s has smaller id than t, copy the id to t
                      HAVING t.@cc_id != t.@cc_id'
                  ;
          END;

          IF file_path != "" THEN
            f.println("Vertex_ID","Component_ID");
          END;

          Start = {v_type};
          Start = SELECT s FROM Start:s
                  POST-ACCUM
                      IF result_attr != "" THEN s.setAttr(result_attr, s.@cc_id) END,
                      IF print_accum THEN @@compSizes += (s.@cc_id -> 1) END,
                      IF file_path != "" THEN f.println(s, s.@cc_id) END;

          IF print_accum THEN
              IF output_limit >= 0 THEN
                  Start = SELECT s FROM Start:s LIMIT output_limit;
              END;
              FOREACH (compId,size) IN @@compSizes DO
                  @@compGroupBySize += (size -> compId);
              END;
          PRINT @@compGroupBySize;
            PRINT @@compSizes as sizes;
            PRINT Start[Start.@cc_id];
          END;
      }
      CREATE QUERY betweenness(SET<STRING> v_type, SET<STRING> e_type, STRING re_type,INT max_hops=10,
        INT top_k=100, BOOL print_accum = True, STRING result_attr = "",
        STRING file_path = "", BOOL display_edges = FALSE){
        /* Compute Closeness Centrality for each VERTEX.
        Use multi-sourse BFS.
        Link of the paper: http://www.vldb.org/pvldb/vol8/p449-then.pdf
        Parameters:
        v_type: vertex types to traverse                 print_accum: print JSON output
        e_type: edge types to traverse                   result_attr: INT attr to store results to
        max_hops: look only this far from each vertex    file_path: file to write CSV output to
        top_k: report only this many top scores          display_edges: output edges for visualization
         ,*/
        TYPEDEF TUPLE<VERTEX Vertex_ID, FLOAT score> VertexScore;
        HeapAccum<VertexScore>(top_k, score DESC) @@topScores;
        SumAccum<INT> @@currDist; #current distance
        BitwiseOrAccum @visitNext; #use bitwise instead of setAccum
        BitwiseOrAccum @seen;
        BitwiseOrAccum @visit;
        SumAccum<INT> @@count=1;#used to set unique ID
        SumAccum<INT> @id; #store the unique ID
        SetAccum<INT> @@batchSet; #used to set unique ID
        MapAccum<INT,INT> @@map; #used to set unique ID
        SetAccum<EDGE> @@edgeSet;
        SumAccum<FLOAT> @delta=0;
        MapAccum<INT,BitwiseOrAccum> @times;
        MapAccum<INT,SumAccum<INT>> @sigma;
        ListAccum<INT> @test;
        INT empty=0;
        FILE f (file_path);
        INT numVert;
        INT batch_number;

      # Compute betweenness
        all = {v_type};
        numVert = all.size();
        batch_number = numVert/60;
        IF batch_number==0 THEN batch_number=1; END;

        #Calculate the sum of distance to other vertex for each vertex
        FOREACH i IN RANGE[0, batch_number-1] do
                Current = SELECT s FROM all:s
                          WHERE getvid(s)%batch_number == i
                          POST-ACCUM @@map+=(getvid(s)->0),@@batchSet+=getvid(s);

                FOREACH ver in @@batchSet DO @@map+=(ver->@@count); @@count+=1;END; #set a unique ID for each vertex, ID from 1-63
                Start = SELECT s FROM Current:s POST-ACCUM s.@id=@@map.get(getvid(s));
                Start = Select s FROM Current:s
                        POST-ACCUM s.@seen=1<<s.@id,
                                   s.@visit=s.@seen,
                                   s.@sigma+=(0->1),
                                   s.@times+=(0->1<<s.@visit); # set initial seen and visit
                @@batchSet.clear();
                @@map.clear();
                @@count=0;

                WHILE (Start.size() > 0) LIMIT max_hops DO

                        @@currDist+=1;
                        Start = SELECT t FROM Start:s -(re_type:e)-v_type:t
                                WHERE s.@visit&-t.@seen-1>0 and s!=t #use -t.@seen-1 to get the trverse of t.@seen
                                ACCUM                               #updatevisitNext
                                      INT c = s.@visit&-t.@seen-1,
                                      IF c>0 THEN
                                          t.@visitNext+=c,
                                          t.@seen+=c
                                      END,
                                      t.@sigma+=(@@currDist->s.@sigma.get(@@currDist-1)) #set sigma based on depth
                                POST-ACCUM
                                      t.@visit=t.@visitNext,
                                      t.@times+=(@@currDist->t.@visit),
                                      t.@visitNext=0;
                END;
                @@currDist+=-1;

                Start = Select s from all:s WHERE s.@sigma.get(@@currDist)!=0;
                WHILE (Start.size()>0) LIMIT max_hops DO
                          @@currDist+=-1;
                          Start = SELECT t FROM Start:s -(re_type:e)-> v_type:t
                      WHERE t.@times.get(@@currDist)&s.@times.get(@@currDist+1)!=0
                              ACCUM
                                       FLOAT currValue=t.@sigma.get(@@currDist)/(s.@sigma.get(@@currDist+1)*(1+s.@delta)),
                                       INT r=t.@times.get(@@currDist)&s.@times.get(@@currDist+1),
                                       INT plus=0,
                                       WHILE r>0 DO
                                              r=r&(r-1),plus=plus+1 #count how many 1 in the number, same as setAccum,size()
                                       END,
                                       FLOAT value = currValue*plus/2.0,
                                       t.@delta+=value;

                  Start = Select s from all:s WHERE s.@sigma.get(@@currDist)!=0;
              END;
              @@currDist=0;
              Start = SELECT s FROM all:s
                      POST-ACCUM s.@seen=0,s.@visit=0,s.@sigma.clear(),s.@times.clear();
        END;

        #PRINT all [all.@delta];
        Start = SELECT s FROM all:s
                  POST-ACCUM
                          IF result_attr != "" THEN s.setAttr(result_attr, s.@delta) END,
                          IF print_accum THEN @@topScores += VertexScore(s, s.@delta) END,
                          IF file_path != "" THEN f.println(s, s.@delta) END;
        #Output
          IF file_path != "" THEN
                  f.println("Vertex_ID", "Betweenness");
          END;

          IF print_accum THEN
              PRINT @@topScores AS top_scores;
              IF display_edges THEN
                  PRINT Start[Start.@delta];
                  Start = SELECT s
                  FROM Start:s -(e_type:e)->:t
                      ACCUM @@edgeSet += e;
                  PRINT @@edgeSet;
              END;
          END;

      }
      CREATE QUERY pagerank (STRING v_type, STRING e_type,
       FLOAT max_change=0.001, INT max_iter=25, FLOAT damping=0.85, INT top_k = 100,
       BOOL print_accum = TRUE, STRING result_attr =  "", STRING file_path = "",
       BOOL display_edges = FALSE) {
      /*
       Compute the pageRank score for each vertex in the GRAPH
       In each iteration, compute a score for each vertex:
           score = (1-damping) + damping*sum(received scores FROM its neighbors).
       The pageRank algorithm stops when either of the following is true:
       a) it reaches max_iter iterations;
       b) the max score change for any vertex compared to the last iteration <= max_change.
       v_type: vertex types to traverse          print_accum: print JSON output
       e_type: edge types to traverse            result_attr: INT attr to store results to
       max_iter; max #iterations                 file_path: file to write CSV output to
       top_k: #top scores to output              display_edges: output edges for visualization
       max_change: max allowed change between iterations to achieve convergence
       damping: importance of traversal vs. random teleport

       This query supports only taking in a single edge for the time being (8/13/2020).
      ,*/
          TYPEDEF TUPLE<VERTEX Vertex_ID, FLOAT score> Vertex_Score;
          HeapAccum<Vertex_Score>(top_k, score DESC) @@topScores;
          MaxAccum<FLOAT> @@max_diff = 9999;    # max score change in an iteration
          SumAccum<FLOAT> @recvd_score = 0; # sum of scores each vertex receives FROM neighbors
          SumAccum<FLOAT> @score = 1;           # initial score for every vertex is 1.
          SetAccum<EDGE> @@edgeSet;             # list of all edges, if display is needed
          FILE f (file_path);

      # PageRank iterations
          Start = {v_type};                     # Start with all vertices of specified type(s)
          WHILE @@max_diff > max_change LIMIT max_iter DO
                  @@max_diff = 0;
                  V = SELECT s
                      FROM Start:s -(e_type:e)-> v_type:t
                      ACCUM t.@recvd_score += s.@score/(s.outdegree(e_type))
                      POST-ACCUM s.@score = (1.0-damping) + damping * s.@recvd_score,
                                 s.@recvd_score = 0,
                                 @@max_diff += abs(s.@score - s.@score');
          END; # END WHILE loop

      # Output
          IF file_path != "" THEN
            f.println("Vertex_ID", "PageRank");
          END;

          V = SELECT s FROM Start:s
              POST-ACCUM
                  IF result_attr != "" THEN s.setAttr(result_attr, s.@score) END,
                  IF file_path != "" THEN f.println(s, s.@score) END,
                  IF print_accum THEN @@topScores += Vertex_Score(s, s.@score) END;

          IF print_accum THEN
              PRINT @@topScores;
              IF display_edges THEN
                  PRINT Start[Start.@score];
                  Start = SELECT s
                          FROM Start:s -(e_type:e)-> v_type:t
                          ACCUM @@edgeSet += e;
                 PRINT @@edgeSet;
              END;
          END;
      }
      CREATE QUERY NetBuild(vertex<BuildSchedule> input) FOR GRAPH SupplyChain {
        // build schedule -> models -> bom -> item
        // bom, get the required amts per part
        // R21MAE57BX102021-07-27T00:00:00Z

        //Typedef Tuple <string item, int itemQty> modelTuple;

        //MapAccum<String, ListAccum<modelTuple>> @@modelItems;
        GroupByAccum<String modelNumber, String itemNumber, String site, SumAccum<int> qtyper1, SumAccum<int> qty> @gb;
        GroupByAccum<String modelNumber, String itemNumber, String site,SumAccum<int> qtyper, SumAccum<int> qtyonhand, SumAccum<int> qtyLeftOver, SumAccum<int> howManyCanWeBuild> @@res;
        //MapAccum<String, int> @onHand;
        SumAccum<int> @onHand;
        SumAccum<String> @site;
        SumAccum<int> @howManyCanWeBuild;
        int orderqty = 0;

        Start = {input};

        getmodels = select t from Start:s-(scheduled_to_build:e)-models:t
                    post-accum orderqty = s.order_qty;

        boms = select t from getmodels:s-(model_bom:e)-BillofMaterial:t;

        items = select t from boms:s-(item_in_bom:e)-item:t
                accum t.@site += s.site_id,
                      t.@gb += (s.model_number, s.item_number, s.site_id -> s.quantity_per, (s.quantity_per * orderqty));

        getcounts = select s from items:s-(in_warehouse_inventory:e)-warehouse:t
                    where t.WarehouseId == s.@site
                    accum if e.qtyonhand < 0 then
                            //s.@onHand += (t.WarehouseId -> 0)
                            s.@onHand = 0
                          else
                            s.@onHand += e.qtyonhand
                            //s.@onHand += (t.WarehouseId -> e.qtyonhand)
                          end
                    post-accum foreach entry in s.@gb do
                                 if entry.qtyper1 == 0 then
                                   @@res += (entry.modelNumber, entry.itemNumber,
                                             entry.site -> entry.qtyper1, s.@onHand, (s.@onHand - entry.qty),0)
                                 else
                                   @@res += (entry.modelNumber, entry.itemNumber,
                                             entry.site -> entry.qtyper1, s.@onHand, (s.@onHand - entry.qty), (s.@onHand / entry.qtyper1))
                                 end
                               end;


        print orderqty;
        //print getcounts[getcounts.@onHand];
        print @@res;
      }
    #+end_src
    #+begin_src bash :tangle solutions/supplychain/03-create-queries.sh
      gsql -g SupplyChain queries.gsql
    #+end_src
  + Do eet
    #+begin_src bash :tangle solutions/supplychain/runall.sh
      ./01-create-graph.sh
      ./02-load-data.sh
      ./03-create-queries.sh
    #+end_src
** TODO Synthea
   :LOGBOOK:
   - State "DONE"       from "TODO"       [2021-11-02 Tue 12:13]
   :END:
   + Resources
     + Data: GitHub
     + Last export:
   +
** DONE Fraud and Anti-Money Laundering Detection
   :LOGBOOK:
   - State "DONE"       from              [2021-12-03 Fri 15:33]
   :END:
   + ref: https://www.tigergraph.com/starterkits/
   + Data set: s3://gjgeksbackup/antifraud-cloud_data.tar.gz
*** Resources
*** Create the graph
    #+begin_src sql :tangle solutions/antifraud/01-create-graph.gsql :mkdirp yes
      USE GLOBAL
      DROP GRAPH AntiFraud

      CREATE GRAPH AntiFraud()

      USE GRAPH AntiFraud
      CREATE SCHEMA_CHANGE JOB schema_change_job_shmedley FOR GRAPH AntiFraud {
            ADD VERTEX Transaction(PRIMARY_ID id STRING, ts UINT, amount FLOAT) WITH STATS="OUTDEGREE_BY_EDGETYPE", PRIMARY_ID_AS_ATTRIBUTE="false";
            ADD VERTEX User(PRIMARY_ID id STRING, signupEpoch UINT, mobile STRING, trust_score FLOAT) WITH STATS="OUTDEGREE_BY_EDGETYPE", PRIMARY_ID_AS_ATTRIBUTE="false";
            ADD VERTEX Device_Token(PRIMARY_ID id STRING, is_banned BOOL, os_name STRING, os_version STRING, model STRING, carrier STRING, is_rooted BOOL, is_emulator BOOL, device_name STRING, trust_score FLOAT) WITH STATS="OUTDEGREE_BY_EDGETYPE", PRIMARY_ID_AS_ATTRIBUTE="false";
            ADD VERTEX Payment_Instrument(PRIMARY_ID id STRING, token_handle STRING, token_type STRING, card_issuing_country_iso2 STRING, card_issuing_bank STRING, card_bin STRING, trust_score FLOAT) WITH STATS="OUTDEGREE_BY_EDGETYPE", PRIMARY_ID_AS_ATTRIBUTE="false";
            ADD DIRECTED EDGE User_Transfer_Transaction(FROM User, TO Transaction) WITH REVERSE_EDGE="User_Transfer_Transaction_Rev";
            ADD DIRECTED EDGE User_Recieve_Transaction(FROM User, TO Transaction) WITH REVERSE_EDGE="User_Recieve_Transaction_Rev";
            ADD UNDIRECTED EDGE User_to_Device(FROM User, TO Device_Token);
            ADD UNDIRECTED EDGE User_to_Payment(FROM User, TO Payment_Instrument);
            ADD DIRECTED EDGE User_Refer_User(FROM User, TO User) WITH REVERSE_EDGE="User_Referred_By_User";
            }
      RUN SCHEMA_CHANGE JOB schema_change_job_shmedley
      DROP JOB schema_change_job_shmedley
    #+end_src

*** Load data
    #+begin_src sql :tangle solutions/antifraud/02-load-data.gsql :mkdirp yes
      USE GRAPH AntiFraud
      CREATE LOADING JOB load_job_antifraud FOR GRAPH AntiFraud {
        DEFINE FILENAME client="m1:/home/tigergraph/mydata/antifraud/data/client.csv";
        DEFINE FILENAME mytransaction="m1:/home/tigergraph/mydata/antifraud/data/transaction.csv";
        DEFINE FILENAME device="m1:/home/tigergraph/mydata/antifraud/data/device.csv";
        DEFINE FILENAME userDevice="m1:/home/tigergraph/mydata/antifraud/data/userDevice.csv";
        DEFINE FILENAME payment="m1:/home/tigergraph/mydata/antifraud/data/payment.csv";
        DEFINE FILENAME referral="m1:/home/tigergraph/mydata/antifraud/data/client_referral.csv";
        DEFINE FILENAME document="m1:/home/tigergraph/mydata/antifraud/data/document.csv";

        LOAD client TO VERTEX User VALUES($0, $1, $2, $3) USING SEPARATOR="\t", HEADER="true", EOL="\n";

        LOAD mytransaction TO VERTEX Transaction VALUES($0, $4, $3) USING SEPARATOR="\t", HEADER="true", EOL="\n";
        LOAD mytransaction TO EDGE User_Recieve_Transaction VALUES($1, $0) USING SEPARATOR="\t", HEADER="true", EOL="\n";
        LOAD mytransaction TO EDGE User_Transfer_Transaction VALUES($2, $0) USING SEPARATOR="\t", HEADER="true", EOL="\n";

        LOAD device TO VERTEX Device_Token VALUES($0, $1, $2, $3, $4, $5, $6, $7, $8, $10) USING SEPARATOR="\t", HEADER="true", EOL="\n";
        LOAD userDevice TO EDGE User_to_Device VALUES($0, $1) USING SEPARATOR="\t", HEADER="true", EOL="\n";

        LOAD payment TO VERTEX Payment_Instrument VALUES($1, $2, $3, _, $5, $7, $10) USING SEPARATOR="\t", HEADER="true", EOL="\n";
        LOAD payment TO EDGE User_to_Payment VALUES($0, $1) USING SEPARATOR="\t", HEADER="true", EOL="\n";

        LOAD referral TO EDGE User_Refer_User VALUES($1, $0) USING SEPARATOR="\t", HEADER="true", EOL="\n";
          }

        RUN LOADING JOB load_job_antifraud
        DROP JOB load_job_antifraud
    #+end_src
*** Install query functions
    #+begin_src c++ :tangle solutions/antifraud/ExprFunctions.hpp
      /******************************************************************************
       ,* Copyright (c) 2015-2016, TigerGraph Inc.
       ,* All rights reserved.
       ,* Project: TigerGraph Query Language
       ,* udf.hpp: a library of user defined functions used in queries.
       ,*
       ,* - This library should only define functions that will be used in
       ,*   TigerGraph Query scripts. Other logics, such as structs and helper
       ,*   functions that will not be directly called in the GQuery scripts,
       ,*   must be put into "ExprUtil.hpp" under the same directory where
       ,*   this file is located.
       ,*
       ,* - Supported type of return value and parameters
       ,*     - int
       ,*     - float
       ,*     - double
       ,*     - bool
       ,*     - string (don't use std::string)
       ,*     - accumulators
       ,*
       ,* - Function names are case sensitive, unique, and can't be conflict with
       ,*   built-in math functions and reserve keywords.
       ,*
       ,* - Please don't remove necessary codes in this file
       ,*
       ,* - A backup of this file can be retrieved at
       ,*     <tigergraph_root_path>/dev_<backup_time>/gdk/gsql/src/QueryUdf/ExprFunctions.hpp
       ,*   after upgrading the system.
       ,*
       ,******************************************************************************/

      #ifndef EXPRFUNCTIONS_HPP_
      #define EXPRFUNCTIONS_HPP_

      #include <stdlib.h>
      #include <stdio.h>
      #include <string>
      #include <gle/engine/cpplib/headers.hpp>

      /**     XXX Warning!! Put self-defined struct in ExprUtil.hpp **
       ,*  No user defined struct, helper functions (that will not be directly called
       ,*  in the GQuery scripts) etc. are allowed in this file. This file only
       ,*  contains user-defined expression function's signature and body.
       ,*  Please put user defined structs, helper functions etc. in ExprUtil.hpp
       ,*/
      #include "ExprUtil.hpp"

      namespace UDIMPL {
        typedef std::string string; //XXX DON'T REMOVE

        /****** BIULT-IN FUNCTIONS **************/
        /****** XXX DON'T REMOVE ****************/
        inline int str_to_int (string str) {
          return atoi(str.c_str());
        }

        inline int float_to_int (float val) {
          return (int) val;
        }

        inline string to_string (double val) {
          char result[200];
          sprintf(result, "%g", val);
          return string(result);
        }

        inline void getVertexesFromEdge(SetAccum<EDGE>& edgeSet, SetAccum<VERTEX>& res) {
          for (auto it = edgeSet.data_.begin(); it != edgeSet.data_.end(); ++it) {
            res += it->srcVid;
            res += it->tgtVid;
          }
        }

        template<typename EdgeTuple>
          inline bool PathContainsV(ListAccum<EdgeTuple>& pathAccum, VERTEX v) {
            std::vector<EdgeTuple>& path = pathAccum.data_;
            for (uint32_t i = 0; i < path.size(); ++i) {
              if (v == path[i].v) {
                return true;
              }
            }
            return false;
          }

      }


      /****************************************/

      #endif /* EXPRFUNCTIONS_HPP_ */
    #+end_src
*** Add queries
    #+begin_src sql :tangle solutions/antifraud/03-queries.gsql :mkdirp yes
      USE GRAPH AntiFraud
      CREATE OR REPLACE QUERY circleDetection (vertex<User> srcId)  FOR GRAPH AntiFraud {
      /*
        This is an anti-money laundering query. It detects money flow circle from a starting user.

        Start from a user, find all the transaction paths originated from the input user
        and eventually come back to the user. The path length is limited from 3 to 6.

        Sample input
        User: any integer between 1 and 500.
      ,*/
        Typedef tuple<EDGE e, VERTEX v, double amount, int ts> EdgeTuple;
        MinAccum<int> @minLeftDist = GSQL_INT_MAX;
        MinAccum<int> @minRightDist = GSQL_INT_MAX;
        MinAccum<int> @@minSrcSendTime = GSQL_INT_MAX;
        MaxAccum<int> @@maxSrcReceiveTime = 0;

        OrAccum @isValid = false;//flag used to record valid vertices in the subgraph

        int stepLowLimit = 3;
        int stepHighLimit = 6;

        int halfStep;
        int step;
        //The following are used for aggregation on src
        SumAccum<int> @validTransNum = 0;
        SumAccum<int> @validTransSum = 0;
        MaxAccum<int> @maxRank = 0;
        ListAccum<ListAccum<EdgeTuple>> @edgeTupleList;
        ListAccum<ListAccum<EdgeTuple>> @newEdgeTupleList;
        ListAccum<ListAccum<EdgeTuple>> @@circleEdgeTuples;
        OrAccum @receiveNewPath = false;

        //The following is used for printing edges and vertices
        SetAccum<vertex> @@vSet;
        ListAccum<ListAccum<Edge>> @@circlePaths;

        //starting from input User vertex
        Seed = {srcId};

        //oneStep to find out the src's minSendTime and maxReceiveTime, initialize the distance info for srcId
        Seed = SELECT src
            FROM Seed:src - ((User_Transfer_Transaction|User_Recieve_Transaction):e) -> Transaction:tgt
            ACCUM
              CASE WHEN e.type == "User_Transfer_Transaction"
                   THEN @@minSrcSendTime += tgt.ts
              ELSE
                @@maxSrcReceiveTime += tgt.ts
              END
            Post-ACCUM
              src.@minLeftDist = 0,
              src.@minRightDist = 0,
              src.@isValid = true
            //make sure that it has a loop, if @@maxSrcRecievTime < @@minSrcSendTime, then there is no loop
            //Because, if @@maxSrcRecievTime < @@minSrcSendTime, all the valid money it receives are before it sends out money
            HAVING @@maxSrcReceiveTime >= @@minSrcSendTime
            ;

        //PRINT epoch_to_datetime(@@maxSrcReceiveTime), epoch_to_datetime(@@minSrcSendTime), startTime, endTime;
        #Now start the bidirectional search of loops for srcId
        # 1) First bidirecitonal search for the potential subgraph for all loops of srcId
        # 2) Then one directional search to valid each path inside the subgraph using path filters, i.e. time increase along the path

        //set X as Seed
        X (_) = Seed;//X is used to do positive direction traversal
        Y (_) = Seed;//Y is used to do negative direction traversal

        # In order to do bidirectional search, we separate search into two steps,
        # i) search for half of totoal steps, only touch unmark vertices,
        #  i.e. positive directional search only touch positive unmarked vertices,
        #       negative search only touch negative unmarked vertices
        # ii) After the first half search, the following search only happens for marked vertices,
        #  i.e. positive directional search only touch negative marked and positive unmarked vertices
        #       negative search only touch negative positive marked and negative unmarked vertices
        # if one of touched vertex fulfil the condition that positive distance + negative distance < stepHighLimit, it is a valid vertex
        //First search for half of total steps
        halfStep = (stepHighLimit + 1)/2;
        step = 0;
        WHILE step <= halfStep AND X.size() + Y.size() > 0
        DO
          IF X.size() > 0
          THEN
            //from User to Transaction
            X = SELECT tgt
              FROM X:src - (User_Transfer_Transaction:e) -> Transaction:tgt
              WHERE
                //tgt ts must be bigger than minSrcSendTime
                //so that all paths has increasing time
                tgt.ts >= @@minSrcSendTime
                AND src.@minLeftDist < GSQL_INT_MAX
                AND tgt.@minLeftDist == GSQL_INT_MAX
              ACCUM
                tgt.@minLeftDist += src.@minLeftDist + 1
              POST-ACCUM
                CASE WHEN tgt.@minLeftDist < GSQL_INT_MAX and tgt.@minRightDist < GSQL_INT_MAX
                       AND tgt.@minLeftDist + tgt.@minRightDist <= 2 * stepHighLimit
                     THEN
                       tgt.@isValid = true
                END
              ;
             //from Transaction to User
            X = SELECT tgt
              FROM X:src - (User_Recieve_Transaction_Rev:e) -> User:tgt
              WHERE src.@minLeftDist < GSQL_INT_MAX
                //only when tgt is not left visited, update the distance info
                AND tgt.@minLeftDist == GSQL_INT_MAX
              ACCUM
                tgt.@minLeftDist += src.@minLeftDist + 1
              POST-ACCUM
                CASE WHEN tgt.@minLeftDist < GSQL_INT_MAX and tgt.@minRightDist < GSQL_INT_MAX
                          AND tgt.@minLeftDist + tgt.@minRightDist <= 2 * stepHighLimit
                     THEN
                       tgt.@isValid = true
                END
              HAVING tgt != srcId
            ;
          END;

          IF Y.size() > 0
          THEN
            Y = SELECT tgt
              FROM Y:src - (User_Recieve_Transaction:e) -> Transaction:tgt
              WHERE
                tgt.ts <= @@maxSrcReceiveTime
                AND src.@minRightDist < GSQL_INT_MAX
                AND tgt.@minRightDist == GSQL_INT_MAX
              ACCUM
                tgt.@minRightDist += src.@minRightDist + 1
              POST-ACCUM
                CASE WHEN tgt.@minLeftDist < GSQL_INT_MAX
                       AND tgt.@minRightDist < GSQL_INT_MAX
                       AND tgt.@minLeftDist + tgt.@minRightDist <= 2 * stepHighLimit
                     THEN
                       tgt.@isValid = true
                END
              ;
            //from Transaction to User
            Y = SELECT tgt
               FROM Y:src - (User_Transfer_Transaction_Rev:e) -> User:tgt
               WHERE src.@minRightDist < GSQL_INT_MAX
                 //only when tgt is not left visited, update the distance info
                 AND tgt.@minRightDist == GSQL_INT_MAX
               ACCUM
                 tgt.@minRightDist += src.@minRightDist + 1
               POST-ACCUM
                 CASE WHEN tgt.@minLeftDist < GSQL_INT_MAX and tgt.@minRightDist < GSQL_INT_MAX
                           AND tgt.@minLeftDist + tgt.@minRightDist <= 2 * stepHighLimit
                      THEN
                        tgt.@isValid = true
                 END
               HAVING tgt != srcId
            ;
          END;
          step = step + 1;
        END;
        # start the last half of search, only touch marked vertices
        WHILE step <= stepHighLimit AND X.size() + Y.size() > 0
        DO
          IF X.size() > 0
          THEN
            //from User to Transaction
            X = SELECT tgt
              FROM X:src - (User_Transfer_Transaction:e) -> Transaction:tgt
              WHERE tgt.@minRightDist < GSQL_INT_MAX//tgt must be touched in the above the negative search
                AND tgt.ts >= @@minSrcSendTime
                AND src.@minLeftDist < GSQL_INT_MAX
                AND tgt.@minLeftDist == GSQL_INT_MAX
              ACCUM
                tgt.@minLeftDist += src.@minLeftDist + 1
              POST-ACCUM
                CASE WHEN tgt.@minLeftDist < GSQL_INT_MAX and tgt.@minRightDist < GSQL_INT_MAX
                       AND tgt.@minLeftDist + tgt.@minRightDist <= 2 * stepHighLimit
                     THEN
                       tgt.@isValid = true
                END
              ;
             //from Transaction to User
            X = SELECT tgt
              FROM X:src - (User_Recieve_Transaction_Rev:e) -> User:tgt
              WHERE tgt.@minRightDist < GSQL_INT_MAX//tgt must be touched in the above the negative search
                AND src.@minLeftDist < GSQL_INT_MAX
                //only when tgt is not left visited, update the distance info
                AND tgt.@minLeftDist == GSQL_INT_MAX
              ACCUM
                tgt.@minLeftDist += src.@minLeftDist + 1
              POST-ACCUM
                CASE WHEN tgt.@minLeftDist < GSQL_INT_MAX and tgt.@minRightDist < GSQL_INT_MAX
                          AND tgt.@minLeftDist + tgt.@minRightDist <= 2 * stepHighLimit
                     THEN
                       tgt.@isValid = true
                END
              HAVING tgt != srcId
            ;
          END;

          IF Y.size() > 0
          THEN
            Y = SELECT tgt
              FROM Y:src - (User_Recieve_Transaction:e) -> Transaction:tgt
              WHERE tgt.@minLeftDist < GSQL_INT_MAX//tgt must be touched in the above positive search
                AND tgt.ts <= @@maxSrcReceiveTime
                AND src.@minRightDist < GSQL_INT_MAX
                AND tgt.@minRightDist == GSQL_INT_MAX
              ACCUM
                tgt.@minRightDist += src.@minRightDist + 1
              POST-ACCUM
                CASE WHEN tgt.@minLeftDist < GSQL_INT_MAX
                       AND tgt.@minRightDist < GSQL_INT_MAX
                       AND tgt.@minLeftDist + tgt.@minRightDist <= 2 * stepHighLimit
                     THEN
                       tgt.@isValid = true
                END
              ;
            //from Transaction to User
            Y = SELECT tgt
              FROM Y:src - (User_Transfer_Transaction_Rev:e) -> User:tgt
              WHERE tgt.@minLeftDist < GSQL_INT_MAX//tgt must be touched in the above positive search
                AND src.@minRightDist < GSQL_INT_MAX
                //only when tgt is not left visited, update the distance info
                AND tgt.@minRightDist == GSQL_INT_MAX
              ACCUM
                tgt.@minRightDist += src.@minRightDist + 1
              POST-ACCUM
                CASE WHEN tgt.@minLeftDist < GSQL_INT_MAX and tgt.@minRightDist < GSQL_INT_MAX
                          AND tgt.@minLeftDist + tgt.@minRightDist <= 2 * stepHighLimit
                     THEN
                       tgt.@isValid = true
                END
              HAVING tgt != srcId
              ;
          END;
          step = step + 1;
        END;

        #start valid path traversal and circle detection
        step = 0;
        //reset X as Seed
        X = Seed;
        WHILE step <= stepHighLimit
        DO
          //from User to Transaction
          X = SELECT tgt
            FROM X:src - (User_Transfer_Transaction:e) -> Transaction:tgt
            WHERE tgt.@isValid == true
            ACCUM
              int ts = tgt.ts,
              CASE
                //if X is Seed, then only send edge over
                WHEN src.@edgeTupleList.size() == 0
                  THEN tgt.@newEdgeTupleList += [EdgeTuple(e, src, tgt.amount, ts)]
                ELSE
                  FOREACH path in src.@edgeTupleList
                  DO
                    tgt.@newEdgeTupleList += path + [EdgeTuple(e, src, tgt.amount, ts)]
                  END
              END,
              //reset receiveNewPath as false
              tgt.@receiveNewPath = false
            POST-ACCUM
              CASE
                WHEN tgt.@newEdgeTupleList.size() > 0
                THEN
                  tgt.@edgeTupleList = tgt.@newEdgeTupleList,
                  tgt.@receiveNewPath = true,
                  tgt.@newEdgeTupleList.clear()
              END
            HAVING tgt.@receiveNewPath == true
          ;

          //from Transaction to User
          X = SELECT tgt
            FROM X:src - (User_Recieve_Transaction_Rev:e) -> User:tgt
            WHERE tgt.@isValid == true
            ACCUM
              FOREACH path in src.@edgeTupleList
              DO
                CASE WHEN tgt == srcId OR (NOT PathContainsV(path, tgt))
                     THEN
                       tgt.@newEdgeTupleList += path + [EdgeTuple(e, src, src.amount, src.ts)]
                END
              END,
              //reset receiveNewPath as false
              tgt.@receiveNewPath = false
            POST-ACCUM
              CASE
                WHEN tgt.@newEdgeTupleList.size() > 0
                THEN
                  CASE
                    //if it backs to start point, there is a valid circle
                    WHEN tgt == srcId
                      THEN
                        //step + 1 gives the current updated step
                        //it is the number of User -> User steps for current paths (there maybe multiple paths but all of them should have the same length)
                        CASE WHEN step + 1 >= stepLowLimit
                             THEN @@circleEdgeTuples += tgt.@newEdgeTupleList
                        END
                    //else, overwrite the old @edgeTupleList, since the old one is already used
                    ELSE tgt.@edgeTupleList = tgt.@newEdgeTupleList
                  END,
                  tgt.@receiveNewPath = true,
                  tgt.@newEdgeTupleList.clear()
              END
            HAVING tgt.@receiveNewPath == true and tgt != srcId
          ;

          step = step + 1;
        END;
        //printJSON only if it is directly called or else return @@circleEdgeTuples directly
        //use the drainRatio to filter out invalid paths
        //store all valid vertices into @@vSet and all paths into @@circlePaths
        PRINT @@circleEdgeTuples;
      }
       CREATE OR REPLACE QUERY SameRecieverSender(vertex<Transaction> transaction) FOR GRAPH AntiFraud {
      /*
       This query is used to find out whether a user conduct fradulent transaction for themselves
       via fake accounts.

       Given an input transaction, return true when its reciever and sender are connected via
       Device_Token and Payment_Instrument within 4 steps.

        Sample input
        transaction: any integer between 1 and 1000.
      ,*/
        OrAccum<bool> @fromReciever, @fromSender;
        OrAccum<bool> @@isSame;

        SetAccum<edge> @@edgeSet;

        Start (ANY) = {transaction};

        // get the sender and reciever
        Start = SELECT t FROM Start:s-((User_Recieve_Transaction_Rev|User_Transfer_Transaction_Rev):e)-:t
                ACCUM
                  // mark the sender and reciver according to the edge type
                  case when e.type == "User_Recieve_Transaction_Rev" then
                    t.@fromReciever += true
                  else
                    t.@fromSender += true
                  end
                  ,@@edgeSet += e
        ;

        // traverse for 4 steps, or the paths of sender and reciever meets each other
        WHILE Start.size() > 0 AND @@isSame == false LIMIT 4 DO
          Start = SELECT t FROM Start:s-((User_to_Device|User_to_Payment):e)-:t
                  // do not traverse the vertexes that were visited
                  WHERE t.@fromReciever == false AND t.@fromSender == false
                  ACCUM t.@fromReciever += s.@fromReciever,
                        t.@fromSender += s.@fromSender
                        ,@@edgeSet += e
                  POST-ACCUM
                    // when two paths meets in the middle
                    CASE WHEN t.@fromReciever == true AND t.@fromSender THEN
                      @@isSame += true
                    END
          ;
        END;

        // output the result
        PRINT @@isSame;
        PRINT @@edgeSet;
      }
      CREATE OR REPLACE QUERY MultiTransaction (VERTEx<Transaction> transaction) FOR GRAPH AntiFraud{
      /*
       This query is motivated by detecting money laundering activities between two groups. Given
       a transaction, it finds the network of users related to the sender, and finds the network
       of users related to the receiver. Then, it finds all transactions among the two networks.

       Intuitively this query can help data analysts to visualize the money laundering activities,
       since it can visualize the transactions between the sender and receiver groups, and the
       transaction patterns within each network.

        1) Start from an input transaction, find its sender and reciever
        2) Start from the sender, via Device_Token and Payment_Instrument edges find users within 4 steps.
        3) Start from the reciever, via Device_Token and Payment_Instrument edges find users within 4 steps.
        4) Record transactions cross the sender and receiver groups.

        Sample input
        transaction: any integer between 1 and 500.
      ,*/

        //declare flags to indicate a user is a sender or a receiver
        OrAccum<bool> @fromReciever, @fromSender;

        //declare set to store sender/receiver in sender/receiver group.
        SetAccum<VERTEX> @@recieverSet, @@senderSet;
        SetAccum<EDGE> @@edgeSet;

        //assign the input transaction to the "Start" variable, which is a SET.
        Start (ANY) = {transaction};

        // find the sender and reciever of the input transaction. Mark them.
        // Now, Start becomes {sender, receiver} set.
        Start = SELECT t FROM Start:s-((User_Recieve_Transaction_Rev|User_Transfer_Transaction_Rev):e)-:t
                ACCUM
                  // mark different groups according to edge type
                  case when e.type == "User_Recieve_Transaction_Rev" then
                    t.@fromReciever += true,
                    @@recieverSet += t
                  else
                    t.@fromSender += true,
                    @@senderSet += t
                  end,
                    @@edgeSet += e;

        //via the User_to_Device, User_to_Payment edge types, traverse 4 steps and
        //put sender reacheable users to the sender set, and reciever reachable
        //users to the receiver set
        WHILE Start.size() > 0 LIMIT 4 DO
          Start = SELECT t FROM Start:s-((User_to_Device|User_to_Payment):e)-:t
                  WHERE t.@fromReciever == false AND t.@fromSender == false
                  ACCUM
                    t.@fromReciever += s.@fromReciever,
                    t.@fromSender += s.@fromSender,
                    @@edgeSet += e
                  POST-ACCUM
                    CASE WHEN t.type == "User" AND t.@fromSender == true THEN
                        @@senderSet += t
                    WHEN t.@fromReciever == true then
                        @@recieverSet += t
                    END
                  HAVING t.@fromReciever OR t.@fromSender;
        END;

        // from the reciever set mark the transactions 1-step related to its group member
        Start = {@@recieverSet};
        Start = SELECT t FROM Start:s-((User_Recieve_Transaction|User_Transfer_Transaction):e)-:t
                WHERE t != transaction
                ACCUM
                   t.@fromReciever += s.@fromReciever,
                   @@edgeSet += e;

        // from the sender set, find transactions 1-step related to its group member.
        // Record those transactions 1-step related to both a sender member and a receiver memeber.
        Start = {@@senderSet};
        Start = SELECT t FROM Start:s-((User_Recieve_Transaction|User_Transfer_Transaction):e)-:t
                WHERE t != transaction
                ACCUM
                   t.@fromSender += s.@fromSender,
                   @@edgeSet += e
                HAVING t.@fromReciever AND t.@fromSender;

        //print cross sender and receiver group transactions.
        print Start;
        //print within sender and receiver subgraph
        print @@edgeSet;
      }
      CREATE OR REPLACE QUERY fraudConnectivity (VERTEX<User> inputUser, FLOAT trustScore) FOR GRAPH AntiFraud {
      /*
        This query finds all connect users/payment cards/device that has low credit score.

        Starting with a user X find all other users connected to
        X through device token, payment instrument connected via transactions in 3 steps

        Sample input
        User: any integer between 1 and 500
        trustScore: any float number (e.g. 0.1)
      ,*/

        OrAccum<bool> @visited;
        SumAccum<int> @@result;
        SetAccum<edge> @@visResult;

        Start (_) = {inputUser};

        // keep traverse for 3 steps
        WHILE Start.size()>0 limit 3 DO
          Start = SELECT t
               FROM Start:s-(:e)-:t
               // sample clause for better visualization result
               SAMPLE 15 EDGE WHEN s.outdegree() >= 20
               WHERE t.@visited == false AND t != inputUser
               ACCUM
                 @@visResult += e
               POST-ACCUM
                 CASE WHEN t.trust_score < trustScore THEN
                   @@result += 1
                 END,
                 t.@visited += true

          ;
        END;

        print @@result;
        print @@visResult;
      }
      CREATE OR REPLACE QUERY InvitedUserBehavior (VERTEX<User> inputUser) FOR GRAPH AntiFraud {
      /*
       This query is motivated to detect those fradulent users who conduct activities to earn
       referral bonus. How do we do that?

       Given an input user, this query traverses the graph, finds out how many two-hop users that
       are indirectely invited by the input user. That is, the users invited by the input user's
       invitees. It also calculates the transferred total money from the one-hop invitees.
       Finally, the traversed subgraph is returned. Intuitively, if it's a fradulent user, we
       can tell from the money transferred from their direct invitees; and their indirect invitees
       should be small or zero.

      Sample input
        inputUser: 5354357 | 30746939 | 23189347
      ,*/
        //declare some variables to store aggregates.
        SumAccum<int> @@invitedPersonNum;
        SumAccum<float> @@totalAmountSent;
        SetAccum<edge> @@visRes;

        //assign the input user to the "start" variable, which is a SET.
        start = {inputUser};

        //one-step traversal. From the start set, via the User_Refer_User edge,
        //find all the invitees of the input user; store them into the "users" variable.
        //Put all touched edges into a variable for visualization purpose.
        users = SELECT t
                FROM start:s-(User_Refer_User:e)-:t
            ACCUM @@visRes += e;

        //Aggregate the amounts of all transactions conducted by the one-hop invitees into
        //variable @@totalAmountSent. Also, store the traversed edges into variable  @@visRes.
        trans = SELECT t
                FROM users:s-((User_Transfer_Transaction):e)-:t
                ACCUM
                  @@totalAmountSent += t.amount,
                  @@visRes += e;

        //Second-hop traversal. Find users invited by the one-hop invitees.
        //store their count in @@invitedPersonNum. And record the traversed edges into @@visRes.
        users = SELECT t
                FROM users:s-(User_Refer_User:e)-:t
                WHERE t != inputUser
                ACCUM @@visRes += e
                  POST-ACCUM @@invitedPersonNum += 1;
        //return 2-hop invitees count, total transfered money by 1-hop and 2-hop invitees,
        //and the subgraph.
        PRINT @@invitedPersonNum, @@totalAmountSent, @@visRes;
      }
      CREATE OR REPLACE QUERY TransferredAmount (vertex<User> sender, dateTime startDate=to_dateTime("2000/12/31"), dateTime endDate=to_dateTime("2020/12/31")) for GRAPH AntiFraud{
      /**
        This query answer the question that given a user, find out how much money has been transferred out from
        her connected users within a date range.

        1) Start from an user, find all other users connected via Device_Tokent or Payment_Instrument within 4 steps.
        2) Then start from all the connected users, find transferred transactions between input start date and end date.
        3) Calculate total transfered money amount of the transcations collected in step 2)

        Sample input
        User : any random integer between 1 and 500
        endDate : 2000-12-31 00:00:00
        startDate : 2020-12-31 00:00:00
      ,*/
        SumAccum<float> @@transAmount;
        OrAccum<bool> @visited;
        // the iteration number
        int iterNum = 0;
        SetAccum<edge> @@edgeSet;

        Start (ANY) = {sender};

        // from the input user, go 4 steps with a while loop to find her connected users.
        WHILE (Start.size() > 0) limit 4 DO
          Start = select t from Start:s-((User_to_Device|User_to_Payment):e)-:t
                  where t.@visited == false AND (t.type != "User" OR t != sender)
                  ACCUM
                    @@edgeSet += e
                  POST-ACCUM
                    t.@visited += true
          ;

          // collect the transferred money number for the users found in 2nd and 4th iteration
          case when iterNum%2 == 1 then
            tmp = select s from Start:s-(User_Transfer_Transaction:e)-:t
                  where epoch_to_datetime(t.ts) < endDate AND epoch_to_datetime(t.ts) > startDate
                  accum @@transAmount += t.amount,
                        @@edgeSet += e;
          end;
          iterNum = iterNum + 1;
        END;

        print @@transAmount;
        print @@edgeSet;
      }
      CREATE OR REPLACE QUERY RepeatedUser (vertex<User> reciever) for GRAPH AntiFraud {
      /**
       Given a money receiver, this query is to disover whether there exists relationships among
       those people who have sent money to this receiver.

        1) Start from a reciever find all her receiving money transactions.
        2) Find all the senders from the transactions collected in step 1)
        3) Start from the senders in step 2), go as far as 8 steps from each sender,
           find all the senders that are connected to other senders by a path made of
           Device_Token, Payment_Instrument, and Users.
        4) Output all the transactions started by the senders found in step 3) and recieved by the input user.

        Sample input
        reciever: Recommend to use 1223 as input. Or, try integer between 1 and 500.
      ,*/

        SumAccum<int> @msgRcv;
        OrAccum<bool> @isS, @isRepeated;
        MaxAccum<vertex> @max;
        MinAccum<vertex> @min;
        SetAccum<vertex> @@linkedJoint;

        SetAccum<edge> @@edgeSet;

        Start (ANY) = {reciever};

        // get all transactions the receiver get money from.
        transactions = select t from Start:s-(User_Recieve_Transaction:e)-:t
                ACCUM @@edgeSet += e
                post-accum t.@isS += true;

        // get all senders related to the above transactions.
        Start = select t from transactions:s-(User_Transfer_Transaction_Rev:e)-:t
                ACCUM @@edgeSet += e
                post-accum t.@msgRcv += 1,
                           t.@isS += true,
                           t.@max = t,
                           t.@min = t;

        // Traverse 8 step from the senders. min/max is used to find joint node
        WHILE (Start.size() > 0) limit 8 DO
          Start = select t from Start:s-((User_to_Device|User_to_Payment):e)-:t
                  WHERE t.@msgRcv == 0
                  ACCUM
                    t.@msgRcv += 1,
                    t.@min += s.@min,
                    t.@max += s.@max
                  POST-ACCUM
                    // when received message from different source
                    CASE WHEN t.@msgRcv > 1 AND t.@min != t.@max THEN
                      @@linkedJoint += t
                    END
          ;
        END;

        Start = {@@linkedJoint};

        // trace back to the source senders from the vertexes that joint multiple paths
        WHILE (Start.size() > 0) DO
          Start = select t from Start:s-((User_to_Device|User_to_Payment):e)-:t
                  WHERE t.@msgRcv != 0
                  ACCUM @@edgeSet += e
                  POST-ACCUM
                    s.@msgRcv = 0,
                    CASE WHEN t.@isS THEN
                      t.@isRepeated += true
                    END;
        END;
        // get the transactions to output
        transactions = select s from transactions:s-(User_Transfer_Transaction_Rev:e)-:t
                where t.@isRepeated == true
                accum @@edgeSet += e
        ;

        print transactions [transactions.amount];
        print @@edgeSet;
      }
      set exit_on_error = "true"
    #+end_src
** TODO Experimental graph: Fraud Detection in Medicare Claims!
   + Initial article: [[https://towardsdatascience.com/fraud-detection-with-graph-analytics-2678e817b69e][Fraud Detection with Graph Analytics | by Lina Faik | Towards Data Science]]
     + [[https://kundusoumya98.medium.com/healthcare-provider-fraud-detection-and-analysis-using-machine-learning-632f7a380c79][Healthcare Provider Fraud Detection And Analysis using Machine learning:- | b...]]
     + [[https://rohansoni-jssaten2019.medium.com/healthcare-provider-fraud-detection-and-analysis-machine-learning-6af6366caff2][Healthcare Provider Fraud Detection And Analysis  Machine learning | by Roha...]]
   + Kaggle data set page: [[https://www.kaggle.com/rohitrox/healthcare-provider-fraud-detection-analysis/version/1?select=Train_Outpatientdata-1542865627584.csv][HEALTHCARE PROVIDER FRAUD DETECTION ANALYSIS | Kaggle]]
     #+begin_quote
     Provider Fraud is one of the biggest problems facing Medicare. According to the
     government, the total Medicare spending increased exponentially due to frauds in
     Medicare claims. Healthcare fraud is an organized crime which involves peers of
     providers, physicians, beneficiaries acting together to make fraud claims.
     #+end_quote
** Health Care Suppliers from starter kits
*** Resources
*** Create graph
    #+begin_src sql :tangle solutions/healthcarereferral/01-create-schema.gsql :mkdirp yes
      USE GLOBAL
      DROP GRAPH HealthCareReferral
      CREATE GRAPH HealthCareReferral()
      USE GRAPH HealthCareReferral
      DROP JOB healthcarereferral_schema
      CREATE SCHEMA_CHANGE JOB healthcarereferral_schema FOR GRAPH HealthCareReferral {
        ADD VERTEX Specialty(PRIMARY_ID id STRING) WITH STATS="OUTDEGREE_BY_EDGETYPE", PRIMARY_ID_AS_ATTRIBUTE="false";
        ADD VERTEX SubSpecialty(PRIMARY_ID id STRING) WITH STATS="OUTDEGREE_BY_EDGETYPE", PRIMARY_ID_AS_ATTRIBUTE="false";
        ADD VERTEX Prescriber(PRIMARY_ID Prescriber_id STRING, pageRank FLOAT, communityId INT) WITH STATS="OUTDEGREE_BY_EDGETYPE", PRIMARY_ID_AS_ATTRIBUTE="false";
        ADD VERTEX Claim(PRIMARY_ID Claim_id STRING, rx_fill_date DATETIME, ICD10Code STRING, ICD10CodeDescription STRING, CodeGroupTitle STRING) WITH STATS="OUTDEGREE_BY_EDGETYPE", PRIMARY_ID_AS_ATTRIBUTE="false";
        ADD VERTEX Patient(PRIMARY_ID Patient_id STRING) WITH STATS="OUTDEGREE_BY_EDGETYPE", PRIMARY_ID_AS_ATTRIBUTE="false";
        ADD DIRECTED EDGE submitted_by(FROM Claim, TO Prescriber) WITH REVERSE_EDGE="reverse_submitted_by";
        ADD DIRECTED EDGE associated(FROM Claim, TO Patient) WITH REVERSE_EDGE="reverse_associated";
        ADD DIRECTED EDGE specialty_subspecialty(FROM Specialty, TO SubSpecialty) WITH REVERSE_EDGE="reverse_specialty_subspecialty";
        ADD DIRECTED EDGE subspecialty_prescriber(FROM SubSpecialty, TO Prescriber) WITH REVERSE_EDGE="reverse_subspecialty_prescriber";
        ADD DIRECTED EDGE referral(FROM Prescriber, TO Prescriber, num_patient INT DEFAULT "0") WITH REVERSE_EDGE="reverse_referral";
      }
      RUN SCHEMA_CHANGE JOB healthcarereferral_schema
    #+end_src
*** Load data
    #+begin_src sql :tangle solutions/healthcarereferral/02-load-data.gsql :mkdirp yes
      USE GRAPH HealthCareReferral
      DROP JOB load_healthcarereferral
      CREATE LOADING JOB load_healthcarereferral FOR GRAPH HealthCareReferral {
        DEFINE FILENAME claim = "m1:/home/tigergraph/mydata/healthcarereferral/data/claim.csv";
        DEFINE FILENAME claim_associated_with_patient = "m1:/home/tigergraph/mydata/healthcarereferral/data/claim associated with patient.csv";
        DEFINE FILENAME claim_submitted_by_prescriber = "m1:/home/tigergraph/mydata/healthcarereferral/data/claim submitted_by prescriber.csv";
        DEFINE FILENAME specialties_subspecialties = "m1:/home/tigergraph/mydata/healthcarereferral/data/specialties-subspecialties.csv";
        DEFINE FILENAME subspecialties_prescriber = "m1:/home/tigergraph/mydata/healthcarereferral/data/subspecialties-prescriber.csv";


        LOAD claim to vertex Claim values($0, $1, $4, $6, $2) USING SEPARATOR=",", HEADER="true", EOL="\n";
        LOAD claim_associated_with_patient TO EDGE associated VALUES($0, $1) USING SEPARATOR=",", HEADER="true", EOL="\n";


        LOAD claim_submitted_by_prescriber TO EDGE submitted_by VALUES($0, $1) USING SEPARATOR=",", HEADER="true", EOL="\n";


        LOAD subspecialties_prescriber TO EDGE subspecialty_prescriber VALUES($1, $0) USING SEPARATOR=",", HEADER="true", EOL="\n";
        LOAD subspecialties_prescriber TO EDGE associated VALUES($0, $1) USING SEPARATOR=",", HEADER="true", EOL="\n";

        LOAD specialties_subspecialties TO EDGE specialty_subspecialty VALUES($0, $1) USING SEPARATOR=",", HEADER="true", EOL="\n";
      }
      RUN LOADING JOB load_healthcarereferral

    #+end_src
*** Create Queries
    #+begin_src sql :tangle solutions/healthcarereferral/03-create-queries.gsql :mkdirp yes
      USE GRAPH HealthCareReferral
      set syntax_version = "v2"
      CREATE OR REPLACE QUERY algo_louvain(INT iter1 = 10, INT iter2 = 10, INT iter3 = 10, INT split = 10, INT outputLevel = 0) FOR GRAPH HealthCareReferral {

      /*
       ,* Louvain Method with Parallelism and Refinement
       ,* https://arxiv.org/pdf/1304.4453
       ,* The minimum label heuristics are implemented: https://doi.org/10.1016/j.parco.2015.03.003
       ,* iter: There are three phases in the algorithm -- move, merge and refine. Their max number of iterations are set by iter1, iter2, iter3 respectively.
       ,* split: To save memory, split number is 10 by default. When the split number is larger, the query is closer to sequential Louvain Method, which is slower. \
       ,* When the split number is 1, the query is parallel, but requires more memory.
       ,* outputLevel: 0, only list number; 1, also list members
       ,* fComm, fDist: files to store community label and community distribution
       ,*/

              TYPEDEF TUPLE <INT csize, INT number> ClusterNum;
              TYPEDEF TUPLE <VERTEX node, INT cid, FLOAT deltaQ> vDeltaQ;
              HeapAccum<vDeltaQ>(1, deltaQ DESC, cid ASC) @largestDeltaQ;   # if deltaQ is the same, select the one with mininal vid
              MapAccum<INT, FLOAT> @@totIncidentCluster;   # sun of weight incident to clusters
              MapAccum<INT, INT> @@clusterSizes;                # size of a cluster
              MapAccum<INT, FLOAT> @weightToCluster;  # weight from one vertex incident to that cluster

              SumAccum<FLOAT> @@totalWeight;   # total weight of all edges

              SumAccum<FLOAT> @weight;          # total weight incident to this vertex

              SumAccum<FLOAT> @cweight;       # total weight incident to this aggregate vertex

              SumAccum<INT> @uid;        # which vertex it belongs to

              SumAccum<INT> @cid;        # which cluster it belongs to

              SumAccum<INT> @vid;        # internal id

              SumAccum<FLOAT> @deltaQ;         # contribution to the modularity

              SumAccum<FLOAT> @@modularity;

              SumAccum<FLOAT> @@modularity2;

              MapAccum<INT, MapAccum<INT, FLOAT>> @@weightToClusterMap;   # calculate edges between communities

              MapAccum<INT, SetAccum<INT>> @@moveComm; # map of communities that changed its community id

              MapAccum<INT, MinAccum<VERTEX>> @@representMap;

              SetAccum<VERTEX> @@representSet;

              MapAccum<INT, FLOAT> @@vertexMap;

              MapAccum<INT, MapAccum<INT, FLOAT>> @@edgeMap;

              HeapAccum<ClusterNum>(100, csize ASC) @@clusterDist;

              MapAccum<INT, INT> @@clusterMap;

              MapAccum<INT, ListAccum<INT>> @@clusterMembers;

              FLOAT last_modularity = 0;

              FLOAT last_modularity2 = 0;

              INT iteration;

              INT Iter1;

              FLOAT epsilon = 0.0001;

              INT iteration2;

              INT partitions;

              INT loop;

              INT debug = 0;  # debug: 0, no modularity info; 1, show debug log; 2, modularity for each iteration



              partitions = split;

              CASE WHEN split < 1 THEN

                      partitions = 1;

              END;



      # Initialize: count edges and set a unique cluster ID for each vertex
              Start (ANY) = {Prescriber.*};

              S (ANY) = SELECT s

                  FROM Start:s -((referral>|<referral):e)- :t

                  ACCUM @@totalWeight += e.num_patient,

                        s.@weight += e.num_patient

                  POST-ACCUM s.@vid = getvid(s),

                             s.@uid = s.@vid,

                             s.@cid = s.@vid;  # Label each vertex with its own internal ID



      # Special first iteration of Phase 1

              iteration = 1;

              S = SELECT s

                  FROM Start:s -((referral>|<referral):e)- :t

                  WHERE s.@cid > t.@cid

                  ACCUM s.@largestDeltaQ += vDeltaQ(t, t.@cid, e.num_patient - 2 * s.@weight * s.@weight/ @@totalWeight)

                        # weightToCluster is just e.num_patient

                  POST-ACCUM INT bestCluster = s.@largestDeltaQ.top().cid,

                             IF s.@largestDeltaQ.size() > 0 and s.@largestDeltaQ.top().deltaQ > 0 and s.@cid != bestCluster THEN

                                     s.@cid = bestCluster

                             END,

                             s.@largestDeltaQ.clear();



              S = SELECT s

                  FROM Start:s-((referral>|<referral):e)-:t

                  WHERE s.@cid == t.@cid

                  ACCUM @@modularity += e.num_patient - s.@weight * t.@weight / (@@totalWeight);



              @@modularity = @@modularity / @@totalWeight;

              PRINT iteration AS Phase1Iter, @@modularity;

              log(debug > 0, "[redrain]#move", iteration, @@modularity);



      # Phase 1 -- Move

      # For each vertex, calculate the change in modularity FROM adding it to each of the nearby clusters

      # Add vertex to cluster with highest positive change in modularity

      # Repeat the above until no vertices change cluster anymore

              S = SELECT s

                  FROM Start:s

                  ACCUM @@totIncidentCluster += (s.@cid -> s.@weight);



              iteration = 1;

              Iter1 = iter1 - 1;



              WHILE (iteration < 2 OR @@modularity - last_modularity > epsilon) LIMIT Iter1 DO

                      iteration = iteration + 1;

                      loop = 0;

                      WHILE (loop < partitions) DO

                              S = SELECT s

                                  FROM Start:s -((referral>|<referral):e)- :t

                                  WHERE s.@uid % partitions == loop    # for different split

                                        # At least one cluster not singlet(a cluster on its own). If both clusters are singlets, consider only when the label of target is smaller to avoid swap

                                        AND (( abs(s.@weight - @@totIncidentCluster.get(s.@cid)) > epsilon   # s is not a singlet

                                        OR abs(t.@weight - @@totIncidentCluster.get(t.@cid)) > epsilon )     # or t is not a singlet

                                        OR (abs(s.@weight - @@totIncidentCluster.get(s.@cid)) < epsilon      # s is a singlet

                                        AND abs(t.@weight - @@totIncidentCluster.get(t.@cid)) < epsilon      # t is also a singlet

                                        AND s.@cid > t.@cid) )                                               # consider only when target label is smaller

                                  ACCUM s.@weightToCluster += (t.@cid -> e.num_patient)

                                  POST-ACCUM INT bestCluster = s.@cid,

                                             FLOAT maxDeltaQ = 0.0,

                                             FLOAT deltaQ_new = 0.0,

                                             FOREACH (cluster, weightToC) IN s.@weightToCluster DO   #would be better if this can be distributed

                                                     FLOAT incident = @@totIncidentCluster.get(cluster),

                                                     deltaQ_new = weightToC - 2 * incident * s.@weight/ @@totalWeight,

                                                     IF deltaQ_new > maxDeltaQ OR (abs(deltaQ_new - maxDeltaQ) < epsilon AND cluster < bestCluster) THEN   # when deltaQ_new is equal to maxDeltaQ, and the cluster label is smaller, also change

                                                             maxDeltaQ = deltaQ_new,

                                                             bestCluster = cluster

                                                     END

                                             END,

                                             IF s.@cid != bestCluster THEN

                                                     @@totIncidentCluster += (s.@cid -> (-1 * s.@weight)),

                                                     @@totIncidentCluster += (bestCluster -> s.@weight),

                                                     s.@cid = bestCluster

                                             END,

                                             s.@weightToCluster.clear();

                              loop = loop + 1;

                      END;

                      last_modularity = @@modularity;

                      @@modularity = 0;

                      T1 = SELECT s

                           FROM Prescriber:s-((referral>|<referral):e)-:t

                           WHERE s.@cid == t.@cid

                           ACCUM @@modularity += e.num_patient - s.@weight * t.@weight / (@@totalWeight);

                      @@modularity = @@modularity / @@totalWeight;

                      PRINT iteration AS Phase1Iter, @@modularity;

                      log(debug > 0, "[redrain]#move", iteration, @@modularity);

              END;



      # Phase 2 --  Merge
              iteration2 = 0;

              WHILE (iteration2 < 2 OR @@modularity2 - last_modularity2 > epsilon) LIMIT iter2 DO

                      iteration2 = iteration2 + 1;

                      Start = SELECT s

                              FROM Start:s

                              ACCUM s.@uid = s.@cid;

                      # Select the vertices with minimal internal id to represent the coarsened graph

                      Start = SELECT s

                              FROM Start:s

                              ACCUM @@representMap += (s.@cid -> s);



                      FOREACH (key, value) IN @@representMap DO

                              @@representSet += value;

                      END;

                      represent = {@@representSet};

                      @@representMap.clear();

                      @@representSet.clear();

                      log(debug > 0, "[redrain]#2_merge", represent.size()); #@@clusterSizes.size());



                  # Get @cweight from totalIncident

                      represent = SELECT s

                                  FROM represent:s

                                  ACCUM s.@cweight = @@totIncidentCluster.get(s.@uid),

                                        @@clusterSizes += (s.@cid -> 1);



                      log(debug > 1, "[redrain]#2_merge", @@weightToClusterMap.size());

                      iteration = 0;

                      last_modularity = 0;

                      @@modularity = 0;



                      WHILE (iteration < 2 OR @@modularity - last_modularity > epsilon) limit iter1 DO

                              iteration = iteration + 1;



                              # Calculate.num_patient incident from vertex to cluster in coarsened graph; change every interation

                              S = SELECT s

                                  FROM Start:s -((referral>|<referral):e)-:t

                                  WHERE s.@cid != t.@cid AND @@totIncidentCluster.get(s.@uid) > 0 AND @@totIncidentCluster.get(t.@cid) > 0   #@@totIncidentCluster keeps changing, can be 0

                                  ACCUM @@weightToClusterMap += (s.@uid -> (t.@cid -> e.num_patient));  # from s, incident to some clusters. Not consider the same cluster

                              represent = SELECT s

                                          FROM represent:s

                                          POST-ACCUM INT bestCluster = s.@cid,

                                                     FLOAT maxDeltaQ = 0.0,

                                                     FLOAT deltaQ_new = 0.0,

                                                     FOREACH (cluster, weightToC) IN @@weightToClusterMap.get(s.@uid) DO

                                                             FLOAT incident = @@totIncidentCluster.get(cluster),

                                                             IF @@clusterSizes.get(s.@cid) == 1 AND @@clusterSizes.get(cluster) == 1 AND s.@cid < cluster THEN

                                                                     CONTINUE

                                                             END,

                                                             deltaQ_new = weightToC - 2 * incident * s.@cweight/ @@totalWeight, #total weight should be the same

                                                             IF deltaQ_new > maxDeltaQ OR abs(deltaQ_new - maxDeltaQ) < epsilon AND cluster < bestCluster THEN      # new cluster is smaller then the current best cluster

                                                                     maxDeltaQ = deltaQ_new,

                                                                     bestCluster = cluster

                                                             END

                                                     END,

                                                     IF s.@cid != bestCluster THEN

                                                             @@totIncidentCluster += (s.@cid -> (-1 * s.@cweight)),

                                                             @@totIncidentCluster += (bestCluster -> s.@cweight),

                                                             @@moveComm += (s.@uid -> bestCluster),

                                                             @@clusterSizes += (s.@cid -> -1),

                                                             @@clusterSizes += (bestCluster -> 1),

                                                             s.@cid = bestCluster

                                                     END;

                              log(debug > 1, "[redrain]#2_merge", @@weightToClusterMap.size());

                              @@weightToClusterMap.clear();



                              log(debug > 1, "[redrain]#2_move:", @@moveComm.size());

                              # move nodes

                              S = SELECT s

                                  FROM Start:s

                                  WHERE @@moveComm.containsKey(s.@uid)

                                  POST-ACCUM FOREACH v IN @@moveComm.get(s.@uid) DO

                                                     s.@cid = v

                                             END;

                              @@moveComm.clear();



                              last_modularity = @@modularity;

                              @@modularity = 0;



                              S = SELECT s

                                  FROM Start:s-((referral>|<referral):e)-:t

                                  WHERE s.@cid == t.@cid

                                  ACCUM @@modularity += e.num_patient - s.@weight * t.@weight / (@@totalWeight);

                                  @@modularity = @@modularity / @@totalWeight;

                                  PRINT iteration AS Phase1Iter, @@modularity;

                              log(debug > 0, "[redrain]#2_move", iteration, @@modularity);

                      END;



                      S = SELECT s

                          FROM represent:s

                          ACCUM s.@cweight = 0;

                      @@clusterSizes.clear();



                      last_modularity2 = @@modularity2;

                      @@modularity2 = @@modularity;

                      PRINT iteration2 AS Phase2Iter, @@modularity2;

                      log(debug > 0, "[redrain]#2_merge", iteration2, @@modularity2);



              END;





      # Phase 3 -- Refinement

              iteration = 0;

              @@modularity = 0;

              WHILE (iteration < 2 OR @@modularity - last_modularity > epsilon) LIMIT iter3 DO

                      iteration = iteration + 1;

                      S = SELECT s

                          FROM Start:s -((referral>|<referral):e)- :t

                          WHERE abs(s.@weight - @@totIncidentCluster.get(s.@cid)) > epsilon OR abs(t.@weight - @@totIncidentCluster.get(t.@cid)) > epsilon OR (abs(s.@weight - @@totIncidentCluster.get(s.@cid)) < epsilon AND abs(t.@weight - @@totIncidentCluster.get(t.@cid)) < epsilon AND s.@cid > t.@cid)   # at least one cluster not only itself, or use smaller label

                          ACCUM s.@weightToCluster += (t.@cid -> e.num_patient)

                          POST-ACCUM

                                  INT bestCluster = s.@cid,

                                  FLOAT maxDeltaQ = 0.0,

                                  FLOAT deltaQ_new = 0.0,

                                  FOREACH (cluster, weightToC) IN s.@weightToCluster DO   #would be better if this can be distributed

                                          FLOAT incident = @@totIncidentCluster.get(cluster),

                                          deltaQ_new = weightToC - 2 * incident * s.@weight/ @@totalWeight,

                                          IF deltaQ_new > maxDeltaQ OR (abs(deltaQ_new - maxDeltaQ) < epsilon AND cluster < bestCluster) THEN   # when deltaQ_new is equal to maxDeltaQ, and the cluster label is smaller, also change

                                                  maxDeltaQ = deltaQ_new,

                                                  bestCluster = cluster

                                          END

                                  END,

                                  IF s.@cid != bestCluster THEN

                                          @@totIncidentCluster += (s.@cid -> (-1 * s.@weight)),

                                          @@totIncidentCluster += (bestCluster -> s.@weight),

                                          s.@cid = bestCluster

                                  END,

                                  s.@weightToCluster.clear();



                      last_modularity = @@modularity;

                      @@modularity = 0;

                      T1 = SELECT s

                           FROM Start:s-((referral>|<referral):e)-:t

                           WHERE s.@cid == t.@cid

                           ACCUM @@modularity += e.num_patient - s.@weight * t.@weight / (@@totalWeight);

                      @@modularity = @@modularity / @@totalWeight;

                      PRINT iteration AS Phase3Iter, @@modularity;

                      log(debug > 0, "[redrain]#refine", iteration, @@modularity);

              END;





              Print Start [Start.@cid];

              Start = {ANY};

              Start = SELECT s FROM Start:s

                      POST-ACCUM @@clusterSizes += (s.@cid -> 1),s.communityId=s.@cid

                                 ;

              log(TRUE, @@clusterSizes.size());



              IF outputLevel ==0 THEN

                      FOREACH (cluster, csize) IN @@clusterSizes DO

                              @@clusterMap += (csize -> 1);

                      END;

                      FOREACH (csize, number) IN @@clusterMap DO

                              @@clusterDist += ClusterNum(csize, number);

                      END;

                      PRINT @@clusterDist;

              ELSE

                      FOREACH (cluster, csize) IN @@clusterSizes DO

                              @@clusterMembers += (csize -> cluster);

                      END;

                      PRINT @@clusterMembers;
          PRINT "Community Detection Done";

              END;
      }
      CREATE OR REPLACE QUERY ex2_createReferralEdge(VERTEX<Prescriber> inputPrescriber) FOR GRAPH HealthCareReferral {
          OrAccum<BOOL> @visited, @isReferredClaim;

          ListAccum<DATETIME> @dateList;

          start_set = {inputPrescriber};

          claims = SELECT t FROM start_set:s-(<submitted_by:e)-:t
                   POST-ACCUM t.@visited = true;

          patients = SELECT t FROM claims:s-(associated>:e)-:t
                     ACCUM t.@dateList += s.rx_fill_date;

          claims = SELECT t FROM patients:s-(<associated:e)-:t
                   WHERE t.@visited == false
                   ACCUM FOREACH dt in s.@dateList do
                           CASE WHEN datetime_diff(dt, t.rx_fill_date) BETWEEN 0 AND 2592000 THEN
                             t.@isReferredClaim = true
                           END
                         END
                   HAVING t.@isReferredClaim == true;

          prescribers = SELECT t FROM claims-(submitted_by>:e)-:t
                        POST-ACCUM INSERT INTO referral VALUES(inputPrescriber, t, 1);
          print start_set;

      }
      CREATE OR REPLACE QUERY algo_pageRank(FLOAT maxChange, INT maxIter, FLOAT damping, INT outputLimit) FOR GRAPH HealthCareReferral {

          # Compute the pageRank score for each vertex in the GRAPH
      # In each iteration, compute a score for each vertex:
      #   score = (1-damping) + damping*sum(received scores FROM its neighbors).
      # The pageRank algorithm stops when either of the following is true:
      #  a) it reaches maxIter iterations;
      #  b) the max score change for any vertex compared to the last iteration <= maxChange.

              TYPEDEF TUPLE<vertex Vertex_ID, FLOAT score> vertexScore;
              HeapAccum<vertexScore>(outputLimit, score DESC) @@topScores;
              MaxAccum<float> @@maxDiff = 9999; # max score change in an iteration
              SumAccum<float> @received_score = 0; # sum of scores each vertex receives FROM neighbors
              SumAccum<float> @score = 1;   # Initial score for every vertex is 1.
              SetAccum<EDGE> @@edgeSet;                   # list of all edges, if display is needed

              Start = {Prescriber.*};   #  Start with all vertices of specified type(s)
                V (ANY) = {};
              WHILE @@maxDiff > maxChange LIMIT maxIter DO
                      @@maxDiff = 0;
                      V = SELECT s
                          FROM Start:s -(referral>:e)- :t
                          ACCUM t.@received_score += s.@score/(s.outdegree("referral"))
                          POST-ACCUM s.@score = (1.0-damping) + damping * s.@received_score,
                                     s.@received_score = 0,
                                     @@maxDiff += abs(s.@score - s.@score');
              END; # END WHILE loop


              IF outputLimit > 0 THEN
                      V = SELECT s FROM Start:s
                          POST-ACCUM @@topScores += vertexScore(s, s.@score),s.pageRank=s.@score;
                      PRINT @@topScores;
              END;

      }
      CREATE OR REPLACE QUERY getPatients(vertex<Prescriber> inputPrescriber) FOR GRAPH HealthCareReferral {

          ListAccum<EDGE> @@list;

          start_set = {inputPrescriber};

          claims = SELECT t FROM start_set:s-(<submitted_by:e)-:t
                   ACCUM @@list += e;

          patients = SELECT t FROM claims:s-(associated>:e)-:t
                     ACCUM @@list +=e;

          print claims, @@list;
      }
      CREATE OR REPLACE QUERY ex1_commonPatients(vertex<Prescriber> Prescriber1, vertex<Prescriber> Prescriber2) FOR GRAPH HealthCareReferral {

            OrAccum @visited;
          SetAccum<edge> @@edgeSet;
          Pre1 =  {Prescriber1};
          Pre2 =  {Prescriber2};
          /* Step 1  Start graph Traversal from first prescriber to find all associated claims. Use visited flag to remember claims visited. */
          claims1 = select t
                    from Pre1:s -(<submitted_by:e)- Claim:t
                    accum t.@visited += true;
           /* Step 2  For those claims, find all the linked patients. */
          patients1 = select t
                      from claims1:s -(associated>:e)- Patient:t
                      accum t.@visited += true;
          /* Step 3 Start graph traversal from second prescriber to find all claims */
          claims2 = select t
                    from Pre2:s -(<submitted_by:e)- Claim:t
                    accum t.@visited += true;

          /* Step 4  Find common patients  by starting from claims in Step 3 */
          common_patients = select t
                            from claims2:s -(associated>:e)- Patient:t
                            where t.@visited == true;
          print common_patients;

            /* Step 5  From common patients find all claims that have been visited in earlier steps. Collect the edges so they can be printed.*/
          claims = select t
                   from common_patients:s -(<associated:e)- Claim:t
                   where t.@visited == true
                   accum @@edgeSet += e;
          print claims;

            /* Step 6  From claims find associated prescribers. Collect and print edges (claims  prescribers) and prescribers. */
          pres = select t
                 from claims:s -(submitted_by>:e)- Prescriber:t
                 accum @@edgeSet += e;
          print pres;
          print @@edgeSet;
      }
      CREATE OR REPLACE QUERY getKhopNeighbor(int k, vertex input) FOR GRAPH HealthCareReferral {

          OrAccum<BOOL> @visited;
          ListAccum<EDGE> @@edgeList;

        start = {input};

          WHILE start.size() > 0 limit k DO
            start = SELECT t from start-(:e)-:t
                    WHERE t.@visited == false
                    ACCUM @@edgeList += e
                    POST-ACCUM t.@visited = true;
          END;

          print @@edgeList;
      }
      CREATE OR REPLACE QUERY Print_community(vertex<Prescriber> inputPrescriber) FOR GRAPH HealthCareReferral {
        /* Write query logic here */
        //PRINT "Print_community works!";

          ListAccum<EDGE> @@edgeList;

          SumAccum<int> @@cid;

          Start={inputPrescriber};
          Start=Select s from Start:s post-accum @@cid += s.communityId;

          Start = {Prescriber.*};

          Start = select s from Start:s-(referral>:e)-:t
                  where s.communityId == @@cid and s.communityId == t.communityId
                  accum @@edgeList += e;

          print Start;
          print @@edgeList;
      }
      CREATE OR REPLACE QUERY getClaims(vertex<Prescriber> inputPrescriber) FOR GRAPH HealthCareReferral {

          ListAccum<EDGE> @@list;

          start_set = {inputPrescriber};

          claims = SELECT t FROM start_set:s-(<submitted_by:e)-:t
                   ACCUM @@list += e;

          print claims, @@list;
      }
      CREATE OR REPLACE QUERY getPrescribers(vertex<Prescriber> inputPrescriber) FOR GRAPH HealthCareReferral {
        ListAccum<EDGE> @@list;
          OrAccum<BOOL> @visited;

          start_set = {inputPrescriber};

          claims = SELECT t FROM start_set:s-(<submitted_by:e)-:t
                   ACCUM @@list += e
                   POST-ACCUM t.@visited = true;

          patients = SELECT t FROM claims:s-(associated>:e)-:t
                     ACCUM @@list +=e;

          claims = SELECT t FROM patients:s-(<associated:e)-:t
                   WHERE t.@visited == false
                   ACCUM @@list +=e;

          prescribers = SELECT t FROM claims-(submitted_by>:e)-:t
                        ACCUM @@list +=e;

          print prescribers, @@list;
      }
      CREATE OR REPLACE QUERY ex2_main_query(/* Parameters here */) FOR GRAPH HealthCareReferral {

          all_prescribers = select s from Prescriber:s accum ex2_createReferralEdge(s);

        print all_prescribers;
      }
    #+end_src
* Transfer Solutions to TigerGraph server
  First set up all the parameters needed to access the TG server (usually the =m1= node)

  We will use a TRAMP URI which can support any relevant protocol: =ssh=, =docker=,
  =kube=.  In the case of a bastion host, use the multi-hop TRAMP syntax.

  The user name (=tigergraph= by default) is encoded into the TRAMP URI for =ssh= and
  =docker= methods.  For =kube=, the Kubernetes namespace is specified instead, and the
  correct user will be used by the image used for Kubernetes.

  Examples of TRAMP URIs:
  + /ssh:tigergraph@localhost:
  + /ssh:centos@gcp-bastion.34.106.15.242.nip.io|ssh:tigergraph@gg-tg-0:  # multi-hop
  + /ssh:protomolecule|ssh:tigergraph@192.168.122.67: # multi-hop to a VM
  + /docker:tigergraph@tigergraph:
  + /kube:tigergraph@tigergraph-0:  # in this case, use <namespace>@<pod-name>

** Configuration
   Just set TRAMP URI and subdir
  #+NAME: tramp-uri
  : /kube:poodle@tigergraph-0:
  #+NAME: subdir
  : mydata/

  #+begin_src emacs-lisp :var uri=tramp-uri
    (message uri)
    (cd uri)
    ;; (shell-command "gadmin config get System.DataRoot")
    ;; (call-process-shell-command "")
    (shell "xfer-sh")
  #+end_src

  #+NAME: data-root
  #+begin_src bash :session xfer-sh
    gadmin config get System.DataRoot
  #+end_src

  #+RESULTS: data-root
  : /home/tigergraph/tigergraph/data

** Copy it all by evaluating this Elisp code block
  Copy everything in the local =solutions= directory to the solutions path,
  creating subdirs as necessary.  =C-c C-c=
  #+begin_src emacs-lisp :var uri=tramp-uri :var dataroot=data-root  :var subdir=subdir :results value replace
    (copy-directory "solutions" (concat uri dataroot "/" subdir) nil t t)
    (concat "Copied solutions to " uri dataroot "/" subdir)
  #+end_src

  #+RESULTS:
  : Copied solutions to /kube:poodle@tigergraph-0:/home/tigergraph/mydata/
