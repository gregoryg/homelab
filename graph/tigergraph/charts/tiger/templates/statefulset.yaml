# TODO: set up headless service for 3 ports: api, ui, ssh
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: {{ .Values.name }}
  labels:
    app: tigergraph
    {{- include "tiger.labels" . | nindent 4 }}
spec:
  {{- if not .Values.autoscaling.enabled }}
  replicas: {{ .Values.replicaCount }}
  {{- end }}
  podManagementPolicy: {{ .Values.podManagementPolicy }}
  selector:
    matchLabels:
      app: tigergraph
      {{- include "tiger.selectorLabels" . | nindent 6 }}
  serviceName: {{ .Values.name }}
  template:
    metadata:
      {{- with .Values.podAnnotations }}
      annotations:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      labels:
        app: tigergraph
        {{- include "tiger.selectorLabels" . | nindent 8 }}
    spec:
      {{- with .Values.imagePullSecrets }}
      imagePullSecrets:
        {{- toYaml . | nindent 8 }}
      {{- end }}
      # serviceAccountName: {{ include "tiger.serviceAccountName" . }}
      {{- if empty .Values.nodeSelector.label }}
      nodeSelector: {}
      {{- else }}
      nodeSelector:
        {{ .Values.nodeSelector.label }}: {{ .Values.nodeSelector.value }}
      {{- end }}
      securityContext:
        {{- toYaml .Values.podSecurityContext | nindent 8 }}
      containers:
      - name: tigergraph
        env:
        - name: SERVICE_NAME
          valueFrom:
            configMapKeyRef:
              name: env-config
              key: service.headless.name
        - name: POD_PREFIX
          valueFrom:
            configMapKeyRef:
              name: env-config
              key: pod.prefix
        - name: NAMESPACE
          valueFrom:
            configMapKeyRef:
              name: env-config
              key: namespace
        - name: CLUSTER_SIZE
          valueFrom:
            configMapKeyRef:
              name: env-config
              key: cluster_size
        securityContext:
          {{- toYaml .Values.securityContext | nindent 12 }}
        image: "{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}"
        imagePullPolicy: {{ .Values.image.pullPolicy }}
        # command: ["/bin/sleep", "24d"]
        ports:
        - containerPort: 9000
          name: rest
        - containerPort: 14240
          name: graphstudio
        - containerPort: 22
          name: ssh
        livenessProbe:
          initialDelaySeconds: 300
          failureThreshold: 3
          timeoutSeconds: 2
          periodSeconds: 30
          tcpSocket:
            port: 9000
        # readinessProbe:
        #   httpGet:
        #     path: /
        #     port: http
        resources:
          {{- toYaml .Values.resources | nindent 12 }}
        volumeMounts:
        - mountPath: {{ .Values.dataVolume.mountPath | quote }}
          name: {{ .Values.dataVolume.name  }}
        - mountPath: /tmp/init_tg_cfg
          name: config-volume
          subPath: init_tg_cfg
        {{- if .Values.ingestVolume.create }}
        - mountPath: {{ .Values.ingestVolume.mountPath | quote }}
          name: {{ .Values.ingestVolume.name }}
        {{- end }}

        lifecycle:
          postStart:
            exec:
              command:
              - "/bin/bash"
              - "-c"
              - |
                (
                  if [ "$(ls -A /home/tigergraph/tigergraph/data/|grep -v lost|tail -1)" ]; then
                    for i in $(seq 1 ${CLUSTER_SIZE});
                    do
                      until nslookup ${POD_PREFIX}-$((i-1)).${SERVICE_NAME}.${NAMESPACE}.svc.cluster.local;
                      do
                        echo "wait dns to be updated";
                        sleep 1;
                      done;
                    done;
                    sleep 15;
                    export PATH=/home/tigergraph/tigergraph/app/cmd:$PATH
                    ln -sf /home/tigergraph/tigergraph/data/configs/tg.cfg /home/tigergraph/.tg.cfg
                    grun all "hostname"
                    echo "starting service at $(date)"
                    gadmin start all --with-config /home/tigergraph/.tg.cfg;
                  else
                    sudo chown -R tigergraph:tigergraph /home/tigergraph/tigergraph/data;
                    tg_cfg=$(find /home/tigergraph/tigergraph/app/ -name .tg.cfg|head -n 1)
                    ln -sf $tg_cfg .tg.cfg
                  fi
                ) > /tmp/init.log 2>&1 &
                disown -a
                exit 0
      volumes:
      - name: config-volume
        configMap:
          name: tg-config
          items:
          - key: init_tg_cfg
            path: init_tg_cfg

  volumeClaimTemplates:
  - metadata:
      name: {{ .Values.dataVolume.name }}
    spec:
      accessModes:
        - {{ .Values.dataVolume.accessMode | quote }}
    {{- if .Values.dataVolume.storageClass }}
    {{- if (eq "-" .Values.dataVolume.storageClass) }}
      storageClassName: ""
    {{- else }}
      storageClassName: "{{ .Values.dataVolume.storageClass }}"
      {{- end }}
    {{- end }}
      resources:
        requests:
          storage: "{{ .Values.dataVolume.size }}"
  {{- if .Values.ingestVolume.create }}
  - metadata:
      name: {{ .Values.ingestVolume.name }}
    spec:
      accessModes:
        - {{ .Values.ingestVolume.accessMode | quote }}
    {{- if .Values.ingestVolume.storageClass }}
    {{- if (eq "-" .Values.ingestVolume.storageClass) }}
      storageClassName: ""
    {{- else }}
      storageClassName: "{{ .Values.ingestVolume.storageClass }}"
      {{- end }}
    {{- end }}
      resources:
        requests:
          storage: "{{ .Values.ingestVolume.size }}"
    {{- end }}
