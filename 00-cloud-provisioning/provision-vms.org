#+PROPERTY: header-args:bash :comments org :shebang #!/usr/bin/env bash :tangle no :eval never-export
#+setupfile: ~/projects/emacs/org-html-themes/org/theme-readtheorg.setup
* Provision HA VMs in GCP

** OS options for cluster nodes
   The =cluster-os= will be referred to in subsequent install scripts.  Select a value
   from the first column in =os-sets= table
   #+name: os-sets
   | OS     | image-family    | image-project  |
   |--------+-----------------+----------------|
   | debian | debian-10       | debian-cloud   |
   | ubuntu | ubuntu-2004-lts | ubuntu-cloud   |
   | leap   | opensuse-leap   | opensuse-cloud |
   | centos | centos-7        | centos-cloud   |

** Select OS for cluster nodes

   #+name: cluster-os
   #+begin_src emacs-lisp :var os=os-sets[,0] :results table replace
     (completing-read "Ohai: " os)
   #+end_src

   #+name: cluster-image-family
    #+begin_src bash :var ostable=os-sets :var clusteros=cluster-os :results table replace
     echo ${ostable[$clusteros]}|cut -d' ' -f1
   #+end_src

   #+name: cluster-image-project
    #+begin_src bash :var ostable=os-sets :var clusteros=cluster-os :results table replace
     echo ${ostable[$clusteros]}|cut -d' ' -f2
    #+end_src

** Network resource setup and cleanup scripts
   Provision network, subnetwork and NAT gateways for use with all nodes in this file

   #+begin_src bash :tangle ~/bin/provision-gg-kubernetes-net.sh :async true
     gcloud compute networks create gg-rancher-ha --subnet-mode custom

     # Create the =gg-kubernetes= subnet in the =gg-rancher-ha= VPC network:

     # Create the =gg-rancher-ha= custom VPC network:
     gcloud compute networks subnets create gg-kubernetes \
            --network gg-rancher-ha \
            --range 10.240.0.0/24

     # Create the Cloud NAT and Cloud Router for outbound internet access

     gcloud compute routers create ggrha-router \
            --network gg-rancher-ha

     gcloud compute routers nats create ggrha-nat \
            --router=ggrha-router \
            --auto-allocate-nat-external-ips \
            --nat-all-subnet-ip-ranges \
            --enable-logging

     # Create a firewall rule that allows internal communication across all protocols:


     gcloud compute firewall-rules create gg-rancher-ha-allow-internal \
            --allow tcp,udp,icmp \
            --network gg-rancher-ha \
            --source-ranges 10.240.0.0/24,10.200.0.0/16

     # Create a firewall rule that allows external SSH, ICMP, and HTTPS:


     gcloud compute firewall-rules create gg-rancher-ha-allow-external \
            --allow tcp:22,tcp:6443,icmp \
            --network gg-rancher-ha \
            --source-ranges 0.0.0.0/0

     # No need for public IP
     # gcloud compute addresses create gg-rancher-ha \
         #   --region $(gcloud config get-value compute/region)


   #+end_src

   #+begin_src bash :tangle ~/bin/cleanup-gg-kubernetes-net.sh
     # Delete Cloud NAT and Cloud Router

     gcloud -q compute routers nats delete ggrha-nat --router ggrha-router
     gcloud -q compute routers delete ggrha-router

     # Delete the external load balancer network resources:


     gcloud -q compute forwarding-rules delete kubernetes-forwarding-rule \
            --region $(gcloud config get-value compute/region)

     gcloud -q compute target-pools delete kubernetes-target-pool

     gcloud -q compute http-health-checks delete kubernetes

     gcloud -q compute addresses delete gg-rancher-ha

     # Delete the =gg-rancher-ha= firewall rules:


     gcloud -q compute firewall-rules delete \
            gg-rancher-ha-allow-nginx-service \
            gg-rancher-ha-allow-internal \
            gg-rancher-ha-allow-external \
            gg-rancher-ha-allow-health-check

     # Delete the =gg-rancher-ha= network VPC:


     gcloud -q compute routes delete \
            kubernetes-route-10-200-0-0-24 \
            kubernetes-route-10-200-1-0-24 \
            kubernetes-route-10-200-2-0-24

     gcloud -q compute networks subnets delete gg-kubernetes

     gcloud -q compute networks delete gg-rancher-ha
   #+end_src
** Provision the cluster nodes

  This script creates 3 etcd nodes backed by high-IOPS disk, 3 controlplane nodes,
  $NUM_WORKERS worker nodes and a bastion / gateway host.

  The script is modeled after the =gregoryg= fork of Kelsey Hightower's Kubernetes the
  Hard Way repo.
  + [[https://github.com/gregoryg/kubernetes-the-hard-way][gregoryg's fork kubernetes-the-hard-way]]
  + [[https://github.com/kelseyhightower/kubernetes-the-hard-way][kelseyhightower/kubernetes-the-hard-way]] - the original

  We create images with a particular OS for all cluster nodes, and separately for the
  bastion host



  #+begin_src bash :tangle ~/bin/provision-rancher-ha-gcp.sh :var IMAGE_FAMILY=cluster-image-family :var IMAGE_PROJECT=cluster-image-project
    NUM_CONTROL=1
    NUM_ETCD=0   # 0 for co-location with controlplane
    NUM_WORKERS=3
    # family / project pairs such as
    ## ubuntu-1604-lts / ubuntu-os-cloud
    ## opensuse-leap / opensuse-cloud
    ## debian-10 / debian-cloud
    # IMAGE_FAMILY=${IMAGE_FAMILY}
    # IMAGE_PROJECT=${IMAGE_PROJECT}
    # build up lists of nodes for use with PDSH and suchlike
    rm -vf /tmp/worker-nodes.txt /tmp/control-nodes.txt /tmp/all-nodes.txt

    # assure our network exists
    netname=$(gcloud compute networks describe gg-kubernetes --format=json|jq -r '.name')
    if [ "${netname}" != "gg-kubernetes" ] ; then
       ~/bin/provision-gg-kubernetes-net.sh
    fi
    # Create three compute instances which will host the Kubernetes control plane:

    CONTROL_INSTANCE_TYPE=e2-standard-4
      for i in $(seq 0 $((${NUM_CONTROL} - 1))) ; do
      # for i in 0 1 2; do
        gcloud compute instances create gg-control-${i} \
          --async \
          --no-address \
          --boot-disk-size 200GB \
          --can-ip-forward \
          --image-family ${IMAGE_FAMILY} \
          --image-project ${IMAGE_PROJECT} \
          --machine-type ${CONTROL_INSTANCE_TYPE} \
          --subnet gg-kubernetes \
          --private-network-ip 10.240.0.1${i} \
          --scopes compute-rw,storage-ro,service-management,service-control,logging-write,monitoring \
          --tags gg-rancher-ha,control \
          --labels owner=greg_grubbs,donotdelete=true
        echo gg-control-${i} >> /tmp/control-nodes.txt
      done

    # Create ${NUM_WORKERS} compute instances which will host the Kubernetes worker nodes:

    WORKER_INSTANCE_TYPE=e2-standard-8
      for i in $(seq 0 $((${NUM_WORKERS} - 1))) ; do
      # for i in 0 1 2; do
        gcloud compute instances create gg-worker-${i} \
          --async \
          --no-address \
          --boot-disk-size 200GB \
          --can-ip-forward \
          --image-family ${IMAGE_FAMILY} \
          --image-project ${IMAGE_PROJECT} \
          --machine-type ${WORKER_INSTANCE_TYPE} \
          --subnet gg-kubernetes \
          --metadata pod-cidr=10.200.${i}.0/24 \
          --private-network-ip 10.240.0.2${i} \
          --scopes compute-rw,storage-ro,service-management,service-control,logging-write,monitoring \
          --tags gg-rancher-ha,worker \
          --labels owner=greg_grubbs,donotdelete=true
        echo gg-worker-${i} >> /tmp/worker-nodes.txt
      done
    cat /tmp/control-nodes.txt /tmp/worker-nodes.txt > /tmp/all-nodes.txt
  #+end_src
** Provision the bastion host
   Put up a simple Debian bastion host
   #+begin_src bash :tangle ~/bin/provision-bastion-gcp.sh
     BASTION_IMAGE_FAMILY=debian-10
     BASTION_IMAGE_PROJECT=debian-cloud

     # Create a small instance as a bastion/gateway host - the only VM with a public IP
     gcloud compute instances create gg-bastion \
            --async \
            --boot-disk-size 200GB \
            --can-ip-forward \
            --image-family ${BASTION_IMAGE_FAMILY} \
            --image-project ${BASTION_IMAGE_PROJECT} \
            --machine-type e2-small \
            --subnet gg-kubernetes \
            --private-network-ip 10.240.0.2 \
            --scopes compute-rw,storage-ro,service-management,service-control,logging-write,monitoring \
            --tags gg-bastion \
            --labels owner=greg_grubbs,donotdelete=true

     # Assure that it is not conflicting in known_hosts
     sleep 5
     ip=$(gcloud compute instances describe gg-bastion --format='get(networkInterfaces[0].accessConfigs[0].natIP)')
     ssh-keygen -R ${ip}
     ssh gregj@${ip} -o StrictHostKeyChecking=no pwd
#+end_src
* Set up the bastion host
** Copy node names to our bastion host for use with =pdsh=
   #+begin_src bash :session rancher-rke-sh :results none
     (cd /tmp; gcloud compute scp all-nodes.txt control-nodes.txt worker-nodes.txt gg-bastion:)
   #+end_src

   Open a shell on the bastion host as buffer named =gg-bastion-sh=

  #+name: bastion_ip
   #+begin_src bash
    # Get public IP
    gcloud compute instances describe gg-bastion --format='get(networkInterfaces[0].accessConfigs[0].natIP)'
   #+end_src


   #+begin_src emacs-lisp :var ip=bastion_ip :results none
     (call-process-shell-command (concat "ssh -o StrictHostKeyChecking=no gregj@" ip " id"))
     (cd (concat "/ssh:gregj@" ip ":"))
     (shell "gg-bastion-sh")
   #+end_src
   #+begin_src bash :session gg-bastion-sh :results none
     sudo apt update ; sudo apt -y install pdsh wget
     for i in in `cat all-nodes.txt` ; do
         ssh -o StrictHostKeyChecking=no $i pwd
     done
     WCOLL=all-nodes.txt pdsh -R ssh pwd
   #+end_src

* Set up cluster nodes
** Update all nodes and install Docker


   #+begin_src bash  :session gg-bastion-sh :var clusteros=cluster-os
     if [ ${clusteros} == "leap" ] ; then
         WCOLL=all-nodes.txt pdsh -R ssh 'sudo zypper ref && sudo zypper in -y docker'
     elif [ ${clusteros} == "centos" ] ; then
         WCOLL=all-nodes.txt pdsh -R ssh 'sudo yum -y makecache && sudo yum -y install docker'
         WCOLL=all-nodes.txt pdsh -R ssh 'echo "LANG=en_US.utf-8 LC_ALL=en_US.utf-8" | sudo tee -a /etc/environment'
     else
         WCOLL=all-nodes.txt pdsh -R ssh 'sudo apt update && sudo apt -y install docker.io '
     fi
     WCOLL=all-nodes.txt pdsh -R ssh 'sudo groupadd -f docker'
     WCOLL=all-nodes.txt pdsh -R ssh 'sudo systemctl enable docker && sudo systemctl start docker'
     WCOLL=all-nodes.txt pdsh -R ssh 'sudo usermod -G docker -a gregj && sudo systemctl daemon-reload && sudo systemctl restart docker'
     WCOLL=all-nodes.txt pdsh -R ssh 'docker ps'
   #+end_src

** Create the RKE cluster
*** Download the =rke= CLI to bastion host
    Current [2020-11-06 Fri] version: 1.2.1
    #+begin_src bash :session gg-bastion-sh :async :results value
      wget 'https://github.com/rancher/rke/releases/download/v1.2.4/rke_linux-amd64'
      chmod a+rx rke_linux-amd64
      sudo mv -iv ./rke_linux-amd64 /usr/local/bin/rke
      rke
    #+end_src

*** Our canonical RKE config
    Tangle directly  to the bastion host
   #+begin_src yaml :aavar ip=bastion_ip :tangle no /gssh:gg-bastion:gcp-cluster.yaml
     nodes:
         - address: gg-control-0
           user: gregj
           role:
             - controlplane
             - etcd
         - address: gg-control-1
           user: gregj
           role:
             - controlplane
             - etcd
         - address: gg-control-2
           user: gregj
           role:
             - controlplane
             - etcd
         - address: gg-worker-0
           user: gregj
           role:
             - worker
         - address: gg-worker-1
           user: gregj
           role:
             - worker
         - address: gg-worker-2
           user: gregj
           role:
             - worker
         - address: gg-worker-3
           user: gregj
           role:
             - worker
     # If set to true, RKE will not fail when unsupported Docker versions are found
     ignore_docker_version: false

     # Cluster level SSH private key
     # Used if no ssh information is set for the node
     # ssh_key_path: ~/.ssh/k8s-local
     #
     # Enable use of SSH agent to use SSH private keys with passphrase
     # This requires the environment `SSH_AUTH_SOCK` configured pointing
     # to your SSH agent which has the private key added
     ssh_agent_auth: true

     # Set the name of the Kubernetes cluster
     cluster_name: uncowlanut


     services:

     authorization:
         mode: rbac

     # Add-ons are deployed using kubernetes jobs. RKE will give
     # up on trying to get the job status after this timeout in seconds..
     addon_job_timeout: 30

     # Specify network plugin-in (canal, calico, flannel, weave, or none)
     # GJG remember that Canal is really Flannel+Calico https://rancher.com/blog/2019/2019-03-21-comparing-kubernetes-cni-providers-flannel-calico-canal-and-weave/
     network:
         plugin: canal

     # Specify DNS provider (coredns or kube-dns)
     dns:
         provider: coredns

     addons_include:
       - https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.1/aio/deploy/recommended.yaml
       - https://gist.githubusercontent.com/superseb/499f2caa2637c404af41cfb7e5f4a938/raw/930841ac00653fdff8beca61dab9a20bb8983782/k8s-dashboard-user.yml
       - https://github.com/jetstack/cert-manager/releases/download/v0.15.1/cert-manager.yaml
   #+end_src
*** Run =rke up=
    #+begin_src bash :session gg-bastion-sh :async :results value
      rke up --config gcp-cluster.yaml
    #+end_src

*** Get kubectl and test
    #+begin_src bash :session gg-bastion-sh :async :results value
      sudo apt-get update && sudo apt-get install -y apt-transport-https gnupg2 curl
      curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
      echo "deb https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list
      sudo apt-get update
      sudo apt-get install -y kubectl
    #+end_src

    #+begin_src bash :session gg-bastion-sh :async :results value
      ## GJG
      mkdir -p ~/.kube
      cp -v kube_config_gcp-cluster.yaml ~/.kube/config
      kubectl get nodes
      source <(kubectl completion bash)
      alias k='kubectl'
      alias kn='kubectl config set-context --current --namespace '
      alias kx='kubectl config get-contexts'
      complete -F __start_kubectl k
    #+end_src


* On to Rancher to install these thangs!

** Get Helm on the bastion node
   #+begin_src bash :session gg-bastion-sh :async :results value
     wget 'https://get.helm.sh/helm-v3.4.1-linux-amd64.tar.gz'
     tar xf helm-v3.4.1-linux-amd64.tar.gz
     sudo cp -v linux-amd64/helm /usr/local/bin/
     helm version
   #+end_src

** Install Rancher 2.5 monitoring
   #+begin_src bash :session gg-bastion-sh :async :results value
     helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
     helm repo add stable https://charts.helm.sh/stable
     helm repo update
     helm install rancher-monitoring prometheus-community/kube-prometheus-stack --namespace cattle-monitoring-system --create-namespace
   #+end_src

* Clean it all up once the fun is over
   #+begin_src bash :tangle ~/bin/cleanup-rancher-ha-gcp.sh
     # Delete the controller and worker compute instances:


     gcloud -q compute instances delete \
            $(gcloud compute instances list --filter="tags.items=gg-rancher-ha" --format="csv(name)[no-heading]") \
            --zone $(gcloud config get-value compute/zone)
   #+end_src
* Terraform play
** GCP
   +
*** individual instances vs instance groups
    + Create instance templates
      #+begin_src bash  :var IMAGE_FAMILY=cluster-image-family :var IMAGE_PROJECT=cluster-image-project :results output
        gcloud beta compute --project=rancher-dev \
               instance-templates create gg-leap-4-k3s \
               --machine-type=e2-standard-4 \
               --network=projects/rancher-dev/global/networks/default \
               --no-address \
               --tags=http-server,https-server \
               --image-family=${IMAGE_FAMILY} \
               --image-project=${IMAGE_PROJECT} \
               --boot-disk-size=100GB \
               --boot-disk-type=pd-ssd \
               --boot-disk-device-name=gg-leap-4-k3s \
               --no-shielded-secure-boot \
               --shielded-vtpm \
               --shielded-integrity-monitoring \
               --tags gg-k3s \
               --labels=owner=greg_grubbs,donotdelete=true \
               --reservation-affinity=any
      #+end_src


    + CLI to create an instance group from the preceding instance template
      #+begin_src bash
        # no auto-scaling
        gcloud compute --project=rancher-dev \
               instance-groups managed create gg-k3s-leap-servers \
               --base-instance-name=gg-k3s-server \
               --template=gg-leap-4-k3s \
               --size=2 \
               --zone=us-central1-b
        gcloud compute --project=rancher-dev \
               instance-groups managed create gg-k3s-leap-agents \
               --base-instance-name=gg-k3s-agent \
               --template=gg-leap-4-k3s \
               --size=4 \
               --zone=us-central1-b


        # auto-scaling
        # gcloud beta compute --project "rancher-dev" \
        #        instance-groups managed set-autoscaling "gg-k3s-sles-servers" \
        #        --zone "us-central1-b" \
        #        --cool-down-period "60" \
        #        --max-num-replicas "10" \
        #        --min-num-replicas "1" \
        #        --target-cpu-utilization "0.6" \
        #        --mode "off"
      #+end_src

    + Install k3s on servers and agents
      #+begin_src bash
      #+end_src

    + Delete it all when the day is done
      #+begin_src bash
        gcloud -q compute instance-groups managed delete gg-k3s-leap-servers
        gcloud -q compute instance-groups managed delete gg-k3s-leap-agents
      #+end_src
*** Using openSUSE Leap
    + [[https://github.com/anairinac/suse_gcloud_intro][GitHub - anairinac/suse_gcloud_intro: First steps using Google Cloud Platform...]]
      Key bits:
      + --image-family=opensuse-leap
      + --image-project=opensuse-cloud
    #+begin_src bash
      gcloud beta compute --project=rancher-dev \
             instances create opensuse-leap-15-1 \
             --zone=us-central1-b \
             --machine-type=e2-medium \
             --network=default \
             --network-tier=PREMIUM \
             --maintenance-policy=MIGRATE \
             --service-account=117741801349-compute@developer.gserviceaccount.com \
             --scopes=https://www.googleapis.com/auth/devstorage.read_only,https://www.googleapis.com/auth/logging.write,https://www.googleapis.com/auth/monitoring.write,https://www.googleapis.com/auth/servicecontrol,https://www.googleapis.com/auth/service.management.readonly,https://www.googleapis.com/auth/trace.append \
             --image=opensuse-leap-15-1-v20190618 \
             --image-project=opensuse-cloud \
             --boot-disk-size=10GB \
             --boot-disk-type=pd-standard \
             --boot-disk-device-name=opensuse-leap-15-1 \
             --reservation-affinity=any
    #+end_src
* Provision k3s for single-node use
  + If you're having pod networking problems, try
    + ref https://github.com/k3s-io/k3s/issues/24#issuecomment-489130581
    #+begin_src bash
      firewall-cmd --permanent --direct --add-rule ipv4 filter INPUT 1 -i cni0 -s 10.42.0.0/16 -j ACCEPT
      firewall-cmd --permanent --direct --add-rule ipv4 filter FORWARD 1 -s 10.42.0.0/15 -j ACCEPT
      sudo firewall-cmd --reload
    #+end_src
  Like vehicle install - k3s, local path provisioner

   Put up a simple Centos instance
   #+begin_src bash :session homelab-sh :async
     IMAGE_FAMILY=centos-7
     IMAGE_PROJECT=centos-cloud

     # Use the same network as gg-bastion
     mysubnet=$(gcloud compute instances describe gg-bastion --format='get(networkInterfaces[0].subnetwork)')
     if [ "$?" -ne "0" ] ; then
         echo "Bastion host gg-bastion is not running"
     else
         gcloud compute instances describe gg-bastion --format json|jq -r '.networkInterfaces[0].network'

         gcloud compute instances create gg-k3s \
                --async \
                --boot-disk-size 200GB \
                --can-ip-forward \
                --no-address \
                --image-family ${IMAGE_FAMILY} \
                --image-project ${IMAGE_PROJECT} \
                --machine-type e2-standard-4 \
                --subnet ${mysubnet} \
                --scopes compute-rw,storage-ro,service-management,service-control,logging-write,monitoring \
                --tags gg-k3s \
                --labels owner=greg_grubbs,donotdelete=true
         # --private-network-ip 10.240.0.2 \

         echo "Use bastion host to access"
     fi
   #+end_src

   #+begin_src bash
     INSTALL_K3S_VERSION=1.19
   #+end_src

   #+begin_src bash
     echo 'export KUBECONFIG=~/.kube/config
     source <(kubectl completion bash)
     alias k=kubectl
     complete -F __start_kubectl k
     # alias yp='sudo zypper'
     ' | tee -a ~/.bashrc
   #+end_src

   #+begin_src bash
     # yp ref
     # yp in -y helm
     sudo yum makecache
     sudo yum -y install kubectl openssl iscsi-initiator-utils
     curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3
     chmod 700 get_helm.sh
     ./get_helm.sh

     curl -sfL https://get.k3s.io | sh -
     mkdir ~/.kube/
     sudo cp -iv /etc/rancher/k3s/k3s.yaml /home/gregj/.kube/config
     sudo chown gregj ~/.kube/config
     KUBECONFIG=~/.kube/config kubectl get nodes
   #+end_src

   #+begin_src bash
     kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/master/deploy/local-path-storage.yaml
   #+end_src

   #+begin_src bash
     helm repo add bitnami https://charts.bitnami.com/bitnami
     helm repo update
     helm install my-release bitnami/rabbitmq
   #+end_src
* HA k3s
   Make it simple from the get-go with token on first server cmd line
   #+begin_src bash
     INSTALL_K3S_CHANNEL=v.1.19 INSTALL_K3S-EXEC="server" K3S_TOKEN=mysecret K3S_CLUSTER_INIT=1
   #+end_src
** With k3sup
   https://github.com/alexellis/k3sup#advanced-kubeconfig-options

   Nab the =k3sup= CLI

   #+begin_src bash
     curl -sLS https://get.k3sup.dev | sh
     sudo install k3sup /usr/local/bin/

     k3sup --help
   #+end_src

   Skipping to the chase, set up HA with MariaDB/MySQL
   #+begin_src bash
     CONTEXT_NAME=k3s-lab
     LOCAL_DOMAIN=local
     export SERVER1_HOST=chewbacca.${LOCAL_DOMAIN}
     export SERVER2_HOST=todd.${LOCAL_DOMAIN}
     export AGENT1_HOST=birdperson.${LOCAL_DOMAIN}
     export AGENT2_HOST=marvin.${LOCAL_DOMAIN}
       export DATASTORE="mysql://${dbuser}:${dbpass}@tcp(erebor.${LOCAL_DOMAIN})/k3sup"

     # install the first server
     k3sup install --host ${SERVER1_HOST} --user gregj --ssh-key ~/.ssh/k8s-local --datastore ${DATASTORE} --context ${CONTEXT_NAME} --k3s-channel v1.19 --no-extras

     # install the second server
     k3sup install --host ${SERVER2_HOST} --user gregj --ssh-key ~/.ssh/k8s-local --datastore ${DATASTORE} --context ${CONTEXT_NAME} --k3s-channel v1.19 --no-extras

     # Join the agents - to either server
     k3sup join --host ${AGENT1_HOST} --user gregj --ssh-key ~/.ssh/k8s-local --server-host ${SERVER2_HOST}
     k3sup join --host ${AGENT2_HOST} --user gregj --ssh-key ~/.ssh/k8s-local --server-host ${SERVER2_HOST}
   #+end_src
* Build Rancher 2.3.6
  Put together for customer testing
  + [X] provision nodes on GCP
  + [X] Copy relevant CLI binaries to bastion
    #+begin_src bash :async
      gcloud compute ssh gregj@gg-bastion --command "mkdir -p ~/bin"
      gcloud compute scp \
             /usr/local/bin/rke-1.0.6 \
             /usr/local/bin/rancher-v2.3.2 \
             gregj@gg-bastion:bin/

             # /usr/local/bin/helm-2.16.12 \
             # /usr/local/bin/tiller-2.16.12 \

    #+end_src
  + [X] Bring up the RKE cluster as of RKE 1.0.6

       Tangle directly  to the bastion host
      #+begin_src yaml :tangle /gssh:gg-bastion:gcp-2-3-6cluster.yaml
        nodes:
            - address: gg-control-0
              user: gregj
              role:
                - controlplane
                - etcd
            - address: gg-worker-0
              user: gregj
              role:
                - worker
            - address: gg-worker-1
              user: gregj
              role:
                - worker
            - address: gg-worker-2
              user: gregj
              role:
                - worker
        # If set to true, RKE will not fail when unsupported Docker versions are found
        ignore_docker_version: false
        kubernetes_version: "v1.17.4-rancher1-2"
        ssh_agent_auth: true

        # Set the name of the Kubernetes cluster
        cluster_name: cowxar
        services:
        authorization:
            mode: rbac
        addon_job_timeout: 30
        network:
            plugin: canal
        dns:
            provider: coredns
        # addons_include:
        #   - https://github.com/jetstack/cert-manager/releases/download/v0.15.1/cert-manager.yaml
   #+end_src
  + [ ] Run rke up

  #+begin_src bash :session gg-bastion-sh
    ~/bin/rke-1.0.6 up --config gcp-2-3-6cluster.yaml
  #+end_src

  + [ ] install Helm cahrt

    We're doing this the lazy way and using Helm 3
    #+begin_src bash
      helm repo add rancher-stable https://releases.rancher.com/server-charts/latest
      helm repo update
      helm install rancher rancher-stable/rancher \
           --namespace cattle-system \
           --version 2.3.6 \
           --set hostname=rancher.35.226.175.82.xip.io \
           --create-namespace

           # --set hostname=rancher.192.168.1.210.dnsify.me \
           # --set privateCA=true \
           # --set tls=external
    #+end_src


* Harvester play on GCP
  + Mikhail's starting document: [[https://docs.google.com/document/d/1CRoRyjBCsod6qsdTWi7OY8ez0R0fzElKhaYZmGhGgIE/edit#heading=h.fj0o1ccjvsmb][An Integrated HCI Solution for the Edge - Google Docs]]
  + GCP starting point doc: [[https://docs.google.com/document/d/1CRoRyjBCsod6qsdTWi7OY8ez0R0fzElKhaYZmGhGgIE/edit#heading=h.fj0o1ccjvsmb][An Integrated HCI Solution for the Edge - Google Docs]]
    This assumes Ubuntu 20.04 LTS

   Put up a simple Centos instance
   #+begin_src bash :session homelab-sh :async
     IMAGE_FAMILY=ubuntu-2004-lts
     IMAGE_PROJECT=ubuntu-os-cloud

     gcloud compute instances create gg-harvester-builder \
            --async \
            --boot-disk-size 200GB \
            --can-ip-forward \
            --image-family ${IMAGE_FAMILY} \
            --image-project ${IMAGE_PROJECT} \
            --machine-type e2-small \
            --private-network-ip 10.240.0.2 \
            --scopes compute-rw,storage-ro,service-management,service-control,logging-write,monitoring \
            --tags gg-harvester-builder \
            --labels owner=greg_grubbs,donotdelete=true

     # Assure that it is not conflicting in known_hosts
     sleep 5
     ip=$(gcloud compute instances describe gg-harvester-builder --format='get(networkInterfaces[0].accessConfigs[0].natIP)')
     ssh-keygen -R ${ip}
     ssh gregj@${ip} -o StrictHostKeyChecking=no pwd
   #+end_src

   #+begin_src bash
     echo "deb https://packages.cloud.google.com/apt cloud-sdk main" | sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list
     curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
     sudo apt-get update && sudo apt-get install google-cloud-sdk
   #+end_src
