#+PROPERTY: header-args:bash :comments org :shebang #!/usr/bin/env bash :tangle no :eval never-export
#+setupfile: ~/projects/emacs/org-html-themes/org/theme-readtheorg.setup
* Provision HA VMs in GCP

** Provision the cluster nodes

  This script creates 3 etcd nodes backed by high-IOPS disk, 3 controlplane nodes,
  $NUM_WORKERS worker nodes and a bastion / gateway host.

  The script is modeled after the =gregoryg= fork of Kelsey Hightower's Kubernetes the
  Hard Way repo.
  + [[https://github.com/gregoryg/kubernetes-the-hard-way][gregoryg's fork kubernetes-the-hard-way]]
  + [[https://github.com/kelseyhightower/kubernetes-the-hard-way][kelseyhightower/kubernetes-the-hard-way]] - the original

  We create images with a particular OS for all cluster nodes, and separately for the
  bastion host

  The =cluster-os= will be referred to in subsequent install scripts.  Select a value from
  the first column in =os-sets= table
  #+name: cluster-os
  | leap |

  #+name: os-sets
  | OS     | image-family    | image-project  |
  |--------+-----------------+----------------|
  | debian | debian-10       | debian-cloud   |
  | ubuntu | ubuntu-2004-lts | ubuntu-cloud   |
  | leap   | opensuse-leap   | opensuse-cloud |
  | centos | centos-7        | centos-cloud   |

 #+name: cluster-image-family
  #+begin_src bash :var ostable=os-sets :var clusteros=cluster-os :results table replace
    echo ${ostable[$clusteros]}|cut -d' ' -f1
  #+end_src
 #+name: cluster-image-project
  #+begin_src bash :var ostable=os-sets :var clusteros=cluster-os :results table replace
    echo ${ostable[$clusteros]}|cut -d' ' -f2
  #+end_src


  #+begin_src bash :tangle ~/bin/provision-rancher-ha-gcp.sh :var IMAGE_FAMILY=cluster-image-family :var IMAGE_PROJECT=cluster-image-project
    NUM_CONTROL=3
    NUM_ETCD=0   # 0 for co-location with controlplane
    NUM_WORKERS=4
    # family / project pairs such as
    ## ubuntu-1604-lts / ubuntu-os-cloud
    ## opensuse-leap / opensuse-cloud
    ## debian-10 / debian-cloud
    # IMAGE_FAMILY=${IMAGE_FAMILY}
    # IMAGE_PROJECT=${IMAGE_PROJECT}
    # build up lists of nodes for use with PDSH and suchlike
    rm -vf /tmp/worker-nodes.txt /tmp/control-nodes.txt /tmp/all-nodes.txt
    gcloud compute networks create gg-rancher-ha --subnet-mode custom

    # Create the =gg-kubernetes= subnet in the =gg-rancher-ha= VPC network:

    # Create the =gg-rancher-ha= custom VPC network:
    gcloud compute networks subnets create gg-kubernetes \
      --network gg-rancher-ha \
      --range 10.240.0.0/24

    # Create the Cloud NAT and Cloud Router for outbound internet access

    gcloud compute routers create ggrha-router \
        --network gg-rancher-ha

    gcloud compute routers nats create ggrha-nat \
        --router=ggrha-router \
        --auto-allocate-nat-external-ips \
        --nat-all-subnet-ip-ranges \
        --enable-logging

    # Create a firewall rule that allows internal communication across all protocols:


    gcloud compute firewall-rules create gg-rancher-ha-allow-internal \
      --allow tcp,udp,icmp \
      --network gg-rancher-ha \
      --source-ranges 10.240.0.0/24,10.200.0.0/16

    # Create a firewall rule that allows external SSH, ICMP, and HTTPS:


    gcloud compute firewall-rules create gg-rancher-ha-allow-external \
      --allow tcp:22,tcp:6443,icmp \
      --network gg-rancher-ha \
      --source-ranges 0.0.0.0/0

    # No need for public IP
    # gcloud compute addresses create gg-rancher-ha \
    #   --region $(gcloud config get-value compute/region)


    # Create three compute instances which will host the Kubernetes control plane:

    CONTROL_INSTANCE_TYPE=e2-standard-4
      for i in $(seq 0 $((${NUM_CONTROL} - 1))) ; do
      # for i in 0 1 2; do
        gcloud compute instances create gg-control-${i} \
          --async \
          --no-address \
          --boot-disk-size 200GB \
          --can-ip-forward \
          --image-family ${IMAGE_FAMILY} \
          --image-project ${IMAGE_PROJECT} \
          --machine-type ${CONTROL_INSTANCE_TYPE} \
          --private-network-ip 10.240.0.1${i} \
          --scopes compute-rw,storage-ro,service-management,service-control,logging-write,monitoring \
          --subnet gg-kubernetes \
          --tags gg-rancher-ha,control \
          --labels owner=greg_grubbs,donotdelete=true
        echo gg-control-${i} >> /tmp/control-nodes.txt
      done

    # Create ${NUM_WORKERS} compute instances which will host the Kubernetes worker nodes:

    WORKER_INSTANCE_TYPE=e2-standard-8
      for i in $(seq 0 $((${NUM_WORKERS} - 1))) ; do
      # for i in 0 1 2; do
        gcloud compute instances create gg-worker-${i} \
          --async \
          --no-address \
          --boot-disk-size 200GB \
          --can-ip-forward \
          --image-family ${IMAGE_FAMILY} \
          --image-project ${IMAGE_PROJECT} \
          --machine-type ${WORKER_INSTANCE_TYPE} \
          --metadata pod-cidr=10.200.${i}.0/24 \
          --private-network-ip 10.240.0.2${i} \
          --scopes compute-rw,storage-ro,service-management,service-control,logging-write,monitoring \
          --subnet gg-kubernetes \
          --tags gg-rancher-ha,worker \
          --labels owner=greg_grubbs,donotdelete=true
        echo gg-worker-${i} >> /tmp/worker-nodes.txt
      done
    cat /tmp/control-nodes.txt /tmp/worker-nodes.txt > /tmp/all-nodes.txt
  #+end_src

** Provision the bastion host
   Put up a simple Debian bastion host
   #+begin_src bash :tangle ~/bin/provision-bastion-gcp.sh
     BASTION_IMAGE_FAMILY=debian-10
     BASTION_IMAGE_PROJECT=debian-cloud

     # Create a small instance as a bastion/gateway host - the only VM with a public IP
     gcloud compute instances create gg-bastion \
            --async \
            --boot-disk-size 200GB \
            --can-ip-forward \
            --image-family ${BASTION_IMAGE_FAMILY} \
            --image-project ${BASTION_IMAGE_PROJECT} \
            --machine-type e2-small \
            --private-network-ip 10.240.0.2 \
            --scopes compute-rw,storage-ro,service-management,service-control,logging-write,monitoring \
            --subnet gg-kubernetes \
            --tags gg-bastion \
            --labels owner=greg_grubbs,donotdelete=true

     # Assure that it is not conflicting in known_hosts
     sleep 5
     ip=$(gcloud compute instances describe gg-bastion --format='get(networkInterfaces[0].accessConfigs[0].natIP)')
     ssh-keygen -R ${ip}
     ssh gregj@${ip} -o StrictHostKeyChecking=no pwd
#+end_src

** Copy node names to our bastion host for use with =pdsh=
   #+begin_src bash :session rancher-rke-sh :results none
     (cd /tmp; gcloud compute scp all-nodes.txt control-nodes.txt worker-nodes.txt gg-bastion:)
   #+end_src

   Open a shell on the bastion host as buffer named =gg-bastion-sh=

  #+name: bastion_ip
   #+begin_src bash
    # Get public IP
    gcloud compute instances describe gg-bastion --format='get(networkInterfaces[0].accessConfigs[0].natIP)'
   #+end_src


   #+begin_src emacs-lisp :var ip=bastion_ip :results none
     (call-process-shell-command (concat "ssh -o StrictHostKeyChecking=no gregj@" ip " id"))
     (cd (concat "/ssh:gregj@" ip ":"))
     (shell "gg-bastion-sh")
   #+end_src
   #+begin_src bash :session gg-bastion-sh :results none
     sudo apt update ; sudo apt -y install pdsh wget
     for i in in `cat all-nodes.txt` ; do
         ssh -o StrictHostKeyChecking=no $i pwd
     done
     WCOLL=all-nodes.txt pdsh -R ssh pwd
   #+end_src

** Update all nodes and install Docker
   #+begin_src bash :async :session gg-bastion-sh :var clusteros=cluster-os
     if [ ${clusteros} == "leap" ] ; then
         WCOLL=all-nodes.txt pdsh -R ssh 'sudo zypper ref && sudo zypper in -y docker'
     else
         WCOLL=all-nodes.txt pdsh -R ssh 'sudo apt update && sudo apt -y upgrade && sudo apt -y install docker.io '
     fi
     WCOLL=all-nodes.txt pdsh -R ssh 'sudo usermod -G docker -a gregj && sudo systemctl daemon-reload && sudo systemctl restart docker'
     WCOLL=all-nodes.txt pdsh -R ssh 'docker ps'
   #+end_src

** Create the RKE cluster
*** Download the =rke= CLI to bastion host
    Current [2020-11-06 Fri] version: 1.2.1
    #+begin_src bash :session gg-bastion-sh :async :results value
      wget 'https://github.com/rancher/rke/releases/download/v1.2.1/rke_linux-amd64'
      chmod a+rx rke_linux-amd64
      sudo mv -iv ./rke_linux-amd64 /usr/local/bin/rke
      rke
    #+end_src

*** Our canonical RKE config
    Tangle directly  to the bastion host
   #+begin_src yaml :var ip=bastion_ip :tangle /gssh:gg-bastion:gcp-cluster.yaml
     nodes:
         - address: gg-control-0
           user: gregj
           role:
             - controlplane
             - etcd
         - address: gg-control-1
           user: gregj
           role:
             - controlplane
             - etcd
         - address: gg-control-2
           user: gregj
           role:
             - controlplane
             - etcd
         - address: gg-worker-0
           user: gregj
           role:
             - worker
         - address: gg-worker-1
           user: gregj
           role:
             - worker
         - address: gg-worker-2
           user: gregj
           role:
             - worker
         - address: gg-worker-3
           user: gregj
           role:
             - worker
     # If set to true, RKE will not fail when unsupported Docker versions are found
     ignore_docker_version: false

     # Cluster level SSH private key
     # Used if no ssh information is set for the node
     # ssh_key_path: ~/.ssh/k8s-local
     #
     # Enable use of SSH agent to use SSH private keys with passphrase
     # This requires the environment `SSH_AUTH_SOCK` configured pointing
     # to your SSH agent which has the private key added
     ssh_agent_auth: true

     # Set the name of the Kubernetes cluster
     cluster_name: uncowlanut


     services:

     authorization:
         mode: rbac

     # Add-ons are deployed using kubernetes jobs. RKE will give
     # up on trying to get the job status after this timeout in seconds..
     addon_job_timeout: 30

     # Specify network plugin-in (canal, calico, flannel, weave, or none)
     # GJG remember that Canal is really Flannel+Calico https://rancher.com/blog/2019/2019-03-21-comparing-kubernetes-cni-providers-flannel-calico-canal-and-weave/
     network:
         plugin: canal

     # Specify DNS provider (coredns or kube-dns)
     dns:
         provider: coredns

     addons_include:
       - https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.1/aio/deploy/recommended.yaml
       - https://gist.githubusercontent.com/superseb/499f2caa2637c404af41cfb7e5f4a938/raw/930841ac00653fdff8beca61dab9a20bb8983782/k8s-dashboard-user.yml
       - https://github.com/jetstack/cert-manager/releases/download/v0.15.1/cert-manager.yaml
   #+end_src
*** Run =rke up=
    #+begin_src bash :session gg-bastion-sh :async :results value
      rke up --config gcp-cluster.yaml
    #+end_src

*** Get kubectl and test
    #+begin_src bash :session gg-bastion-sh :async :results value
      sudo apt-get update && sudo apt-get install -y apt-transport-https gnupg2 curl
      curl -s https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -
      echo "deb https://apt.kubernetes.io/ kubernetes-xenial main" | sudo tee -a /etc/apt/sources.list.d/kubernetes.list
      sudo apt-get update
      sudo apt-get install -y kubectl
    #+end_src

    #+begin_src bash :session gg-bastion-sh :async :results value
      ## GJG
      mkdir -p ~/.kube
      cp -v kube_config_gcp-cluster.yaml ~/.kube/config
      kubectl get nodes
      source <(kubectl completion bash)
      alias k='kubectl'
      alias kn='kubectl config set-context --current --namespace '
      alias kx='kubectl config get-contexts'
      complete -F __start_kubectl k
    #+end_src


** On to Rancher to install these thangs!

*** Get Helm on the bastion node
    #+begin_src bash :session gg-bastion-sh :async :results value
      wget 'https://get.helm.sh/helm-v3.4.1-linux-amd64.tar.gz'
      tar xf helm-v3.4.1-linux-amd64.tar.gz
      sudo cp -v linux-amd64/helm /usr/local/bin/
      helm version
    #+end_src

*** Install Rancher 2.5 monitoring
    #+begin_src bash :session gg-bastion-sh :async :results value
      helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
      helm repo add stable https://charts.helm.sh/stable
      helm repo update
      helm install rancher-monitoring prometheus-community/kube-prometheus-stack --namespace cattle-monitoring-system --create-namespace
    #+end_src

** Clean it all up once the fun is over
   #+begin_src bash :tangle ~/bin/cleanup-rancher-ha-gcp.sh
     # Delete the controller and worker compute instances:


     gcloud -q compute instances delete \
            $(gcloud compute instances list --filter="tags.items=gg-rancher-ha" --format="csv(name)[no-heading]") \
            --zone $(gcloud config get-value compute/zone)

     # Delete Cloud NAT and Cloud Router

     gcloud -q compute routers nats delete ggrha-nat --router ggrha-router
     gcloud -q compute routers delete ggrha-router

     # Delete the external load balancer network resources:


     gcloud -q compute forwarding-rules delete kubernetes-forwarding-rule \
            --region $(gcloud config get-value compute/region)

     gcloud -q compute target-pools delete kubernetes-target-pool

     gcloud -q compute http-health-checks delete kubernetes

     gcloud -q compute addresses delete gg-rancher-ha

     # Delete the =gg-rancher-ha= firewall rules:


     gcloud -q compute firewall-rules delete \
            gg-rancher-ha-allow-nginx-service \
            gg-rancher-ha-allow-internal \
            gg-rancher-ha-allow-external \
            gg-rancher-ha-allow-health-check

     # Delete the =gg-rancher-ha= network VPC:


     gcloud -q compute routes delete \
            kubernetes-route-10-200-0-0-24 \
            kubernetes-route-10-200-1-0-24 \
            kubernetes-route-10-200-2-0-24

     gcloud -q compute networks subnets delete gg-kubernetes

     gcloud -q compute networks delete gg-rancher-ha
   #+end_src
* Terraform play
** GCP
   +
*** individual instances vs instance groups
    + Create instance templates
      #+begin_src bash  :var IMAGE_FAMILY=cluster-image-family :var IMAGE_PROJECT=cluster-image-project :results output
        gcloud beta compute --project=rancher-dev \
               instance-templates create gg-leap-4-k3s \
               --machine-type=e2-standard-4 \
               --network=projects/rancher-dev/global/networks/default \
               --no-address \
               --tags=http-server,https-server \
               --image-family=${IMAGE_FAMILY} \
               --image-project=${IMAGE_PROJECT} \
               --boot-disk-size=100GB \
               --boot-disk-type=pd-ssd \
               --boot-disk-device-name=gg-leap-4-k3s \
               --no-shielded-secure-boot \
               --shielded-vtpm \
               --shielded-integrity-monitoring \
               --tags gg-k3s \
               --labels=owner=greg_grubbs,donotdelete=true \
               --reservation-affinity=any
      #+end_src


    + CLI to create an instance group from the preceding instance template
      #+begin_src bash
        # no auto-scaling
        gcloud compute --project=rancher-dev \
               instance-groups managed create gg-k3s-leap-servers \
               --base-instance-name=gg-k3s-server \
               --template=gg-leap-4-k3s \
               --size=2 \
               --zone=us-central1-b
        gcloud compute --project=rancher-dev \
               instance-groups managed create gg-k3s-leap-agents \
               --base-instance-name=gg-k3s-agent \
               --template=gg-leap-4-k3s \
               --size=4 \
               --zone=us-central1-b


        # auto-scaling
        # gcloud beta compute --project "rancher-dev" \
        #        instance-groups managed set-autoscaling "gg-k3s-sles-servers" \
        #        --zone "us-central1-b" \
        #        --cool-down-period "60" \
        #        --max-num-replicas "10" \
        #        --min-num-replicas "1" \
        #        --target-cpu-utilization "0.6" \
        #        --mode "off"
      #+end_src

    + Install k3s on servers and agents
      #+begin_src bash
      #+end_src

    + Delete it all when the day is done
      #+begin_src bash
        gcloud -q compute instance-groups managed delete gg-k3s-leap-servers
        gcloud -q compute instance-groups managed delete gg-k3s-leap-agents
      #+end_src
*** Using openSUSE Leap
    + [[https://github.com/anairinac/suse_gcloud_intro][GitHub - anairinac/suse_gcloud_intro: First steps using Google Cloud Platform...]]
      Key bits:
      + --image-family=opensuse-leap
      + --image-project=opensuse-cloud
    #+begin_src bash
      gcloud beta compute --project=rancher-dev \
             instances create opensuse-leap-15-1 \
             --zone=us-central1-b \
             --machine-type=e2-medium \
             --network=default \
             --network-tier=PREMIUM \
             --maintenance-policy=MIGRATE \
             --service-account=117741801349-compute@developer.gserviceaccount.com \
             --scopes=https://www.googleapis.com/auth/devstorage.read_only,https://www.googleapis.com/auth/logging.write,https://www.googleapis.com/auth/monitoring.write,https://www.googleapis.com/auth/servicecontrol,https://www.googleapis.com/auth/service.management.readonly,https://www.googleapis.com/auth/trace.append \
             --image=opensuse-leap-15-1-v20190618 \
             --image-project=opensuse-cloud \
             --boot-disk-size=10GB \
             --boot-disk-type=pd-standard \
             --boot-disk-device-name=opensuse-leap-15-1 \
             --reservation-affinity=any
    #+end_src
